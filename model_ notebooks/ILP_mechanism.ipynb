{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Liquiidty provisioning is a stochastic optimal control problem\n",
    "It has a well defined a untility function which we need to miaximaize as obejctive function of probelm\n",
    "Some applications of RL in finance include trading bots, risk optimization and portfolio managemnet\n",
    "\n",
    " ## Steps\n",
    " 1. Define Uniswap v3 environemnt\n",
    " 2. Define RL Agent\n",
    " 3. Train RL agent\n",
    " 4. Evaluate the Performance of RL agent\n",
    "\n",
    "# Overview\n",
    "The agent would be your liquidity management system, the environment would be the Uniswap v3 pool, the states would be the historical and real-time data, the actions would be the decisions to keep liquidity as is, add more liquidity, remove liquidity, or change position, and the reward would be the fees earned minus the impermanent loss (IL).\n",
    "\n",
    "### Implementation\n",
    "State Representation: You would need to represent the state of the Uniswap v3 pool in a way that the RL agent can understand. This might include the current price, the total liquidity, the volume of trades, the fees earned, and any other relevant data. You might also include recent changes in these values or other derived features. The state should capture all the information that the agent needs to make a decision.\n",
    "\n",
    "Action Space: The action space is the set of all possible actions that the agent can take. In your case, this might be a discrete action space with four actions (keep liquidity as is, add more liquidity, remove liquidity, change position), or it might be a continuous action space where the agent can choose the amount of liquidity to add or remove.\n",
    "\n",
    "Reward Function: The reward function defines how the agent is rewarded or penalized for its actions. In your case, this could be the fees earned minus the IL for each time step. You might need to normalize or scale this reward to ensure stable and efficient learning.\n",
    "\n",
    "Policy and Value Function: The policy is a function that the agent uses to choose actions based on the current state. The value function estimates the expected cumulative reward for each state or state-action pair. The agent's goal is to learn the optimal policy and value function.\n",
    "\n",
    "RL Algorithm: There are many RL algorithms to choose from, including Q-learning, Deep Q Network (DQN), Policy Gradient, Actor-Critic methods, Proximal Policy Optimization (PPO), etc. You would need to choose an algorithm based on your problem characteristics and computational resources.\n",
    "\n",
    "Training and Evaluation: You would need to train your RL agent on historical data and evaluate its performance on out-of-sample data. This might involve running many episodes where the agent interacts with the environment and learns from the feedback.\n",
    "\n",
    "Deployment: Once your agent is trained, you can deploy it to make real-time decisions about liquidity management. You'll need a system that can implement these decisions on the Ethereum blockchain.\n",
    "\n",
    "# Selection of Approriate Model\n",
    "\n",
    "Liquidity provisioning in platforms like Uniswap v3 is a complex problem, influenced by factors like price volatility, fee earnings, capital efficiency, impermanent loss, and gas costs. Given this complexity, we need to consider several aspects to determine the appropriate model:\n",
    "\n",
    "1. **State and Action Space**:\n",
    "   - Uniswap v3 allows LPs to provide liquidity in specific price ranges, leading to potentially infinite action spaces. Models that handle continuous or large action spaces are preferable.\n",
    "   \n",
    "2. **Sample Efficiency**:\n",
    "   - Interacting with blockchain-based environments like Uniswap can be expensive (in terms of gas costs) and slow. Algorithms that are sample-efficient, i.e., can learn from fewer interactions with the environment, can be beneficial.\n",
    "\n",
    "3. **Stability**:\n",
    "   - Given the financial implications, the RL algorithm should be stable and not produce erratic policies.\n",
    "\n",
    "4. **Exploration vs. Exploitation**:\n",
    "   - The agent should balance between exploring new liquidity ranges and exploiting known profitable ranges. Exploration techniques become crucial.\n",
    "\n",
    "Considering the above, here are potential candidate algorithms:\n",
    "\n",
    "1. **Proximal Policy Optimization (PPO)**:\n",
    "   - **Why?** PPO is known for its stability and robustness. It's a policy gradient method, which means it directly optimizes the policy without needing to estimate value functions, making it more stable\n",
    "   - **Challenges:** Might require more samples compared to off-policy methods\n",
    "\n",
    "2. **Soft Actor-Critic (SAC)**:\n",
    "   - **Why?** SAC can handle continuous action spaces, and its entropy regularization encourages exploration. It's also off-policy, meaning it can learn from past experiences more efficiently.\n",
    "   - **Challenges:** Adapting SAC for the specific action space of Uniswap v3 (i.e., defining liquidity ranges) might be non-trivial.\n",
    "\n",
    "3. **Deep Deterministic Policy Gradient (DDPG) or Twin Delayed DDPG (TD3)**:\n",
    "   - **Why?** Both algorithms are designed for continuous action spaces. TD3 also introduces mechanisms to reduce value overestimation, which can improve stability.\n",
    "   - **Challenges:** Like SAC, adapting these to the liquidity provisioning problem might require careful design.\n",
    "\n",
    "4. **Q-Learning or Deep Q-Networks (DQN)**:\n",
    "   - **Why?** While traditionally designed for discrete action spaces, they can be adapted for liquidity provisioning by discretizing the action space (i.e., having fixed liquidity ranges).\n",
    "   - **Challenges:** Discretization might not capture all nuances of liquidity provisioning in Uniswap v3.\n",
    "\n",
    "# Iteration 1: Simplest Approach\n",
    "\n",
    "### 1. **Understanding the Problem:**\n",
    "- **Environment:** Understand the specifics of Uniswap v3, especially the liquidity ranges and how fees are accrued.\n",
    "- **State Space:** Define what constitutes the state. For starters, you might consider the current liquidity position, recent price movements, and accumulated fees.\n",
    "- **Action Space:** Begin with simple actions like 'add liquidity', 'remove liquidity', or 'do nothing'. Later, you can expand this to include specifying exact liquidity ranges.\n",
    "\n",
    "### 2. **Tabular Q-Learning:**\n",
    "- **Why:** This is the simplest form of RL and doesn't require neural networks or complex optimization.\n",
    "- **Challenges:** Limited to discrete states and actions. Might not scale well as the state and action space grow.\n",
    "\n",
    "### 3. **Deep Q-Learning (DQN):**\n",
    "- **Why:** Can handle larger state spaces using neural networks as function approximators.\n",
    "- **Steps:**\n",
    "  1. **Discretize Action Space:** Start with broad liquidity ranges.\n",
    "  2. **State Representation:** Use a neural network to approximate the Q-values.\n",
    "  3. **Experience Replay:** Store experiences and sample them to train the network.\n",
    "  4. **Target Network:** Use a separate network to stabilize learning.\n",
    "\n",
    "### 4. **Policy Gradient Methods (e.g., PPO):**\n",
    "- **Why:** Directly optimize the policy without needing a value function. Known for stability.\n",
    "- **Steps:**\n",
    "  1. **Continuous Action Space:** PPO can handle continuous actions, which can be useful if you decide to provide liquidity in more granular ranges.\n",
    "  2. **Clipped Objective:** Helps in stabilizing the training.\n",
    "\n",
    "### 5. **Advanced Methods (e.g., SAC, DDPG):**\n",
    "- **Why:** Designed for continuous state and action spaces. Can be more sample-efficient.\n",
    "- **Steps:**\n",
    "  1. **Refine Action Space:** Instead of broad actions like 'add liquidity', you specify exact ranges or amounts.\n",
    "  2. **Entropy Regularization (SAC):** Encourages exploration, which can be crucial in the volatile crypto market.\n",
    "\n",
    "### 7. **Refinement and Iteration:**\n",
    "- **Feature Engineering:** Incorporate more features like gas prices, historical volatility, etc.\n",
    "- **Hyperparameter Tuning:** Experiment with learning rates, discount factors, etc.\n",
    "- **Exploration Strategies:** Experiment with epsilon-greedy, UCB, or Thompson sampling.\n",
    "\n",
    "### 8. **Safety Precautions:**\n",
    "- **Simulations:** Before deploying in the real world, simulate the RL agent's behavior in various market scenarios.\n",
    "- **Backtesting:** Test the agent on historical data.\n",
    "- **Safety Nets:** Implement logic to prevent extreme actions, which might lead to significant financial loss.\n",
    "\n",
    "### 9. **Continuous Learning and Adaptation:**\n",
    "- The crypto market is volatile. Continuously train and adapt your agent to new market conditions.\n",
    "\n",
    "### 10. **Evaluation and Scaling:**\n",
    "- **Metrics:** Monitor not just profits but also risk-adjusted returns, drawdowns, etc.\n",
    "- **Scaling:** As your confidence grows with a smaller amount, consider scaling your liquidity provisioning.\n",
    "\n",
    "Remember, at each step, thoroughly validate the agent's performance and understand its behavior. Especially in financial applications, it's crucial to know when and why your agent makes certain decisions.\n",
    " \n",
    "# Reward Function\n",
    "Designing an appropriate reward function is crucial for training a reinforcement learning agent effectively. For the liquidity provisioning problem in the context of Uniswap v3 or similar protocols, the reward function should reflect the goals of the liquidity provider (LP).\n",
    "\n",
    "Here's a possible approach to designing the reward function:\n",
    "\n",
    "1. **Fees**: The primary source of income for LPs is the trading fees they earn from trades that use their liquidity. Thus, the reward should be positively proportional to the fees earned.\n",
    "\n",
    "   fees =fees earned in the current timestep\n",
    "\n",
    "2. **Impermanent Loss (IL)**: IL is the loss that comes from providing liquidity in a decentralized market maker and is a major concern for LPs. The reward should account for this and penalize the agent if IL increases.\n",
    "\n",
    "   IL = change in IL from the previous timestep\n",
    "\n",
    "3. **Gas Costs**: Every time liquidity is added or removed, there are associated gas costs. These costs should be subtracted from the reward to encourage efficient liquidity management.\n",
    "\n",
    "   gas = gas costs for the current action\n",
    "\n",
    "4. **Liquidity Utilization**: You may want to encourage the agent to provide liquidity where it's most utilized. This can be achieved by rewarding the agent based on the trade volume that uses its liquidity.\n",
    "\n",
    "   utilization = portion of liquidity utilized in trades\n",
    "\n",
    "5. **Safety**: You might also want to introduce a term to ensure the liquidity doesn't fall below a certain threshold, which could expose the LP to risks.\n",
    "\n",
    "   safety = penalty if liquidity falls below a threshold\n",
    "\n",
    "The final reward function could be a weighted sum of these components:\n",
    "\n",
    "\n",
    "R= w_1 *fees + w_2 * IL + w_3 * gas + w_4 * utilization + w_5 * safety\n",
    "\n",
    "\n",
    "# Customized Reward Function based on LP's prefernces\n",
    "Absolutely! In fact, personalizing the reward function according to the LP's preferences is one of the primary advantages of using an RL approach for liquidity provisioning. Each LP might have different priorities and risk tolerances, and the reward function should reflect that. Here's how you can adapt the reward function based on LP preferences:\n",
    "\n",
    "1. **Risk Tolerance**: \n",
    "    - **Risk-Averse**: If an LP is risk-averse, you can increase the weight \\( w_2 \\) to heavily penalize any increase in impermanent loss. You might also want to have a higher penalty in \\( R_{\\text{safety}} \\) to ensure that liquidity doesn't drop to risky levels.\n",
    "    - **Risk-Taking**: For a more risk-taking LP, you might reduce the weight on \\( R_{\\text{IL}} \\) and \\( R_{\\text{safety}} \\) to allow the agent to explore potentially higher reward strategies that come with increased risk.\n",
    "\n",
    "2. **Profit Motivation**:\n",
    "    - **High**: If the primary goal is to maximize profits, increase the weight \\( w_1 \\) associated with fees. You might also want to emphasize liquidity utilization by increasing \\( w_4 \\), as high utilization generally leads to higher fee earnings.\n",
    "    - **Moderate**: For a balanced approach, the weights can be more evenly distributed.\n",
    "\n",
    "3. **Cost Sensitivity**: \n",
    "    - If an LP is particularly sensitive to costs (like gas fees), increase the weight \\( w_3 \\) to make sure the agent is more conservative in making liquidity adjustments that involve on-chain transactions.\n",
    "\n",
    "4. **Liquidity Utilization**:\n",
    "    - **Maximize**: If an LP wants their funds to be utilized to the maximum extent, increase \\( w_4 \\) to encourage the agent to provide liquidity in high volume areas.\n",
    "    - **Balanced**: A balanced weight ensures that the agent doesn't overly concentrate liquidity in high volume areas, spreading risk.\n",
    "\n",
    "5. **Custom Components**: LPs might have other specific preferences or goals not captured in the standard components. For instance:\n",
    "    - **Diversification**: An LP might want to spread their liquidity across multiple pools or tokens. You can introduce a reward component that rewards diversification.\n",
    "    - **Long-term positions**: If an LP has a long-term bullish view on a particular token, the reward function can be designed to favor providing liquidity to that token's pools.\n",
    "\n",
    "Once the LP's preferences are defined, you can adjust the weights \\( w_1, w_2, \\ldots, w_5 \\) (and introduce new components if needed) to reflect those preferences in the reward function.\n",
    "\n",
    "Remember, the key is iteration. Once you've set a reward function based on the LP's preferences, you'll want to monitor the agent's performance and potentially make further adjustments to better align with the LP's goals over time.\n",
    "\n",
    "# State Space Representation\n",
    "\n",
    "1. **Importance of State Space:**\n",
    "   - State space must strike a balance: detailed enough for informed decisions but sparse enough for efficient learning.\n",
    "\n",
    "2. **Inventory:**\n",
    "   - Widely used in models. Represents the assets a market maker holds.\n",
    "   - Monitoring inventory is vital as large holdings can expose market makers to significant price directional risks.\n",
    "   - Different representations:\n",
    "     - **Nominal Quantity:** Represents the exact amount held.\n",
    "     - **Binned Levels:** Categorizes inventory into levels like small, medium, and large for computational efficiency.\n",
    "\n",
    "3. **Time:**\n",
    "   - Some models consider time as a factor.\n",
    "   - Trading periods are discretized, and the state can represent time passed or time remaining.\n",
    "\n",
    "4. **Imbalances:**\n",
    "   - Measures of trade imbalances are common:\n",
    "     - **Order Imbalance:** Compares buy and sell orders.\n",
    "     - **Trade Flow Imbalance:** Measures the magnitude of transactions initiated by buyers and sellers over a time window.\n",
    "   - Imbalances can be predictive of future asset prices.\n",
    "\n",
    "5. **Relative Strength Index (RSI):**\n",
    "   - A technical indicator used to determine if an asset is overbought or oversold.\n",
    "   - Calculated using exponential moving averages of upward and downward price changes.\n",
    "\n",
    "6. **Volatility:**\n",
    "   - Represents price fluctuations and is used in some models.\n",
    "   - Market makers might adjust their strategies based on prevailing volatility.\n",
    "\n",
    "7. **Price Data:**\n",
    "   - Various price-related data points such as bid price, ask price, mid price, bid-ask spread, and others.\n",
    "\n",
    "8. **Limit Order Book (LOB) Data:**\n",
    "   - Comprehensive LOB data includes stationary prices and cumulative notional value or volume at each price level.\n",
    "   - This representation is more common in deep reinforcement learning models due to their capacity to handle larger state spaces.\n",
    "\n",
    "In essence, the state space in market-making reinforcement learning models can encompass a range of variables, from inventory levels to technical indicators like RSI. The choice of which variables to include and how to represent them is crucial for the model's effectiveness and efficiency.\n",
    "\n",
    "# Define Environment\n",
    "## State space Representation\n",
    "Inventory: The current liquidity (tokens or assets) the market maker holds.\n",
    "Time: The time remaining in the trading period.\n",
    "Imbalances: Measures like order book imbalance.\n",
    "RSI (Relative Strength Index): A momentum indicator.\n",
    "Volatility: A measure of price fluctuations.\n",
    "Price Data: Current mid price, bid price, ask price, bid-ask spread, etc.\n",
    "LOB Data: Limit Order Book data including price level distance to mid price for several price levels on both bid and ask sides.\n",
    "\n",
    "### State Space:\n",
    "\n",
    "Current liquidity position (how much liquidity the agent has provided in the pool).\n",
    "Recent price movements (historical data on the asset price).\n",
    "Accumulated fees (fees earned from providing liquidity).\n",
    "Order imbalances (difference between buy and sell orders).\n",
    "Volatility (measuring price fluctuations).\n",
    "Relative Strength Index (RSI) indicating overbought or oversold conditions.\n",
    "\n",
    "## Action Space Representation\n",
    "Bid and Ask Prices (equivalent to setting liquidity ranges in Uniswap v3):\n",
    "\n",
    "Predetermined Bid-Ask Pairs: Instead of stock prices, in the context of Uniswap v3, this would correspond to setting liquidity within predetermined price ranges. For example, an action could be choosing to provide liquidity within a range of predefined price intervals.\n",
    "Independent Liquidity Depths: This would allow the agent to specify the start and end prices of the liquidity range independently. For example, it can decide to provide liquidity from price X to Y for Token A vs. Token B.\n",
    "Liquidity Amount:\n",
    "\n",
    "This would be analogous to the volume in traditional market-making. The agent decides how much liquidity (in terms of the tokens) it wants to allocate within the specified price range. For instance, the agent could decide to provide 10, 50, or 100 tokens within a certain price range.\n",
    "Adjust Liquidity:\n",
    "\n",
    "Add: Increase liquidity within a specific price range.\n",
    "Remove: Decrease or remove liquidity from a specific price range.\n",
    "Shift: Move the liquidity from one price range to another. For instance, if the agent anticipates the price will move out of its current range, it might want to shift its liquidity to a different range.\n",
    "No Action:\n",
    "\n",
    "### Action Space:\n",
    "\n",
    "Bid and Ask Prices: The agent can select from a predetermined set of bid and ask pairs.\n",
    "Volumes: The volume of liquidity to add or remove.\n",
    "Market Orders: An action to clear a part of the agent's liquidity.\n",
    "No Action: An option for the agent to do nothing.\n",
    "\n",
    "Certainly! Based on the above information and the context of the liquidity provisioning problem in Uniswap V3, here's a draft of a reward function for the problem:\n",
    "\n",
    "## Components of the Reward:\n",
    "\n",
    "1. **Profit and Loss (PnL)**: \n",
    "    - Unrealized PnL: The change in the value of the inventory due to price fluctuations.\n",
    "    - Realized PnL: The profit or loss realized from trading.\n",
    "2. **Inventory Penalization**: Penalize large inventory positions to avoid overexposure to directional moves in the asset's price.\n",
    "3. **Sharpe Ratio**: To account for risk-adjusted returns. In the context of Uniswap, where there's no traditional portfolio but a provision of liquidity, this would measure how efficiently the liquidity is being used to generate returns.\n",
    "4. **Trade Completion Reward**: Encourage the agent to provide liquidity in a way that facilitates trade completions.\n",
    "\n",
    "### Reward Function:\n",
    "\n",
    "Given the above components, the reward at a given timestep \\( t \\) can be defined as:\n",
    "\n",
    "\n",
    "R_t = \\alpha \\times \\Delta UPnL_t + \\beta \\times \\Delta RPnL_t - \\gamma \\times \\text{Penalize}(Q_t) + \\delta \\times \\text{SharpeRatio}(R_t, \\sigma_t) + \\epsilon \\times \\text{TradeCompletionReward}\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- Delta UPnL_t is the unrealized PnL at time  t .\n",
    "- Delta RPnL_t  is the realized PnL change from time  t-1  to  t .\n",
    "- Q_t  is the inventory at time  t\n",
    "\n",
    "- R_t  is the return from liquidity provisioning at time  t.\n",
    "- sigma_t  is the volatility or standard deviation of the returns up to time  t .\n",
    "- alpha, \\beta, \\gamma, \\delta, \\epsilon are weights that can be adjusted based on the importance of each component.\n",
    "\n",
    "The `Penalize` function can be a simple penalty based on the size of the inventory:\n",
    "\n",
    "\\text{Penalize}(Q_t) = \\kappa \\times |Q_t|\n",
    "\n",
    "Where  \\kappa  is a constant that determines the severity of the penalty.\n",
    "\n",
    "The `SharpeRatio` function is given by:\n",
    "\n",
    "\\text{SharpeRatio}(R_t, \\sigma_t) = \\frac{R_t - R_f} {\\sigma_t}\n",
    "\n",
    "Where R_f  is the risk-free rate (which may be approximated by some stablecoin yield in the crypto context).\n",
    "\n",
    "The `TradeCompletionReward` can be a fixed reward given when a trade is facilitated due to the liquidity provided by the agent.\n",
    "\n",
    "This is a basic outline of the reward function. Adjustments may need to be made based on specific details of the Uniswap V3 protocol and the objectives of the liquidity provider. The weights  \\alpha, \\beta, \\gamma, \\delta, \\epsilon  and other parameters should be fine-tuned to achieve the desired behavior in the agent.\n",
    "\n",
    "# Modeling Uniswap v3 Market:\n",
    "Uniswap v3 introduces concentrated liquidity, allowing liquidity providers (LPs) to set custom price ranges for their capital, leading to a more capital-efficient system. Given this, modeling the Uniswap v3 market for liquidity provisioning would require some adjustments:\n",
    "\n",
    "### State Representation:\n",
    "\n",
    "In addition to the standard order book state, you'd need to consider the range in which liquidity is provided by each LP.\n",
    "Track the current position of the pool's price relative to the provided ranges.\n",
    "### Actions:\n",
    "\n",
    "Actions would include adding/removing liquidity within specific price ranges.\n",
    "Adjusting the range of existing liquidity.\n",
    "### Reward Structure:\n",
    "\n",
    "Rewards would be based on trading fees earned when trades occur within the LP's price range.\n",
    "Penalties or costs associated with impermanent loss should also be considered.\n",
    "Market Dynamics:\n",
    "\n",
    "Given that Uniswap v3 operates on an automated market maker (AMM) model, the dynamics would be different from a traditional order book. The price movement is determined by the ratio of assets in the pool and the trades that occur.\n",
    "\n",
    "Certainly! Let's design a model for the Uniswap v3 market based on the provided information and the unique characteristics of Uniswap v3:\n",
    "\n",
    "### **Uniswap v3 Market Model**\n",
    "\n",
    "1. **State Representation**:\n",
    "   - **Liquidity Ranges**: For each LP, store the price ranges they've provided liquidity for. This could be represented as a list of tuples, where each tuple contains the lower and upper bounds of the price range.\n",
    "   - **Pool Price**: The current price of the pool, which determines which LPs are earning fees at any given moment.\n",
    "   - **Liquidity Depth**: The amount of liquidity provided within each price range.\n",
    "   - **Pool Reserves**: The current reserves of each token in the pool, which determine the pool's price according to the x * y = k invariant.\n",
    "\n",
    "2. **Actions**:\n",
    "   - **Add Liquidity**: An LP can add liquidity within a specific price range. This action would require specifying the amount of each token to add and the desired price range.\n",
    "   - **Remove Liquidity**: An LP can remove some or all of their liquidity from a specific price range.\n",
    "   - **Adjust Range**: An LP can adjust the price range for their existing liquidity, effectively moving their liquidity to a different range.\n",
    "   - **Swap**: Traders can swap one token for another, which will change the pool's price and reserves.\n",
    "\n",
    "3. **Reward Structure**:\n",
    "   - **Trading Fees**: LPs earn fees from trades that occur within their specified price range. The reward for an LP at any given moment is proportional to the amount of liquidity they've provided relative to the total liquidity in the current price range.\n",
    "   - **Impermanent Loss**: LPs can experience impermanent loss when the price of the pool diverges significantly from the price when they provided liquidity. This should be modeled as a cost or penalty that reduces the LP's overall reward.\n",
    "\n",
    "4. **Market Dynamics**:\n",
    "   - **Price Movement**: In Uniswap v3, the price movement is determined by the pool's reserves and the x * y = k invariant. When a trade occurs, the reserves of the tokens change, which in turn changes the pool's price.\n",
    "   - **Liquidity Activation/Deactivation**: As the pool's price moves, different LPs' liquidity will become active or inactive. Only the liquidity within the current price range earns fees.\n",
    "\n",
    "5. **Transitions**:\n",
    "   - When a trade occurs, the pool's price will move, potentially activating or deactivating certain LPs' liquidity.\n",
    "   - When an LP adds, removes, or adjusts liquidity, the liquidity depth in the affected price ranges will change.\n",
    "\n",
    "6. **Environment Dynamics**:\n",
    "   - External factors, such as significant market news or events, can lead to sudden surges in trading activity, affecting the pool's price and the rewards for LPs.\n",
    "   - The overall demand and supply for the tokens in the pool can also influence trading activity and price movement.\n",
    "\n",
    "This model captures the essential dynamics of liquidity provisioning in Uniswap v3. It can be used as a foundation for building a reinforcement learning environment or for other types of simulations and analyses.\n",
    "\n",
    "# Challenges\n",
    "1. Feature extraction from noisy pool data\n",
    "2. Defining an action space for agent (discrete, continuous)\n",
    "3. Design a reward function which optimizes LP's objective function\n",
    "\n",
    "## ILP_n_m\n",
    "\n",
    "#### Objective Function:\n",
    "The objective function would ideally maximize the rewards for the LP based on his preferences. This could be a combination of maximizing fees, minimizing impermanent loss, optimizing for long-term vs short-term gains, or any other preference the LP might have. The objective function can be represented as:\n",
    "\n",
    "\\[\n",
    "\\text{Objective}(S, A) = w_1 \\times \\text{Fees}(S, A) - w_2 \\times \\text{ImpermanentLoss}(S, A) + w_3 \\times \\text{PreferenceScore}(S, A)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( S \\) is the state (all relevant data about the pools and market)\n",
    "- \\( A \\) is the action taken by the LP\n",
    "- \\( w_1, w_2, w_3 \\) are weights that can be adjusted based on the LP's preferences.\n",
    "\n",
    "#### State Space:\n",
    "1. **Pool Data**: This would include current and historical data for all Uniswap pools. Features could include the current price, liquidity concentration, volume, fees collected, recent price movements, etc.\n",
    "2. **Market Data**: Broader market data can provide context. This can include overall market sentiment, price movements of major crypto assets, news or events that might affect the crypto market, etc.\n",
    "3. **Agent's Inventory**: Current holdings of the LP, including assets not yet committed to any pool.\n",
    "\n",
    "#### Action Space:\n",
    "1. **Select Pool**: Choose which pool to provide liquidity to.\n",
    "2. **Select Amount**: Decide the amount of assets to provide as liquidity.\n",
    "3. **Select Tick Range**: Define the price range within which the LP wants to earn fees.\n",
    "4. **Re-adjust Position**: Move the liquidity to a different tick range based on changing market conditions.\n",
    "5. **Do Nothing**: Sometimes, the best action might be to maintain the current position without making changes.\n",
    "6. **Sit Idle**: Decide to not participate in liquidity provisioning, either waiting for a better opportunity or based on some other strategy.\n",
    "\n",
    "#### Transition Dynamics:\n",
    "1. **Market Movements**: After every action, the market might move. Prices could change based on trades, liquidity might be added or removed by other LPs, etc.\n",
    "2. **Fee Accumulation**: Fees are accumulated based on trades happening within the selected tick range.\n",
    "3. **Impermanent Loss**: If prices move out of the chosen tick range, the LP might experience impermanent loss.\n",
    "\n",
    "#### Rewards:\n",
    "The reward at each step would be based on the objective function. It would factor in the fees earned, any impermanent loss experienced, and any other preferences the LP might have.\n",
    "\n",
    "#### Initialization:\n",
    "1. The agent starts with a defined inventory.\n",
    "2. Initial data about all pools and the broader market is loaded.\n",
    "\n",
    "#### Termination:\n",
    "The environment could run indefinitely, or you could set conditions for termination, such as a certain amount of profit/loss, a maximum number of steps, etc.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "You could use reinforcement learning frameworks like OpenAI Gym or Ray Rllib to implement this. The state space might be high dimensional given the breadth of data, so deep reinforcement learning techniques, possibly combined with other techniques like feature engineering, feature selection, or dimensionality reduction, might be required.\n",
    "\n",
    "Another critical aspect would be the simulation environment. Given the complexity and unpredictability of crypto markets, having a realistic simulation environment is essential for training the agent before deploying in the real world. You can use historical data to replay market scenarios or even integrate with platforms like cadCAD for more sophisticated simulations. \n",
    "\n",
    "Finally, continuous learning is crucial. The agent should periodically retrain or adapt its strategy based on new data and changing market conditions.\n",
    "Certainly, starting with a simpler model allows for easier debugging and understanding of the dynamics, while also providing a foundation to progressively incorporate more advanced features. Let's define the basic model, **ILP1_1**:\n",
    "\n",
    "### ILP1_1: Basic Intelligent Liquidity Provisioning Model\n",
    "\n",
    "#### Objective Function:\n",
    "The objective is simply to maximize the fees collected by the LP over the duration of the strategy (30 days). The objective function can be represented as:\n",
    "\\[\n",
    "\\text{Objective}(S, A) = \\text{Fees}(S, A)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( S \\) is the state (data about the pool).\n",
    "- \\( A \\) is the action taken by the LP (upper tick and lower tick).\n",
    "\n",
    "#### State Space:\n",
    "1. **Current Price**: The current price of the asset in the pool.\n",
    "2. **Price Volatility**: Measure of price fluctuations in the pool over a defined period.\n",
    "3. **TVL (Total Value Locked)**: Total assets locked in the pool.\n",
    "4. **VolumeUSD**: Trading volume in the pool over a defined period.\n",
    "5. **FeesUSD**: Fees collected in the pool over a defined period.\n",
    "6. **n_LPs**: Number of liquidity providers in the pool.\n",
    "\n",
    "#### Action Space:\n",
    "#### Actions:\n",
    "The agent's action is to determine the optimal upper and lower ticks for the liquidity position. Given that the tick represents a specific price, the agent's action space is essentially choosing two prices: one as the lower bound and one as the upper bound of the price range where it wants to provide liquidity.\n",
    "\n",
    "1. **Upper Tick**: Define the upper price boundary within which the LP wants to earn fees.\n",
    "2. **Lower Tick**: Define the lower price boundary within which the LP wants to earn fees.\n",
    "\n",
    "#### Transition Dynamics:\n",
    "1. **Market Movements**: After every action, the market might move. Prices could change based on trades, liquidity might be added or removed by other LPs, etc.\n",
    "2. **Fee Accumulation**: Fees are accumulated based on trades happening within the selected tick range.\n",
    "\n",
    "#### Rewards:\n",
    "The reward at each step would be the fees earned within the selected tick range.\n",
    "\n",
    "#### Initialization:\n",
    "1. The agent starts with a defined liquidity amount.\n",
    "2. Initial data about the pool is loaded.\n",
    "\n",
    "#### Constraints:\n",
    "1. **Pool**: Only data from one pool is considered.\n",
    "2. **Liquidity Amount**: The amount of liquidity provided is fixed.\n",
    "3. **Positions**: Only one position can be opened at a time.\n",
    "4. **Sitting Idle**: The agent always has to open a position.\n",
    "5. **Max Time**: Each position is opened for a magit ximum of 7 days.\n",
    "6. **Re-adjusting**: The position can be readjusted after every 6 hours.\n",
    "7. **Strategy Termination**: The overall strategy terminates after 30 days.\n",
    "\n",
    "#### Agent:\n",
    "A Deep Q-Network (DQN) agent will be used to learn the optimal policy for liquidity provisioning.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "1. **Environment Definition**: Define a custom RL environment using frameworks like OpenAI Gym. The environment should be able to simulate the pool's behavior based on the agent's actions and provide rewards accordingly.\n",
    "2. **Agent Definition**: Define the DQN agent with an appropriate neural network architecture. Given the relatively low dimensionality of the state and action spaces in this basic model, a simple feed-forward neural network might suffice.\n",
    "3. **Training**: Train the agent using historical data. The agent takes actions, the environment simulates the outcome, and the agent learns from the rewards.\n",
    "4. **Evaluation**: Periodically evaluate the agent's performance using a separate validation dataset or by simulating scenarios.\n",
    "5. **Deployment**: Once satisfied with the agent's performance, deploy it in a live or paper-trading environment.\n",
    "#### Termination:\n",
    "The environment can run indefinitely, or you can set a termination condition, such as a maximum number of steps or once a specific amount of fees has been earned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation Space\n",
    "1. Agent state observation\n",
    "2. Environment state observation\n",
    "### Model Deployment\n",
    "\n",
    "#### Get Real time data of Unsiwap pool\n",
    "Use Uniswap subgraph to get data of pool for state representation\n",
    "\n",
    "#### Safety Mechanisms:\n",
    "Implement mechanisms to prevent any extreme or unwanted actions. For instance:\n",
    "Double-check the predicted ticks to ensure they're within expected bounds.\n",
    "Set maximum/minimum liquidity limits.\n",
    "Have alerts in place if the agent's action deviates significantly from expected behavior.\n",
    "## Continuous Learning\n",
    "Agent is trained once and then it uses its learned policy to make decisions.\n",
    "\n",
    "If you want the agent to learn continuously in a live environment (also known as online learning), you'll need to:\n",
    "\n",
    "Collect Data: Continuously store the states, actions, rewards, and next states the agent encounters in the live environment.\n",
    "\n",
    "Update the Model: Periodically (or after a set number of new data points) use this data to update the model. This can be done by running the learning function of the DQN agent on this new data.\n",
    "\n",
    "Safety Considerations: Online learning in a financial environment can be risky. The agent may encounter states that it hasn't seen before and could react unpredictably. Additionally, if there's any feedback loop where the agent's actions influence future states significantly, the agent might inadvertently \"game\" the system in a way that's not truly beneficial in the long run.\n",
    "\n",
    "Model Stability: Continuous learning can sometimes make the model unstable if the environment changes drastically or if there's noise in the data. Techniques like Experience Replay or Target Networks (common in DQN architectures) can help mitigate these issues.\n",
    "\n",
    "Evaluation: It's crucial to have a mechanism to evaluate the agent's performance over time. You might want to periodically test the agent in a simulated environment to ensure its updated policy is still sound. If performance degrades, you might revert to a previous model or retrain from scratch.\n",
    "\n",
    "Human Oversight: Especially in the beginning, it's essential to monitor the agent's decisions closely to ensure they are logical and align with your goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "    https://kth.diva-portal.org/smash/get/diva2:1695877/FULLTEXT01.pdf\n",
    "    https://arxiv.org/pdf/2305.15821.pdf\n",
    "    https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/tree/main\n",
    "    https://arxiv.org/ftp/arxiv/papers/2211/2211.01346.pdf\n",
    "    https://arxiv.org/pdf/2004.06985.pdf\n",
    "    https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9682687\n",
    "    https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0277042\n",
    "    https://deliverypdf.ssrn.com/delivery.php?ID=104119098102026014120072084014107007042068069003049020126088025087121115103007084028042013055035009000054122074096068089064070102052026003014069082076098016080066026088066039027093020006122067093104092065070020126069068106118079127088008098077106031120&EXT=pdf&INDEX=TRUE\n",
    "\n",
    "\n",
    "https://huggingface.co/learn/deep-rl-course/unit2/two-types-value-based-methods\n",
    "https://github.com/javifalces/HFTFramework/blob/main/python/trading_algorithms/dqn_algorithm.py\n",
    "https://github.com/hudson-and-thames/mlfinlab\n",
    "https://github.com/AminHP/gym-mtsim\n",
    "https://github.com/AminHP/gym-anytrading/tree/master\n",
    "\n",
    "\n",
    "# Observations and Potential Improvements in V1.2\n",
    "\n",
    "Market Dynamics Simulation: The current step function simulates market dynamics using random noise. Incorporating a more sophisticated market model or real market data would enhance realism and robustness.\n",
    "\n",
    "Reward Function Refinement: The reward function could be refined to better capture the complexities of liquidity provisioning, such as impermanent loss, slippage, and other factors that influence LPs' profits.\n",
    "\n",
    "Action Space Definition: The action space currently allows for continuous values for ticks. Depending on the specific implementation of Uniswap V3, you might want to constrain the action space to discrete ticks or include additional actions like adding/removing liquidity.\n",
    "\n",
    "Exploration Strategy: The epsilon-greedy strategy is used for exploration. Depending on the complexity of the environment, other exploration techniques like entropy regularization or Upper Confidence Bound (UCB) might be worth exploring.\n",
    "\n",
    "Network Architecture and Hyperparameters: Tuning the neural network architecture, learning rate, discount factor, and other hyperparameters could lead to better performance.\n",
    "\n",
    "Utilizing Advanced RL Algorithms: Given your familiarity with stable-baseline3 and other RL libraries, you might consider experimenting with more advanced algorithms like Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), or others that might be more suitable for this problem.\n",
    "\n",
    "Incorporating Research Insights: Since you've studied various research papers, you might have come across specific techniques, models, or insights that could be integrated into this framework to enhance its performance.\n",
    "\n",
    "# State representation\n",
    "## Features Engineering\n",
    "\n",
    "### 1. **Price Statistics**:\n",
    "   - **Current Price**: The current price of the trading pair.\n",
    "   - **Price Volatility**: Standard deviation of price changes over a recent time window.\n",
    "   - **Price Trend**: Slope of a linear regression on recent price data, indicating the direction and strength of the trend.\n",
    "\n",
    "### 2. **Liquidity Distribution**:\n",
    "   - **Total Liquidity**: Total liquidity across all tick ranges.\n",
    "   - **Liquidity Concentration**: Measures of how liquidity is concentrated across different tick ranges (e.g., Gini coefficient, entropy).\n",
    "   - **Liquidity in Current Range**: Liquidity within the current price range.\n",
    "\n",
    "### 3. **Trading Volume Statistics**:\n",
    "   - **Total Volume**: Total trading volume over a recent time window.\n",
    "   - **Volume Distribution**: Distribution of trading volume across different tick ranges.\n",
    "   - **Volume Momentum**: Rate of change in trading volume.\n",
    "\n",
    "### 4. **Swap Data**:\n",
    "   - **Swap Frequency**: Number of swaps occurring in a given time window.\n",
    "   - **Average Swap Size**: Average size of swaps over a recent time window.\n",
    "   - **Swap Direction**: Ratio of buy to sell swaps, indicating market sentiment.\n",
    "\n",
    "### 5. **Fee Metrics**:\n",
    "   - **Total Fees Earned**: Total fees earned by LPs over a recent time window.\n",
    "   - **Fee Rate**: The fee rate associated with the current liquidity range.\n",
    "\n",
    "### 6. **External Market Indicators**:\n",
    "   - **Correlated Asset Prices**: Prices of other assets that are correlated with the trading pair.\n",
    "   - **Market Sentiment Indicators**: Metrics derived from social media, news, or other sources that may influence trading behavior.\n",
    "\n",
    "### 7. **Historical Data and Moving Averages**:\n",
    "   - **Moving Averages**: Various moving averages of price, volume, etc., over different time windows.\n",
    "   - **Historical Price Levels**: Past support and resistance levels that might influence trading behavior.\n",
    "\n",
    "### 8. **Network Metrics**:\n",
    "   - **Gas Prices**: Current Ethereum gas prices, which may influence trading and liquidity provisioning behavior.\n",
    "   - **Transaction Queue**: Information about the pending transaction queue on the Ethereum network.\n",
    "\n",
    "### Conclusion\n",
    "These engineered features aim to capture various aspects of the market dynamics, liquidity provisioning behavior, and external factors that might influence Uniswap V3. The selection and design of these features would depend on the specific problem you are addressing and may require experimentation, validation, and possibly feature scaling or normalization.\n",
    "\n",
    "Keep in mind that feature engineering is both an art and a science, and it often benefits from domain expertise, iterative experimentation, and a deep understanding of the underlying mechanics of Uniswap V3.\n",
    "\n",
    "Calculate the Reward: Based on the final state, calculate the reward for your agent. This should reflect the goals and constraints of your agent within the Uniswap pool.\n",
    "\n",
    "Proceed to the Next Timestep: Continue to the next timestep, repeating the process.\n",
    "\n",
    "By modeling both types of actions, you create a more realistic simulation environment that accounts for the complex dynamics of the Uniswap pool. This can help your agent learn more robust and effective strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\tokenspice\\ILP_mechanism.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function_pb2\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m rewriter_config_pb2\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# source: tensorflow/core/framework/function.proto\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m builder \u001b[39mas\u001b[39;00m _builder\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m descriptor \u001b[39mas\u001b[39;00m _descriptor\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m descriptor_pool \u001b[39mas\u001b[39;00m _descriptor_pool\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression  # Replace with appropriate ML model\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gym import spaces\n",
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'liquidity_ranges': [(444741764.1356279, 454634004.0177212, 0.0), (412607821.9483857, 425172963.1539906, 0.0), (384714329.63756645, 434629940.8477182, 0.0), (314037123.69593394, 499939933.7975011, 0.0), (410960769.3130819, 442523715.9566586, 0.0), (415921756.8963616, 439876655.13499963, 0.0), (427304044.68175, 428159465.1361321, 0.0), (426024117.39383185, 426450333.27321035, 0.0), (350901535.2019599, 512595332.7747065, 0.0), (421785317.4774468, 422207292.6489402, 0.0), (419262308.3170634, 419681759.3437395, 0.0), (406871844.7581571, 416337865.8679679, 0.0), (414675923.600571, 415506063.80495846, 746755412055405.0), (363036709.6754226, 487596982.30670816, 0.0), (412607821.9483857, 413020615.4933756, 0.0), (394847556.70360464, 428587817.3244157, 0.0), (333455094.61223024, 499939933.7975011, 0.0), (346023384.3408586, 489551178.31431454, 0.0), (406465196.6034308, 406871844.7581571, 0.0), (407686361.9682095, 408094231.83797157, 0.0), (410550034.4818099, 410960769.3130819, 0.0), (310601815.92807925, 553069078.2252173, 0.0), (404842664.1704206, 405247689.0623795, 0.0), (409729795.93603504, 411783472.1458642, 0.0), (378986963.68352336, 426450333.27321035, 0.0), (409729795.93603504, 410550034.4818099, 1147770206691903.0), (404438044.080679, 406058954.8732922, 424633036024159.0), (391701534.41554964, 428159465.1361321, 25754023399426.0), (315926915.65688, 459662331.75259376, 0.0), (391701534.41554964, 428159465.1361321, 24834292629283.0), (403226608.58098304, 435935723.0762933, 0.0), (391701534.41554964, 428159465.1361321, 26203807977410.0), (406871844.7581571, 408911196.1461132, 0.0), (405653119.16154015, 468478968.0573675, 0.0), (406058954.8732922, 413433822.0177171, 0.0), (276584825.47555494, 410139710.15955544, 0.0), (333455094.61223024, 499939933.7975011, 0.0), (204084220.82227266, 825891047.0094538, 0.0), (408502509.76119375, 420521920.7364103, 0.0), (349500802.4297178, 427304044.68175, 0.0), (384329826.81620055, 454634004.0177212, 0.0), (406465196.6034308, 417171333.11647594, 0.0), (333121822.84458375, 499939933.7975011, 0.0), (384714329.63756645, 434629940.8477182, 0.0), (405247689.0623795, 409729795.93603504, 0.0), (307819079.81423, 413020615.4933756, 0.0), (400013824.6186782, 444297266.88165116, 0.0), (409320291.40137553, 409729795.93603504, 0.0), (277693324.8742835, 285578217.1102181, 194408905116698.0), (389358478.5562866, 459662331.75259376, 0.0), (333121822.84458375, 455088842.6616064, 0.0), (370000000.7989004, 416754391.13584477, 0.0), (169447315.4488524, 476034537.1501413, 0.0), (406465196.6034308, 411371915.0640651, 0.0), (408094231.83797157, 499939933.7975011, 0.0), (365952505.19745237, 451011621.94832647, 0.0), (332788884.1654787, 500440098.7642721, 0.0), (323924238.72508526, 455544136.34886736, 0.0), (285578217.1102181, 476034537.1501413, 0.0), (393664848.1178033, 417588692.22676164, 0.0), (400013824.6186782, 416337865.8679679, 0.0), (407686361.9682095, 408502509.76119375, 0.0), (406871844.7581571, 408502509.76119375, 0.0), (398815641.2266738, 413433822.0177171, 0.0), (401617003.9807799, 402018801.76061493, 0.0), (387416641.89067346, 439437020.31528354, 0.0), (285863923.87180144, 645790507.6685557, 0.0), (390137935.7385658, 408094231.83797157, 0.0), (398815641.2266738, 399214636.3828053, 0.0), (396033818.56475085, 396826639.11780614, 0.0), (395242581.9890988, 395638002.4776872, 0.0), (394847556.70360464, 395242581.9890988, 0.0), (393664848.1178033, 395242581.9890988, 0.0), (395242581.9890988, 416754391.13584477, 0.0), (333455094.61223024, 476034537.1501413, 0.0), (391701534.41554964, 392093412.2626681, 0.0), (333455094.61223024, 476034537.1501413, 0.0), (394058690.1623508, 394452926.226219, 0.0), (359065386.7500098, 555841209.5654545, 0.0), (324248308.76859546, 394058690.1623508, 0.0), (392878344.5118533, 393271399.6987738, 0.0), (393664848.1178033, 400013824.6186782, 0.0), (392878344.5118533, 400013824.6186782, 0.0), (346023384.3408586, 552516313.2133495, 0.0), (229872959.47452414, 552516313.2133495, 0.0), (392485682.1640257, 465211229.0941518, 0.0), (359424613.7592792, 394058690.1623508, 0.0), (326199548.92787856, 367786757.82273304, 0.0), (318782947.9667224, 445186706.08693576, 0.0), (375216167.98350513, 417171333.11647594, 0.0), (315926915.65688, 421363764.0491314, 0.0), (375216167.98350513, 417171333.11647594, 0.0), (390528249.28320014, 398815641.2266738, 0.0), (390137935.7385658, 394452926.226219, 0.0), (370000000.7989004, 408094231.83797157, 0.0), (375216167.98350513, 445186706.08693576, 0.0), (388969334.13926244, 389358478.5562866, 0.0), (382795654.60614806, 397621046.81923074, 0.0), (377474119.1757941, 399614030.7136886, 0.0), (324248308.76859546, 481299558.342476, 0.0)], 'pool_price': 1847.2303559142795, 'volume': 390955216143.0419, 'fees': 195477608.07152092, 'reserves': {'token0': 221039056.236998, 'token1': 133620.27064191128}, 'recent_swaps': [(54566.215723, -29.525362225405647, 54557.395493533324)]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "UNISWAP_V3_SUBGRAPH_URL = 'https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3'\n",
    "\n",
    "def fetch_pool_data(pool_id):\n",
    "    # Fetching liquidity positions\n",
    "    liquidity_query = \"\"\"\n",
    "    {\n",
    "      positions(where: { pool: \"%s\" }) {\n",
    "        liquidity\n",
    "        tickLower {\n",
    "          price0\n",
    "        }\n",
    "        tickUpper {\n",
    "          price0\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    liquidity_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': liquidity_query}).json()\n",
    "\n",
    "    # Fetching pool price, volume, fees, and reserves\n",
    "    pool_query = \"\"\"\n",
    "    {\n",
    "      pools(where: { id: \"%s\" }) {\n",
    "        token0Price\n",
    "        token1Price\n",
    "        volumeUSD\n",
    "        feesUSD\n",
    "        totalValueLockedToken0\n",
    "        totalValueLockedToken1\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    pool_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': pool_query}).json()\n",
    "\n",
    "    # Fetching swaps and trades\n",
    "    swaps_query = \"\"\"\n",
    "    {\n",
    "      swaps(where: { pool: \"%s\" }, first: 10, orderBy: timestamp, orderDirection: desc) {\n",
    "        amount0\n",
    "        amount1\n",
    "        amountUSD\n",
    "        timestamp\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    swaps_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': swaps_query}).json()\n",
    "\n",
    "        # Constructing the state representation with float data type\n",
    "    state = {\n",
    "        'liquidity_ranges': [(float(position['tickLower']['price0']), float(position['tickUpper']['price0']), float(position['liquidity'])) for position in liquidity_data['data']['positions']],\n",
    "        'pool_price': float(pool_data['data']['pools'][0]['token0Price']),\n",
    "        'volume': float(pool_data['data']['pools'][0]['volumeUSD']),\n",
    "        'fees': float(pool_data['data']['pools'][0]['feesUSD']),\n",
    "        'reserves': {\n",
    "            'token0': float(pool_data['data']['pools'][0]['totalValueLockedToken0']),\n",
    "            'token1': float(pool_data['data']['pools'][0]['totalValueLockedToken1'])\n",
    "        },\n",
    "        'recent_swaps': [(float(swap['amount0']), float(swap['amount1']), float(swap['amountUSD'])) for swap in swaps_data['data']['swaps']]\n",
    "    }\n",
    "\n",
    "    return state\n",
    "# Example usage\n",
    "pool_id = \"0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640\"\n",
    "state_representation = fetch_pool_data(pool_id)\n",
    "print(state_representation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(400, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(300, activation='relu')\n",
    "        self.mu = tf.keras.layers.Dense(n_actions, activation='tanh')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        mu = self.mu(x)\n",
    "        return mu\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(400, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(300, activation='relu')\n",
    "        self.q = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action], axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.q(x)\n",
    "        return q\n",
    "    \n",
    "class DDPG:\n",
    "    def __init__(self, alpha=0.001, beta=0.002, input_dims=[8], tau=0.005, env=None,\n",
    "                 gamma=0.99, n_actions=2, max_size=1000000, layer1_size=400, \n",
    "                 layer2_size=300, batch_size=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = Actor(n_actions=n_actions)\n",
    "        self.critic = Critic(n_actions=n_actions)\n",
    "\n",
    "        self.target_actor = Actor(n_actions=n_actions)\n",
    "        self.target_critic = Critic(n_actions=n_actions)\n",
    "\n",
    "        self.actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=beta))\n",
    "        self.target_actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.target_critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=beta))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_actor.weights\n",
    "        for i, weight in enumerate(self.actor.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_actor.set_weights(weights)\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_critic.weights\n",
    "        for i, weight in enumerate(self.critic.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        actions = self.actor(state)\n",
    "\n",
    "        return actions[0].numpy()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                      self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(states_)\n",
    "            critic_value_ = tf.squeeze(self.target_critic(\n",
    "                                states_, target_actions), 1)\n",
    "            critic_value = tf.squeeze(self.critic(states, actions), 1)\n",
    "            target = reward + self.gamma*critic_value_*(1-done)\n",
    "            critic_loss = tf.keras.losses.MSE(target, critic_value)\n",
    "\n",
    "        critic_network_gradient = tape.gradient(critic_loss, \n",
    "                                            self.critic.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(\n",
    "            critic_network_gradient, self.critic.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_policy_actions = self.actor(states)\n",
    "            actor_loss = -self.critic(states, new_policy_actions)\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, \n",
    "                                    self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(\n",
    "            actor_network_gradient, self.actor.trainable_variables))\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = UniswapV3Environment(1000,10)\n",
    "n_actions = env.action_space.shape[0]\n",
    "input_dims = env.observation_space.shape\n",
    "agent = DDPG(alpha=0.001, beta=0.002, input_dims=input_dims, tau=0.005, env=env,\n",
    "             n_actions=n_actions, layer1_size=400, layer2_size=300, batch_size=64)\n",
    "\n",
    "n_episodes = 1000\n",
    "max_steps = 500\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.choose_action(state)\n",
    "        # Introduce exploration noise to the action here if needed\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {i+1}: Reward = {episode_reward}\")\n",
    "\n",
    "# After training, save your agent if needed\n",
    "# agent.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 1\n",
      "Episode: 100, Total Reward: 1\n",
      "Episode: 200, Total Reward: 1\n",
      "Episode: 300, Total Reward: 1\n",
      "Episode: 400, Total Reward: 1\n",
      "Episode: 500, Total Reward: 1\n",
      "Episode: 600, Total Reward: 1\n",
      "Episode: 700, Total Reward: 1\n",
      "Episode: 800, Total Reward: 1\n",
      "Episode: 900, Total Reward: 1\n",
      "{(50, 0): 0.0, (60, 0): 0.0, (70, 0): 0.0, (80, 0): 0.0, (90, 0): 0.0, (100, 0): 0.0, (110, 0): 0.0, (120, 0): 0.0, (130, 0): 0.0, (140, 0): 0.0, (150, 0): 0.0, (160, 0): 0.0, (170, 1): 0.0, (170, 0): 0.0, (180, 0): 0.0, (190, 0): 0.0, (200, 0): 0.0, (210, 0): 0.0, (220, 0): 0.0, (230, 0): 0.0, (240, 0): 0.0, (250, 0): 0.0, (260, 0): 0.0, (270, 0): 0.0, (280, 0): 0.0, (290, 0): 0.0, (300, 0): 0.0, (310, 0): 0.0, (320, 0): 0.0, (330, 0): 0.0, (340, 0): 0.0, (350, 0): 0.0, (360, 0): 0.0, (370, 0): 0.0, (380, 0): 0.0, (390, 0): 0.0, (400, 0): 0.0, (410, 0): 0.0, (420, 0): 0.0, (430, 0): 0.0, (440, 0): 0.0, (450, 0): 0.0, (460, 0): 0.0, (470, 0): 0.0, (480, 0): 0.0, (490, 0): 0.0, (500, 0): 0.0, (510, 0): 0.0, (520, 1): 0.0, (520, 0): 0.0, (530, 0): 0.0, (540, 0): 0.0, (550, 0): 0.0, (560, 0): 0.0, (570, 0): 0.0, (580, 0): 0.0, (590, 0): 0.0, (600, 0): 0.0, (610, 0): 0.0, (620, 0): 0.0, (630, 0): 0.0, (640, 0): 0.0, (650, 0): 0.0, (660, 0): 0.0, (670, 1): 0.0, (670, 0): 0.0, (680, 0): 0.0, (690, 0): 0.0, (700, 0): 0.0, (710, 0): 0.0, (720, 0): 0.0, (730, 0): 0.0, (740, 0): 0.0, (750, 0): 0.0, (760, 1): 0.0, (760, 0): 0.0, (770, 0): 0.0, (780, 0): 0.0, (790, 1): 0.0, (790, 0): 0.0, (800, 0): 0.0, (810, 0): 0.0, (820, 0): 0.0, (830, 0): 0.0, (840, 0): 0.0, (850, 0): 0.0, (860, 0): 0.0, (870, 0): 0.0, (880, 0): 0.0, (890, 0): 0.0, (900, 0): 0.0, (910, 0): 0.0, (920, 0): 0.0, (930, 0): 0.0, (940, 0): 0.0, (950, 0): 0.0, (960, 0): 0.0, (970, 0): 0.0, (980, 1): 0.0, (980, 0): 0.0, (990, 0): 0.0, (1000, 0): 0.0, (1010, 0): 0.0, (1020, 0): 0.0, (1030, 0): 0.0, (1040, 0): 0.0, (1050, 0): 0.0, (1060, 0): 0.0, (1070, 0): 0.0, (1080, 0): 0.0, (1090, 0): 0.0, (1100, 0): 0.0, (1110, 0): 0.0, (1120, 0): 0.0, (1130, 1): 0.0, (1130, 0): 0.0, (1140, 0): 0.0, (1150, 0): 0.0, (1160, 0): 0.0, (1170, 0): 0.0, (1180, 0): 0.0, (1190, 0): 0.0, (1200, 0): 0.0, (1210, 0): 0.0, (1220, 0): 0.0, (1230, 0): 0.0, (1240, 0): 0.0, (1250, 0): 0.0, (1260, 0): 0.0, (1270, 0): 0.0, (1280, 1): 0.0, (1280, 0): 0.0, (1290, 0): 0.0, (1300, 0): 0.0, (1310, 0): 0.0, (1320, 0): 0.0, (1330, 0): 0.0, (1340, 0): 0.0, (1350, 0): 0.0, (1360, 0): 0.0, (1370, 0): 0.0, (1380, 0): 0.0, (1390, 0): 0.0, (1400, 0): 0.0, (1410, 0): 0.0, (1420, 0): 0.0, (1430, 0): 0.0, (1440, 0): 0.0, (1450, 0): 0.0, (1460, 0): 0.0, (1470, 0): 0.0, (1480, 0): 0.0, (1490, 0): 0.0, (1500, 0): 0.0, (1510, 0): 0.0, (1520, 0): 0.0, (1530, 0): 0.0, (1540, 1): 0.0, (1540, 0): 0.0, (1550, 0): 0.0, (1560, 0): 0.0, (1570, 0): 0.0, (1580, 1): 0.0, (1580, 0): 0.0, (1590, 0): 0.0, (1600, 0): 0.0, (1610, 0): 0.0, (1620, 0): 0.0, (1630, 0): 0.0, (1640, 0): 0.0, (1650, 0): 0.0, (1660, 0): 0.0, (1670, 0): 0.0, (1680, 1): 0.0, (1680, 0): 0.0, (1690, 0): 0.0, (1700, 0): 0.0, (1710, 0): 0.0, (1720, 0): 0.0, (1730, 0): 0.0, (1740, 0): 0.0, (1750, 0): 0.0, (1760, 0): 0.0, (1770, 0): 0.0, (1780, 0): 0.0, (1790, 0): 0.0, (1800, 0): 0.0, (1810, 0): 0.0, (1820, 0): 0.0, (1830, 0): 0.0, (1840, 0): 0.0, (1850, 0): 0.0, (1860, 0): 0.0, (1870, 0): 0.0, (1880, 0): 0.0, (1890, 0): 0.0, (1900, 0): 0.0, (1910, 0): 0.0, (1920, 0): 0.0, (1930, 0): 0.0, (1940, 0): 0.0, (1950, 0): 0.0, (1960, 1): 0.0, (1960, 0): 0.0, (1970, 0): 0.0, (1980, 0): 0.0, (1990, 0): 0.0, (2000, 0): 0.0, (2010, 0): 0.0, (2020, 0): 0.0, (2030, 0): 0.0, (2040, 0): 0.0, (2050, 0): 0.0, (2060, 0): 0.0, (2070, 0): 0.0, (2080, 0): 0.0, (2090, 0): 0.0, (2100, 0): 0.0, (2110, 0): 0.0, (2120, 0): 0.0, (2130, 0): 0.0, (2140, 0): 0.0, (2150, 0): 0.0, (2160, 0): 0.0, (2170, 0): 0.0, (2180, 0): 0.0, (2190, 0): 0.0, (2200, 1): 0.0, (2200, 0): 0.0, (2210, 0): 0.0, (2220, 0): 0.0, (2230, 0): 0.0, (2240, 0): 0.0, (2250, 1): 0.0, (2250, 0): 0.0, (2260, 0): 0.0, (2270, 0): 0.0, (2280, 0): 0.0, (2290, 0): 0.0, (2300, 1): 0.0, (2300, 0): 0.0, (2310, 0): 0.0, (2320, 0): 0.0, (2330, 0): 0.0, (2340, 0): 0.0, (2350, 0): 0.0, (2360, 0): 0.0, (2370, 0): 0.0, (2380, 0): 0.0, (2390, 0): 0.0, (2400, 0): 0.0, (2410, 0): 0.0, (2420, 0): 0.0, (2430, 1): 0.0, (2430, 0): 0.0, (2440, 0): 0.0, (2450, 0): 0.0, (2460, 0): 0.0, (2470, 0): 0.0, (2480, 0): 0.0, (2490, 0): 0.0, (2500, 0): 0.0, (2510, 0): 0.0, (2520, 0): 0.0, (2530, 0): 0.0, (2540, 0): 0.0, (2550, 0): 0.0, (2560, 0): 0.0, (2570, 0): 0.0, (2580, 0): 0.0, (2590, 0): 0.0, (2600, 0): 0.0, (2610, 0): 0.0, (2620, 0): 0.0, (2630, 0): 0.0, (2640, 0): 0.0, (2650, 0): 0.0, (2660, 0): 0.0, (2670, 0): 0.0, (2680, 0): 0.0, (2690, 0): 0.0, (2700, 0): 0.0, (2710, 0): 0.0, (2720, 0): 0.0, (2730, 0): 0.0, (2740, 0): 0.0, (2750, 0): 0.0, (2760, 1): 0.0, (2760, 0): 0.0, (2770, 0): 0.0, (2780, 0): 0.0, (2790, 0): 0.0, (2800, 0): 0.0, (2810, 0): 0.0, (2820, 0): 0.0, (2830, 0): 0.0, (2840, 0): 0.0, (2850, 0): 0.0, (2860, 1): 0.0, (2860, 0): 0.0, (2870, 0): 0.0, (2880, 0): 0.0, (2890, 0): 0.0, (2900, 0): 0.0, (2910, 0): 0.0, (2920, 0): 0.0, (2930, 0): 0.0, (2940, 0): 0.0, (2950, 0): 0.0, (2960, 0): 0.0, (2970, 0): 0.0, (2980, 0): 0.0, (2990, 0): 0.0, (3000, 0): 0.0, (3010, 0): 0.0, (3020, 1): 0.0, (3020, 0): 0.0, (3030, 0): 0.0, (3040, 1): 0.0, (3040, 0): 0.0, (3050, 0): 0.0, (3060, 0): 0.0, (3070, 0): 0.0, (3080, 0): 0.0, (3090, 0): 0.0, (3100, 0): 0.0, (3110, 0): 0.0, (3120, 0): 0.0, (3130, 0): 0.0, (3140, 0): 0.0, (3150, 0): 0.0, (3160, 0): 0.0, (3170, 0): 0.0, (3180, 0): 0.0, (3190, 0): 0.0, (3200, 0): 0.0, (3210, 0): 0.0, (3220, 0): 0.0, (3230, 0): 0.0, (3240, 0): 0.0, (3250, 0): 0.0, (3260, 0): 0.0, (3270, 0): 0.0, (3280, 0): 0.0, (3290, 0): 0.0, (3300, 0): 0.0, (3310, 0): 0.0, (3320, 0): 0.0, (3330, 0): 0.0, (3340, 0): 0.0, (3350, 0): 0.0, (3360, 0): 0.0, (3370, 0): 0.0, (3380, 0): 0.0, (3390, 0): 0.0, (3400, 1): 0.0, (3400, 0): 0.0, (3410, 0): 0.0, (3420, 0): 0.0, (3430, 0): 0.0, (3440, 0): 0.0, (3450, 1): 0.0, (3450, 0): 0.0, (3460, 0): 0.0, (3470, 0): 0.0, (3480, 0): 0.0, (3490, 0): 0.0, (3500, 0): 0.0, (3510, 0): 0.0, (3520, 0): 0.0, (3530, 0): 0.0, (3540, 0): 0.0, (3550, 0): 0.0, (3560, 0): 0.0, (3570, 0): 0.0, (3580, 0): 0.0, (3590, 1): 0.0, (3590, 0): 0.0, (3600, 0): 0.0, (3610, 0): 0.0, (3620, 0): 0.0, (3630, 0): 0.0, (3640, 0): 0.0, (3650, 0): 0.0, (3660, 0): 0.0, (3670, 0): 0.0, (3680, 0): 0.0, (3690, 0): 0.0, (3700, 0): 0.0, (3710, 0): 0.0, (3720, 0): 0.0, (3730, 0): 0.0, (3740, 0): 0.0, (3750, 0): 0.0, (3760, 0): 0.0, (3770, 1): 0.0, (3770, 0): 0.0, (3780, 0): 0.0, (3790, 0): 0.0, (3800, 0): 0.0, (3810, 0): 0.0, (3820, 0): 0.0, (3830, 0): 0.0, (3840, 0): 0.0, (3850, 1): 0.0, (3850, 0): 0.0, (3860, 0): 0.0, (3870, 0): 0.0, (3880, 0): 0.0, (3890, 0): 0.0, (3900, 0): 0.0, (3910, 0): 0.0, (3920, 0): 0.0, (3930, 0): 0.0, (3940, 0): 0.0, (3950, 0): 0.0, (3960, 0): 0.0, (3970, 0): 0.0, (3980, 0): 0.0, (3990, 0): 0.0, (4000, 0): 0.0, (4010, 1): 0.0, (4010, 0): 0.0, (4020, 0): 0.0, (4030, 0): 0.0, (4040, 0): 0.0, (4050, 0): 0.0, (4060, 0): 0.0, (4070, 0): 0.0, (4080, 0): 0.0, (4090, 0): 0.0, (4100, 0): 0.0, (4110, 0): 0.0, (4120, 0): 0.0, (4130, 0): 0.0, (4140, 0): 0.0, (4150, 1): 0.0, (4150, 0): 0.0, (4160, 0): 0.0, (4170, 0): 0.0, (4180, 0): 0.0, (4190, 0): 0.0, (4200, 1): 0.0, (4200, 0): 0.0, (4210, 0): 0.0, (4220, 0): 0.0, (4230, 0): 0.0, (4240, 0): 0.0, (4250, 0): 0.0, (4260, 0): 0.0, (4270, 0): 0.0, (4280, 0): 0.0, (4290, 0): 0.0, (4300, 0): 0.0, (4310, 0): 0.0, (4320, 0): 0.0, (4330, 0): 0.0, (4340, 0): 0.0, (4350, 0): 0.0, (4360, 0): 0.0, (4370, 0): 0.0, (4380, 0): 0.0, (4390, 0): 0.0, (4400, 0): 0.0, (4410, 0): 0.0, (4420, 0): 0.0, (4430, 0): 0.0, (4440, 0): 0.0, (4450, 0): 0.0, (4460, 0): 0.0, (4470, 0): 0.0, (4480, 0): 0.0, (4490, 0): 0.0, (4500, 0): 0.0, (4510, 0): 0.0, (4520, 1): 0.0, (4520, 0): 0.0, (4530, 0): 0.0, (4540, 0): 0.0, (4550, 0): 0.0, (4560, 0): 0.0, (4570, 0): 0.0, (4580, 0): 0.0, (4590, 0): 0.0, (4600, 0): 0.0, (4610, 0): 0.0, (4620, 0): 0.0, (4630, 0): 0.0, (4640, 0): 0.0, (4650, 0): 0.0, (4660, 0): 0.0, (4670, 1): 0.0, (4670, 0): 0.0, (4680, 0): 0.0, (4690, 0): 0.0, (4700, 0): 0.0, (4710, 0): 0.0, (4720, 0): 0.0, (4730, 0): 0.0, (4740, 0): 0.0, (4750, 0): 0.0, (4760, 0): 0.0, (4770, 0): 0.0, (4780, 0): 0.0, (4790, 0): 0.0, (4800, 0): 0.0, (4810, 0): 0.0, (4820, 1): 0.0, (4820, 0): 0.0, (4830, 0): 0.0, (4840, 0): 0.0, (4850, 0): 0.0, (4860, 0): 0.0, (4870, 0): 0.0, (4880, 0): 0.0, (4890, 0): 0.0, (4900, 0): 0.0, (4910, 0): 0.0, (4920, 0): 0.0, (4930, 0): 0.0, (4940, 0): 0.0, (4950, 0): 0.0, (4960, 0): 0.0, (4970, 0): 0.0, (4980, 0): 0.0, (4990, 0): 0.0, (5000, 0): 0.0, (5010, 0): 0.0, (5020, 0): 0.0, (5030, 0): 0.0, (5040, 0): 0.0, (5050, 0): 0.0, (5060, 0): 0.0, (5070, 0): 0.0, (5080, 0): 0.0, (5090, 0): 0.0, (5100, 0): 0.0, (5110, 0): 0.0, (5120, 0): 0.0, (5130, 0): 0.0, (5140, 0): 0.0, (5150, 0): 0.0, (5160, 0): 0.0, (5170, 0): 0.0, (5180, 0): 0.0, (5190, 1): 0.0, (5190, 0): 0.0, (5200, 0): 0.0, (5210, 0): 0.0, (5220, 0): 0.0, (5230, 1): 0.0, (5230, 0): 0.0, (5240, 0): 0.0, (5250, 0): 0.0, (5260, 0): 0.0, (5270, 0): 0.0, (5280, 0): 0.0, (5290, 0): 0.0, (5300, 0): 0.0, (5310, 0): 0.0, (5320, 0): 0.0, (5330, 0): 0.0, (5340, 0): 0.0, (5350, 0): 0.0, (5360, 0): 0.0, (5370, 0): 0.0, (5380, 0): 0.0, (5390, 0): 0.0, (5400, 0): 0.0, (5410, 0): 0.0, (5420, 0): 0.0, (5430, 0): 0.0, (5440, 0): 0.0, (5450, 0): 0.0, (5460, 0): 0.0, (5470, 0): 0.0, (5480, 0): 0.0, (5490, 0): 0.0, (5500, 0): 0.0, (5510, 0): 0.0, (5520, 0): 0.0, (5530, 0): 0.0, (5540, 0): 0.0, (5550, 0): 0.0, (5560, 0): 0.0, (5570, 0): 0.0, (5580, 0): 0.0, (5590, 0): 0.0, (5600, 0): 0.0, (5610, 0): 0.0, (5620, 0): 0.0, (5630, 0): 0.0, (5640, 0): 0.0, (5650, 0): 0.0, (5660, 0): 0.0, (5670, 0): 0.0, (5680, 0): 0.0, (5690, 0): 0.0, (5700, 0): 0.0, (5710, 0): 0.0, (5720, 0): 0.0, (5730, 0): 0.0, (5740, 0): 0.0, (5750, 0): 0.0, (5760, 0): 0.0, (5770, 0): 0.0, (5780, 0): 0.0, (5790, 0): 0.0, (5800, 0): 0.0, (5810, 0): 0.0, (5820, 0): 0.0, (5830, 0): 0.0, (5840, 0): 0.0, (5850, 0): 0.0, (5860, 0): 0.0, (5870, 0): 0.0, (5880, 0): 0.0, (5890, 0): 0.0, (5900, 0): 0.0, (5910, 0): 0.0, (5920, 0): 0.0, (5930, 0): 0.0, (5940, 0): 0.0, (5950, 0): 0.0, (5960, 0): 0.0, (5970, 0): 0.0, (5980, 0): 0.0, (5990, 0): 0.0, (6000, 0): 0.0, (6010, 0): 0.0, (6020, 0): 0.0, (6030, 0): 0.0, (6040, 0): 0.0, (6050, 0): 0.0, (6060, 0): 0.0, (6070, 1): 0.0, (6070, 0): 0.0, (6080, 0): 0.0, (6090, 1): 0.0, (6090, 0): 0.0, (6100, 0): 0.0, (6110, 0): 0.0, (6120, 0): 0.0, (6130, 0): 0.0, (6140, 0): 0.0, (6150, 0): 0.0, (6160, 0): 0.0, (6170, 0): 0.0, (6180, 0): 0.0, (6190, 0): 0.0, (6200, 0): 0.0, (6210, 0): 0.0, (6220, 0): 0.0, (6230, 0): 0.0, (6240, 0): 0.0, (6250, 0): 0.0, (6260, 0): 0.0, (6270, 0): 0.0, (6280, 0): 0.0, (6290, 0): 0.0, (6300, 0): 0.0, (6310, 0): 0.0, (6320, 1): 0.0, (6320, 0): 0.0, (6330, 0): 0.0, (6340, 0): 0.0, (6350, 0): 0.0, (6360, 0): 0.0, (6370, 0): 0.0, (6380, 0): 0.0, (6390, 0): 0.0, (6400, 0): 0.0, (6410, 0): 0.0, (6420, 0): 0.0, (6430, 0): 0.0, (6440, 0): 0.0, (6450, 0): 0.0, (6460, 0): 0.0, (6470, 0): 0.0, (6480, 0): 0.0, (6490, 0): 0.0, (6500, 1): 0.0, (6500, 0): 0.0, (6510, 0): 0.0, (6520, 0): 0.0, (6530, 0): 0.0, (6540, 0): 0.0, (6550, 0): 0.0, (6560, 0): 0.0, (6570, 0): 0.0, (6580, 0): 0.0, (6590, 0): 0.0, (6600, 0): 0.0, (6610, 0): 0.0, (6620, 0): 0.0, (6630, 0): 0.0, (6640, 0): 0.0, (6650, 0): 0.0, (6660, 0): 0.0, (6670, 0): 0.0, (6680, 0): 0.0, (6690, 0): 0.0, (6700, 0): 0.0, (6710, 0): 0.0, (6720, 0): 0.0, (6730, 0): 0.0, (6740, 0): 0.0, (6750, 0): 0.0, (6760, 0): 0.0, (6770, 0): 0.0, (6780, 0): 0.0, (6790, 0): 0.0, (6800, 0): 0.0, (6810, 0): 0.0, (6820, 0): 0.0, (6830, 1): 0.0, (6830, 0): 0.0, (6840, 0): 0.0, (6850, 0): 0.0, (6860, 0): 0.0, (6870, 0): 0.0, (6880, 0): 0.0, (6890, 0): 0.0, (6900, 1): 0.0, (6900, 0): 0.0, (6910, 0): 0.0, (6920, 0): 0.0, (6930, 0): 0.0, (6940, 0): 0.0, (6950, 0): 0.0, (6960, 0): 0.0, (6970, 0): 0.0, (6980, 1): 0.0, (6970, 1): 0.0, (6960, 1): 0.0, (6980, 0): 0.0, (6990, 0): 0.0, (7000, 0): 0.0, (7010, 1): 0.0, (7010, 0): 0.0, (7020, 0): 0.0, (7030, 0): 0.0, (7040, 0): 0.0, (7050, 0): 0.0, (7060, 0): 0.0, (7070, 0): 0.0, (7080, 0): 0.0, (7090, 0): 0.0, (7100, 0): 0.0, (7110, 0): 0.0, (7120, 1): 0.0, (7120, 0): 0.0, (7130, 0): 0.0, (7140, 0): 0.0, (7150, 0): 0.0, (7160, 0): 0.0, (7170, 0): 0.0, (7180, 0): 0.0, (7190, 0): 0.0, (7200, 0): 0.0, (7210, 0): 0.0, (7220, 0): 0.0, (7230, 0): 0.0, (7240, 0): 0.0, (7250, 0): 0.0, (7260, 0): 0.0, (7270, 0): 0.0, (7280, 0): 0.0, (7290, 0): 0.0, (7300, 0): 0.0, (7310, 0): 0.0, (7320, 0): 0.0, (7330, 0): 0.0, (7340, 0): 0.0, (7350, 0): 0.0, (7360, 0): 0.0, (7370, 0): 0.0, (7380, 0): 0.0, (7390, 0): 0.0, (7400, 0): 0.0, (7410, 0): 0.0, (7420, 0): 0.0, (7430, 0): 0.0, (7440, 0): 0.0, (7450, 0): 0.0, (7460, 0): 0.0, (7470, 0): 0.0, (7480, 1): 0.0, (7480, 0): 0.0, (7490, 0): 0.0, (7500, 0): 0.0, (7510, 0): 0.0, (7520, 0): 0.0, (7530, 0): 0.0, (7540, 0): 0.0, (7550, 0): 0.0, (7560, 0): 0.0, (7570, 0): 0.0, (7580, 0): 0.0, (7590, 0): 0.0, (7600, 0): 0.0, (7610, 1): 0.0, (7610, 0): 0.0, (7620, 0): 0.0, (7630, 0): 0.0, (7640, 0): 0.0, (7650, 0): 0.0, (7660, 0): 0.0, (7670, 0): 0.0, (7680, 0): 0.0, (7690, 0): 0.0, (7700, 0): 0.0, (7710, 0): 0.0, (7720, 0): 0.0, (7730, 0): 0.0, (7740, 0): 0.0, (7750, 0): 0.0, (7760, 0): 0.0, (7770, 1): 0.0, (7770, 0): 0.0, (7780, 0): 0.0, (7790, 0): 0.0, (7800, 0): 0.0, (7810, 0): 0.0, (7820, 0): 0.0, (7830, 0): 0.0, (7840, 0): 0.0, (7850, 0): 0.0, (7860, 0): 0.0, (7870, 0): 0.0, (7880, 0): 0.0, (7890, 0): 0.0, (7900, 0): 0.0, (7910, 0): 0.0, (7920, 0): 0.0, (7930, 0): 0.0, (7940, 0): 0.0, (7950, 0): 0.0, (7960, 0): 0.0, (7970, 0): 0.0, (7980, 0): 0.0, (7990, 0): 0.0, (8000, 0): 0.0, (8010, 0): 0.0, (8020, 0): 0.0, (8030, 0): 0.0, (8040, 0): 0.0, (8050, 0): 0.0, (8060, 0): 0.0, (8070, 0): 0.0, (8080, 0): 0.0, (8090, 0): 0.0, (8100, 0): 0.0, (8110, 0): 0.0, (8120, 0): 0.0, (8130, 0): 0.0, (8140, 0): 0.0, (8150, 0): 0.0, (8160, 0): 0.0, (8170, 0): 0.0, (8180, 0): 0.0, (8190, 0): 0.0, (8200, 0): 0.0, (8210, 0): 0.0, (8220, 0): 0.0, (8230, 0): 0.0, (8240, 0): 0.0, (8250, 0): 0.0, (8260, 1): 0.0, (8260, 0): 0.0, (8270, 1): 0.0, (8270, 0): 0.0, (8280, 1): 0.0, (8280, 0): 0.0, (8290, 0): 0.0, (8300, 0): 0.0, (8310, 0): 0.0, (8320, 0): 0.0, (8330, 0): 0.0, (8340, 0): 0.0, (8350, 0): 0.0, (8360, 0): 0.0, (8370, 0): 0.0, (8380, 0): 0.0, (8390, 1): 0.0, (8390, 0): 0.0, (8400, 0): 0.0, (8410, 0): 0.0, (8420, 0): 0.0, (8430, 0): 0.0, (8440, 0): 0.0, (8450, 0): 0.0, (8460, 0): 0.0, (8470, 0): 0.0, (8480, 0): 0.0, (8490, 0): 0.0, (8500, 0): 0.0, (8510, 0): 0.0, (8520, 0): 0.0, (8530, 0): 0.0, (8540, 0): 0.0, (8550, 0): 0.0, (8560, 0): 0.0, (8570, 0): 0.0, (8580, 0): 0.0, (8590, 0): 0.0, (8600, 0): 0.0, (8610, 0): 0.0, (8620, 0): 0.0, (8630, 0): 0.0, (8640, 0): 0.0, (8650, 0): 0.0, (8660, 0): 0.0, (8670, 0): 0.0, (8680, 0): 0.0, (8690, 0): 0.0, (8700, 0): 0.0, (8710, 0): 0.0, (8720, 0): 0.0, (8730, 0): 0.0, (8740, 0): 0.0, (8750, 0): 0.0, (8760, 0): 0.0, (8770, 0): 0.0, (8780, 0): 0.0, (8790, 0): 0.0, (8800, 1): 0.0, (8800, 0): 0.0, (8810, 0): 0.0, (8820, 1): 0.0, (8820, 0): 0.0, (8830, 0): 0.0, (8840, 0): 0.0, (8850, 0): 0.0, (8860, 0): 0.0, (8870, 0): 0.0, (8880, 0): 0.0, (8890, 0): 0.0, (8900, 0): 0.0, (8910, 0): 0.0, (8920, 0): 0.0, (8930, 0): 0.0, (8940, 0): 0.0, (8950, 0): 0.0, (8960, 0): 0.0, (8970, 0): 0.0, (8980, 0): 0.0, (8990, 0): 0.0, (9000, 0): 0.0, (9010, 0): 0.0, (9020, 0): 0.0, (9030, 0): 0.0, (9040, 0): 0.0, (9050, 0): 0.0, (9060, 0): 0.0, (9070, 1): 0.0, (9070, 0): 0.0, (9080, 0): 0.0, (9090, 0): 0.0, (9100, 0): 0.0, (9110, 0): 0.0, (9120, 0): 0.0, (9130, 0): 0.0, (9140, 0): 0.0, (9150, 0): 0.0, (9160, 0): 0.0, (9170, 0): 0.0, (9180, 1): 0.0, (9180, 0): 0.0, (9190, 0): 0.0, (9200, 0): 0.0, (9210, 0): 0.0, (9220, 0): 0.0, (9230, 0): 0.0, (9240, 0): 0.0, (9250, 0): 0.0, (9260, 0): 0.0, (9270, 0): 0.0, (9280, 0): 0.0, (9290, 0): 0.0, (9300, 0): 0.0, (9310, 0): 0.0, (9320, 0): 0.0, (9330, 0): 0.0, (9340, 0): 0.0, (9350, 0): 0.0, (9360, 0): 0.0, (9370, 0): 0.0, (9380, 0): 0.0, (9390, 0): 0.0, (9400, 0): 0.0, (9410, 1): 0.0, (9410, 0): 0.0, (9420, 0): 0.0, (9430, 0): 0.0, (9440, 1): 0.0, (9440, 0): 0.0, (9450, 0): 0.0, (9460, 0): 0.0, (9470, 1): 0.0, (9470, 0): 0.0, (9480, 0): 0.0, (9490, 0): 0.0, (9500, 0): 0.0, (9510, 0): 0.0, (9520, 0): 0.0, (9530, 0): 0.0, (9540, 0): 0.0, (9550, 0): 0.0, (9560, 0): 0.0, (9570, 0): 0.0, (9580, 0): 0.0, (9590, 0): 0.0, (9600, 0): 0.0, (9610, 0): 0.0, (9620, 0): 0.0, (9630, 0): 0.0, (9640, 0): 0.0, (9650, 0): 0.0, (9660, 0): 0.0, (9670, 0): 0.0, (9680, 0): 0.0, (9690, 0): 0.0, (9700, 0): 0.0, (9710, 0): 0.0, (9720, 0): 0.0, (9730, 0): 0.0, (9740, 0): 0.0, (9750, 0): 0.0, (9760, 0): 0.0, (9770, 0): 0.0, (9780, 0): 0.0, (9790, 1): 0.0, (9790, 0): 0.0, (9800, 0): 0.0, (9810, 0): 0.0, (9820, 0): 0.0, (9830, 0): 0.0, (9840, 0): 0.0, (9850, 0): 0.0, (9860, 0): 0.0, (9870, 0): 0.0, (9880, 0): 0.0, (9890, 0): 0.0, (9900, 1): 0.0, (9900, 0): 0.0, (9910, 0): 0.0, (9920, 0): 0.0, (9930, 0): 0.0, (9940, 0): 0.0, (9950, 0): 0.0, (9960, 0): 0.0, (9970, 0): 0.0, (9980, 0): 3.711421555661563, (9990, 0): 25.67770681854326, (10000, 0): 29.126171389173713, (10000, 1): 18.57874322035747, (9990, 1): 0.0}\n"
     ]
    }
   ],
   "source": [
    "class LiquidityPool:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.state = 50\n",
    "        # Threshold for maximum and minimum liquidity\n",
    "        self.max_liquidity = 10000\n",
    "        self.min_liquidity = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Increase liquidity\n",
    "        if action == 0:\n",
    "            self.state += 10\n",
    "        # Decrease liquidity\n",
    "        elif action == 1:\n",
    "            self.state -= 10\n",
    "        \n",
    "        # Ensure that liquidity remains within bounds\n",
    "        self.state = max(self.min_liquidity, self.state)\n",
    "        self.state = min(self.max_liquidity, self.state)\n",
    "        \n",
    "        # Reward function\n",
    "        if self.state == self.max_liquidity:\n",
    "            reward = 1\n",
    "        elif self.state == self.min_liquidity:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        done = self.state in [self.min_liquidity, self.max_liquidity]\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = {}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice([0, 1])\n",
    "        else:\n",
    "            return max(list(range(2)), key = lambda x: self.get_Q(state, x))\n",
    "\n",
    "    def get_Q(self, state, action):\n",
    "        return self.Q.get((state, action), 0.0)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = self.choose_action(next_state)\n",
    "        q_next = self.get_Q(next_state, best_next_action)\n",
    "        q_curr = self.get_Q(state, action)\n",
    "        self.Q[(state, action)] = q_curr + self.alpha * (reward + self.gamma * q_next - q_curr)\n",
    "\n",
    "pool = LiquidityPool()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = pool.state\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done = pool.step(action)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "print(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning tabular 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total reward = 11.536499999999998\n",
      "Episode 2: Total reward = 11.625000000000007\n",
      "Episode 3: Total reward = 11.121\n",
      "Episode 4: Total reward = 10.332000000000003\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for axis 0 with size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mchoose_action(state)\n\u001b[0;32m     62\u001b[0m next_state, reward \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m---> 63\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(state, action, reward, next_state)\n\u001b[0;32m     64\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     65\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mQLearningAgent.learn\u001b[1;34m(self, state, action, reward, next_state)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\u001b[39mself\u001b[39m, state, action, reward, next_state):\n\u001b[0;32m     44\u001b[0m     old_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_table[state, action]\n\u001b[1;32m---> 45\u001b[0m     next_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_table[next_state, :])\n\u001b[0;32m     46\u001b[0m     new_value \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha) \u001b[39m*\u001b[39m old_value \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_max)\n\u001b[0;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_table[state, action] \u001b[39m=\u001b[39m new_value\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1000 is out of bounds for axis 0 with size 1000"
     ]
    }
   ],
   "source": [
    "class LiquidityPoolEnvironment:\n",
    "    def __init__(self, initial_liquidity, fee_percentage=0.003):\n",
    "        self.liquidity = initial_liquidity\n",
    "        self.fee_percentage = fee_percentage\n",
    "        self.total_fees = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate trading volume\n",
    "        volume = np.random.randint(1, 100)\n",
    "        \n",
    "        # If action is \"add liquidity\"\n",
    "        if action == 0:\n",
    "            self.liquidity += 10\n",
    "            fees_earned = volume * self.fee_percentage\n",
    "        # If action is \"remove liquidity\"\n",
    "        else:\n",
    "            self.liquidity = max(self.liquidity - 10, 10)\n",
    "            fees_earned = volume * self.fee_percentage * 0.5  # Earn half the fees if liquidity is removed\n",
    "        \n",
    "        self.total_fees += fees_earned\n",
    "        reward = fees_earned  # The reward is the fees earned\n",
    "        next_state = self.liquidity\n",
    "        \n",
    "        return next_state, reward\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, states, actions, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = np.zeros((states, len(actions)))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        old_value = self.q_table[state, action]\n",
    "        next_max = np.max(self.q_table[next_state, :])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[state, action] = new_value\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Simulation\n",
    "env = LiquidityPoolEnvironment(initial_liquidity=100)\n",
    "agent = QLearningAgent(states=1000, actions=[0, 1])\n",
    "\n",
    "for episode in range(100):\n",
    "    state = env.liquidity\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(100):\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Total reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deque' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\tokenspice\\ILP_mechanism.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m state_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# The state is just the liquidity value\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m action_size \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m  \u001b[39m# Add liquidity or remove liquidity\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m dqn_agent \u001b[39m=\u001b[39m DQNAgent(state_size, action_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# Training the DQN agent\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m EPISODES \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\tokenspice\\ILP_mechanism.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon_min \u001b[39m=\u001b[39m epsilon_min\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon_decay \u001b[39m=\u001b[39m epsilon_decay\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39m=\u001b[39m deque(maxlen\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m)  \u001b[39m# Experience replay buffer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hijaz%20tr/Desktop/cadCADProject1/tokenspice/ILP_mechanism.ipynb#X21sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_model()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deque' is not defined"
     ]
    }
   ],
   "source": [
    "class LiquidityPoolEnvironment:\n",
    "    def __init__(self, initial_liquidity, fee_percentage=0.003):\n",
    "        self.initial_liquidity = initial_liquidity\n",
    "        self.liquidity = initial_liquidity\n",
    "        self.fee_percentage = fee_percentage\n",
    "        self.total_fees = 0\n",
    "        self.done = False  # Flag to indicate if the episode is done\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate trading volume\n",
    "        volume = np.random.randint(1, 100)\n",
    "        \n",
    "        # If action is \"add liquidity\"\n",
    "        if action == 0:\n",
    "            self.liquidity += 10\n",
    "            fees_earned = volume * self.fee_percentage\n",
    "        # If action is \"remove liquidity\"\n",
    "        else:\n",
    "            self.liquidity = max(self.liquidity - 10, 10)\n",
    "            fees_earned = volume * self.fee_percentage * 0.5  # Earn half the fees if liquidity is removed\n",
    "        \n",
    "        self.total_fees += fees_earned\n",
    "        reward = fees_earned  # The reward is the fees earned\n",
    "        next_state = self.liquidity\n",
    "        \n",
    "        if self.liquidity >= 200:  # Example termination condition\n",
    "            self.done = True\n",
    "        \n",
    "        return next_state, reward, self.done\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size=32, alpha=0.001, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=2000)  # Experience replay buffer\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()  # Target network\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.alpha))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            q_values = self.model.predict(state)\n",
    "            return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = np.array(np.random.sample(self.memory, self.batch_size))\n",
    "        states = np.vstack(batch[:, 0])\n",
    "        actions = batch[:, 1].astype(int)\n",
    "        rewards = batch[:, 2]\n",
    "        next_states = np.vstack(batch[:, 3])\n",
    "        dones = batch[:, 4].astype(bool)\n",
    "        \n",
    "        target_values = rewards + self.gamma * np.max(self.target_model.predict(next_states), axis=1) * ~dones\n",
    "        target = self.model.predict(states)\n",
    "        target[np.arange(self.batch_size), actions] = target_values\n",
    "        self.model.fit(states, target, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "# Create the environment\n",
    "env = LiquidityPoolEnvironment(initial_liquidity=100)\n",
    "\n",
    "# Create the DQN agent\n",
    "state_size = 1  # The state is just the liquidity value\n",
    "action_size = 2  # Add liquidity or remove liquidity\n",
    "dqn_agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Training the DQN agent\n",
    "EPISODES = 100\n",
    "for episode in range(EPISODES):\n",
    "    state = np.array([[env.liquidity]])  # Convert state to 2D array\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(100):\n",
    "        action = dqn_agent.choose_action(state)\n",
    "        next_state, reward,done = env.step(action)\n",
    "        next_state = np.array([[next_state]])\n",
    "        dqn_agent.remember(state, action, reward, next_state, False)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update DQN agent's Q-values\n",
    "        dqn_agent.replay()\n",
    "        \n",
    "        if env.done:\n",
    "            break\n",
    "    \n",
    "    dqn_agent.update_target_model()\n",
    "    print(f\"Episode {episode+1}: Total reward = {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniswapV3Env1_1(gym.Env):\n",
    "    def __init__(self, initial_state):\n",
    "        super(UniswapV3Env1_1, self).__init__()\n",
    "        \n",
    "        # State space: [current price, price volatility, TVL, volumeUSD, feesUSD, n_LPs]\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0, 0, 0, 0, 0, 0]), \n",
    "                                                high=np.array([np.inf, np.inf, np.inf, np.inf, np.inf, np.inf]), \n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "        # Action space: [lower tick, upper tick]\n",
    "        # which_action = gym.spaces.Discrete(3)\n",
    "        # how_much_action = gym.spaces.Box(low=0,high=np.inf)\n",
    "        #action_space=gym.spaces.Tuple(which_actio, how_much_action)\n",
    "        self.action_space = gym.spaces.Box(low=np.array([0, 0]), high=np.array([np.inf, np.inf]), dtype=np.float32)\n",
    "        \n",
    "        self.current_state = initial_state\n",
    "        self.max_days = 30\n",
    "        self.current_day = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # For simplicity, we can simulate the change in state due to market dynamics as random changes\n",
    "        # However, using real market data or a more sophisticated model would be better\n",
    "        self.current_state += np.random.normal(0, 11, size=self.observation_space.shape[0])\n",
    "        \n",
    "        # Calculate reward as fees earned within the selected tick range\n",
    "        # Here we make a simplistic assumption that fees are proportional to volume and the size of the tick range\n",
    "        reward = self.current_state[3] * 0.03\n",
    "        \n",
    "        self.current_day += 1\n",
    "        done = self.current_day >= self.max_days\n",
    "        \n",
    "        return self.current_state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.array([1.0, 0.1, 10000, 1000, 50, 100])  # Example initial values\n",
    "        self.current_day = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3, gamma=0.99, epsilon=1.0):\n",
    "        self.q_network = QNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.rand(2)  # Random action in the range [0, 1] for both ticks\n",
    "        q_values = self.q_network(torch.FloatTensor(state))\n",
    "        return q_values.detach().numpy()  # Return Q-values as the action (ticks)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        state = torch.FloatTensor([state])\n",
    "        next_state = torch.FloatTensor([next_state])\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        \n",
    "        curr_q = self.q_network(state)\n",
    "        next_q = self.q_network(next_state).detach().max().item()\n",
    "        expected_q = reward + self.gamma * next_q\n",
    "\n",
    "        loss = nn.MSELoss()(curr_q, expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "''''\n",
    "def train(agent, env, episodes=100):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            #print(f\"Lower_tick: {action[0]}, Upper_tick: {action[1]}\")\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        agent.epsilon = max(agent.epsilon * 0.995, 0.01)  # Decay epsilon\n",
    "        \n",
    "        if episode % 5 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    return total_rewards\n",
    "'''\n",
    "\n",
    "def train_with_viz(agent, env, episodes=100):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "    \n",
    "    total_rewards = []\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "    all_episode_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_episode_rewards = []\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_episode_rewards.append(episode_reward)\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        agent.epsilon = max(agent.epsilon * 0.995, 0.01)  # Decay epsilon\n",
    "\n",
    "        all_states.extend(episode_states)\n",
    "        all_actions.extend(episode_actions)\n",
    "        all_episode_rewards.extend(episode_episode_rewards)\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    # Plotting the States\n",
    "    axs[0].plot([s[0] for s in all_states], label='Current Price')\n",
    "    axs[0].plot([s[1] for s in all_states], label='Price Volatility')\n",
    "    axs[0].legend()\n",
    "    axs[0].set_title('States Over Time')\n",
    "\n",
    "    # Plotting the Actions with shaded area\n",
    "    axs[1].fill_between(list(range(len(episode_actions))),\n",
    "                        [a[0] for a in episode_actions],\n",
    "                        [a[1] for a in episode_actions],\n",
    "                        color='skyblue', alpha=0.4, label=\"Liquidity Range\")\n",
    "    axs[1].plot([a[0] for a in episode_actions], label='Lower Tick', color='blue')\n",
    "    axs[1].plot([a[1] for a in episode_actions], label='Upper Tick', color='orange')\n",
    "    axs[1].legend()\n",
    "    axs[1].set_title('Actions Over Time for Last Episode')\n",
    "\n",
    "    # Plotting the Rewards\n",
    "    axs[2].plot(all_episode_rewards, label='Episode Reward')\n",
    "    axs[2].legend()\n",
    "    axs[2].set_title('Rewards Over Time')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return total_rewards\n",
    "\n",
    "\n",
    "def evaluate(agent, env, episodes=100):\n",
    "    total_rewards = []\n",
    "\n",
    "    # Set the epsilon of the agent to 0 to disable exploration\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    # Restore the original epsilon value\n",
    "    agent.epsilon = original_epsilon\n",
    "\n",
    "    avg_reward = sum(total_rewards) / episodes\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# Define State\n",
    "def fetch_pool_data(pool_id= \"0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640\", UNISWAP_V3_SUBGRAPH_URL = 'https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3'):\n",
    "    # Fetching liquidity positions\n",
    "    liquidity_query = \"\"\"\n",
    "    {\n",
    "      positions(where: { pool: \"%s\" }) {\n",
    "        liquidity\n",
    "        tickLower {\n",
    "          price0\n",
    "        }\n",
    "        tickUpper {\n",
    "          price0\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    liquidity_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': liquidity_query}).json()\n",
    "\n",
    "    # Fetching pool price, volume, fees, and reserves\n",
    "    pool_query = \"\"\"\n",
    "    {\n",
    "      pools(where: { id: \"%s\" }) {\n",
    "        token0Price\n",
    "        token1Price\n",
    "        volumeUSD\n",
    "        feesUSD\n",
    "        totalValueLockedToken0\n",
    "        totalValueLockedToken1\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    pool_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': pool_query}).json()\n",
    "\n",
    "    # Fetching swaps and trades\n",
    "    swaps_query = \"\"\"\n",
    "    {\n",
    "      swaps(where: { pool: \"%s\" }, first: 10, orderBy: timestamp, orderDirection: desc) {\n",
    "        amount0\n",
    "        amount1\n",
    "        amountUSD\n",
    "        timestamp\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    swaps_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': swaps_query}).json()\n",
    "\n",
    "        # Constructing the state representation with float data type\n",
    "    state = {\n",
    "        'liquidity_ranges': [(float(position['tickLower']['price0']), float(position['tickUpper']['price0']), float(position['liquidity'])) for position in liquidity_data['data']['positions']],\n",
    "        'pool_price': float(pool_data['data']['pools'][0]['token0Price']),\n",
    "        'volume': float(pool_data['data']['pools'][0]['volumeUSD']),\n",
    "        'fees': float(pool_data['data']['pools'][0]['feesUSD']),\n",
    "        'reserves': {\n",
    "            'token0': float(pool_data['data']['pools'][0]['totalValueLockedToken0']),\n",
    "            'token1': float(pool_data['data']['pools'][0]['totalValueLockedToken1'])\n",
    "        },\n",
    "        'recent_swaps': [(float(swap['amount0']), float(swap['amount1']), float(swap['amountUSD'])) for swap in swaps_data['data']['swaps']]\n",
    "    }\n",
    "\n",
    "    return state\n",
    "\n",
    "state_representation = fetch_pool_data()\n",
    "print(state_representation)\n",
    "\n",
    "# Instantiate the environment\n",
    "current_price=state_representation['pool_price']\n",
    "price_volatility=0.1\n",
    "tvl=state_representation['reserves']['token0'] + state_representation['reserves']['token1']*current_price\n",
    "volumeUSD=state_representation['volume']\n",
    "feesUSD=state_representation['fees']\n",
    "n_LPs=tvl/100000\n",
    "initial_state = np.array([current_price, price_volatility, tvl, volumeUSD, feesUSD, n_LPs])#np.array([1.0, 0.1, 10000, 1000, 50, 100])  # Example initial values\n",
    "env = UniswapV3Env1_1(initial_state)\n",
    "\n",
    "# State dimension is 6 (current price, price volatility, TVL, volumeUSD, feesUSD, n_LPs)\n",
    "# Action dimension is 2 (lower tick, upper tick)\n",
    "agent = DQNAgent(6, 2)\n",
    "\n",
    "# Train the agent\n",
    "rewards = train(agent, env, episodes=100)\n",
    "# Evaluate the agent\n",
    "avg_reward = evaluate(agent, env, episodes=10)\n",
    "print(f\"Average reward over 100 episodes: {avg_reward}\")\n",
    "torch.save(agent.q_network.state_dict(), 'trained_model.pth')\n",
    "model = QNetwork(6, 2)  # Initialize the model architecture\n",
    "model.load_state_dict(torch.load('trained_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Get real time data of pool for state defination, get action in this state from learned agent and apply action in real enviroenmnt \n",
    "current_price=state_representation['pool_price']\n",
    "price_volatility=0.1\n",
    "tvl=state_representation['reserves']['token0'] + state_representation['reserves']['token1']*current_price\n",
    "volumeUSD=state_representation['volume']\n",
    "feesUSD=state_representation['fees']\n",
    "n_LPs=tvl/100000\n",
    "\n",
    "state = np.array([current_price, price_volatility, tvl, volumeUSD, feesUSD, n_LPs])\n",
    "action = agent.act(state)\n",
    "action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniswapV3Env1_1(gym.Env):\n",
    "    def __init__(self, initial_state):\n",
    "        super(UniswapV3Env1_1, self).__init__()\n",
    "        \n",
    "        # State space: [current price, price volatility, TVL, volumeUSD, feesUSD, n_LPs]\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0, 0, 0, 0, 0, 0]), \n",
    "                                                high=np.array([np.inf, np.inf, np.inf, np.inf, np.inf, np.inf]), \n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "       \n",
    "        #action_space=gym.spaces.Tuple(which_actio, how_much_action)\n",
    "        self.action_space = gym.spaces.Box(low=np.array([0, 0]), high=np.array([1000, 1000]), dtype=np.float32)\n",
    "        #self.action_space = Dict({'lower_tick':Box(0,1000), \"upper_tick\":Box(1000,10000)})\n",
    "        \n",
    "        self.current_state = initial_state\n",
    "        self.max_days = 120\n",
    "        self.current_day = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # For simplicity, we can simulate the change in state due to market dynamics as random changes\n",
    "        # However, using real market data or a more sophisticated model would be better\n",
    "        self.current_state += np.random.normal(0, 11, size=self.observation_space.shape[0])\n",
    "        \n",
    "        # Calculate reward as fees earned within the selected tick range\n",
    "        # Here we make a simplistic assumption that fees are proportional to volume and the size of the tick range\n",
    "        reward = self.current_state[3] * 0.03\n",
    "        \n",
    "        self.current_day += 1\n",
    "        done = self.current_day >= self.max_days\n",
    "        \n",
    "        return self.current_state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.array([1.0, 0.1, 10000, 1000, 50, 100])  # Example initial values\n",
    "        self.current_day = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def render(self):\n",
    "        print('Fuck you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UniswapV3Env1_1(initial_state)\n",
    "env.observation_space.sample()\n",
    "env.action_space.sample()\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1)\n",
    "model.learn(total_timesteps=20000)\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done,info = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        print('info', info)\n",
    "        print('obs',obs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class UniswapV3Env(gym.Env):\n",
    "    def __init__(self):\n",
    "        # State space: [current price, liquidity in current range, total volume, total fees, etc.]\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0, 0, 0, 0]), \n",
    "                                                high=np.array([np.inf, np.inf, np.inf, np.inf]), \n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "        # Action space: [lower tick, upper tick]\n",
    "        self.action_space = gym.spaces.Box(low=np.array([0, 0]), high=np.array([np.inf, np.inf]), dtype=np.float32)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize the state with some reasonable values\n",
    "        self.current_state = np.array([1.0, 1000, 1000, 50])  # Example initial values\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Extract lower and upper ticks from the action\n",
    "        lower_tick, upper_tick,amount_usd = action\n",
    "\n",
    "        # Simulate market participants (traders, LPs, arbitrageurs)\n",
    "       \n",
    "\n",
    "        # Simulate liquidity provisioning\n",
    "        self.add_liquidity(lower_tick, upper_tick,amount_usd)\n",
    "        self.simulate_abm_model(steps=10)\n",
    "\n",
    "        # Calculate reward based on fees earned within the selected tick range\n",
    "        reward = self.calculate_reward(action)\n",
    "\n",
    "        # Check if the episode is done (e.g., based on time or other criteria)\n",
    "        done = self.check_done()\n",
    "\n",
    "        return self.current_state, reward, done, {}\n",
    "\n",
    "    def calculate_reward(self,action):\n",
    "        # Calculate the reward based on fees earned within the selected tick range\n",
    "        # Consider factors like volume, liquidity in the current range, fee tier, etc.\n",
    "        fee=collect_fee(action)\n",
    "        return fee  # Example calculation\n",
    "\n",
    "    def check_done(self):\n",
    "        # Determine if the episode is done (e.g., based on time or other criteria)\n",
    "        return False  # Example condition\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "import requests\n",
    "\n",
    "base_url = 'https://api.arbiscan.io/api'\n",
    "params = {\n",
    "    'module': 'account',\n",
    "    'action': 'balance',\n",
    "    'address': '0x88197EaCf7545B2686715FbBE3DBb0ec725A8514',\n",
    "    'tag': 'latest',\n",
    "    'apikey': '1W6Z6MCUXN8GHMEW1AWV7FJRX9YYY6T25K'\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "print(data)\n",
    "\n",
    "base_url = 'https://api.arbiscan.io/api'\n",
    "params = {\n",
    "    'module': 'logs',\n",
    "    'action': 'getLogs',\n",
    "    'address': '0xC36442b4a4522E871399CD717aBDD847Ab11FE88',\n",
    "    'fromBlock': '22523653',\n",
    "    'toBlock': '22524653',\n",
    "    'page': '1',\n",
    "    'offset': '1000',\n",
    "    'apikey': '1W6Z6MCUXN8GHMEW1AWV7FJRX9YYY6T25K'\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "print(data)\n",
    "\n",
    "import requests\n",
    "# Define State\n",
    "def fetch_pool_data(pool_id= \"0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640\", UNISWAP_V3_SUBGRAPH_URL = 'https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3'):\n",
    "    # Fetching liquidity positions\n",
    "    liquidity_query = \"\"\"\n",
    "    {\n",
    "      positions(where: { pool: \"%s\" }) {\n",
    "        liquidity\n",
    "        tickLower {\n",
    "          price0\n",
    "        }\n",
    "        tickUpper {\n",
    "          price0\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    liquidity_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': liquidity_query}).json()\n",
    "\n",
    "    # Fetching pool price, volume, fees, and reserves\n",
    "    pool_query = \"\"\"\n",
    "    {\n",
    "      pools(where: { id: \"%s\" }) {\n",
    "        token0Price\n",
    "        token1Price\n",
    "        volumeUSD\n",
    "        feesUSD\n",
    "        totalValueLockedToken0\n",
    "        totalValueLockedToken1\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    pool_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': pool_query}).json()\n",
    "\n",
    "    # Fetching swaps and trades\n",
    "    swaps_query = \"\"\"\n",
    "    {\n",
    "      swaps(where: { pool: \"%s\" }, first: 10, orderBy: timestamp, orderDirection: desc) {\n",
    "        amount0\n",
    "        amount1\n",
    "        amountUSD\n",
    "        timestamp\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % pool_id\n",
    "    swaps_data = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={'query': swaps_query}).json()\n",
    "\n",
    "        # Constructing the state representation with float data type\n",
    "    state = {\n",
    "        'liquidity_ranges': [(float(position['tickLower']['price0']), float(position['tickUpper']['price0']), float(position['liquidity'])) for position in liquidity_data['data']['positions'] if float(position['liquidity']) != 0.0],\n",
    "        'pool_price': float(pool_data['data']['pools'][0]['token0Price']),\n",
    "        'volumeUSD': float(pool_data['data']['pools'][0]['volumeUSD']),\n",
    "        'feesUSD': float(pool_data['data']['pools'][0]['feesUSD']),\n",
    "        'reserves': {\n",
    "            'token0': float(pool_data['data']['pools'][0]['totalValueLockedToken0']),\n",
    "            'token1': float(pool_data['data']['pools'][0]['totalValueLockedToken1'])\n",
    "        },\n",
    "        'recent_swaps': [(float(swap['amount0']), float(swap['amount1']), float(swap['amountUSD'])) for swap in swaps_data['data']['swaps']]\n",
    "    }\n",
    "\n",
    "    return state\n",
    "\n",
    "pool_state = fetch_pool_data()\n",
    "pool_state\n",
    "\n",
    "\n",
    "class UniswapV3Env(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Define action space as an example, you can customize this\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        # Define observation space based on your state structure\n",
    "        # This is just a placeholder; you should tailor it to fit the structure of your state\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'liquidity_ranges': spaces.Box(low=0, high=np.inf, shape=(7, 3), dtype=np.float32),\n",
    "            'pool_price': spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'volumeUSD': spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'feesUSD': spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'reserves': spaces.Dict({\n",
    "                'token0': spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "                'token1': spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            }),\n",
    "            'recent_swaps': spaces.Box(low=-np.inf, high=np.inf, shape=(10, 3), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize state by fetching latest pool data\n",
    "        self.state = fetch_pool_data()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Your step logic here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for minimum and maximum ticks\n",
    "from gym import spaces\n",
    "tick_min = -10**6\n",
    "tick_max = 10**6\n",
    "lp_positions = {\n",
    "    \"0x9E5360c7624331a785d1f3CDeA8D0EA2ceB42A71\": [\n",
    "        {\"tick_lower\": 71220, \"tick_upper\": 76740, \"liquidity\": 4.795763907841014e+19},\n",
    "        {\"tick_lower\": 70920, \"tick_upper\": 77160, \"liquidity\": 4.841062996167714e+19},\n",
    "        {\"tick_lower\": 71100, \"tick_upper\": 76980, \"liquidity\": 5.264262945287719e+19}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# LP budget\n",
    "lp_budget = 10**6\n",
    "\n",
    "# Action 0: Do nothing\n",
    "do_nothing_space =spaces. Discrete(1)\n",
    "\n",
    "# Action 1: add_liquidity\n",
    "add_liquidity_space = spaces.Dict({\n",
    "    'tick_lower': spaces.Box(low=tick_min, high=tick_max, shape=(1,), dtype=int),\n",
    "    'tick_upper': spaces.Box(low=tick_min, high=tick_max, shape=(1,), dtype=int),\n",
    "    'amount_usd': spaces.Box(low=0, high=lp_budget, shape=(1,), dtype=float)\n",
    "})\n",
    "\n",
    "# Action 2: remove_liquidity (assuming lp_positions is a list of available positions)\n",
    "# For simplicity, we'll just use a Discrete space to index into lp_positions\n",
    "remove_liquidity_space =spaces. Discrete(100)  # Assuming up to 100 positions can be maintained\n",
    "\n",
    "# Combine them into a single action space\n",
    "action_space = spaces.Tuple((\n",
    "    spaces.Discrete(3),  # 0 for 'do nothing', 1 for 'add_liquidity', 2 for 'remove_liquidity'\n",
    "    add_liquidity_space,\n",
    "    remove_liquidity_space,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " OrderedDict([('amount_usd', array([71804.00727666])),\n",
       "              ('tick_lower', array([-742568])),\n",
       "              ('tick_upper', array([-597352]))]),\n",
       " 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete,Space \n",
    "\n",
    "min_tick = -88705  # Make sure these are consistent with your specific requirements\n",
    "max_tick = tick_min\n",
    "lp_budget = 10000\n",
    "lp_liq = 5000  # You should define this based on your requirements\n",
    "\n",
    "action_space = Dict({\n",
    "    'add_liq': Dict({\n",
    "        'tick_lower': Box(low=min_tick, high=max_tick, shape=(1,), dtype=int),\n",
    "        'tick_upper': Box(low=tick_min, high=tick_max, shape=(1,), dtype=int),\n",
    "        'amount_usd': Box(low=0, high=lp_budget, shape=(1,), dtype=int)\n",
    "    }),\n",
    "    'remove_liq': Dict({\n",
    "        'tick_lower': Box(low=min_tick, high=max_tick, shape=(1,), dtype=int),\n",
    "        'tick_upper': Box(low=tick_min, high=tick_max, shape=(1,), dtype=int),\n",
    "        'amount_liq': Box(low=0, high=lp_liq, shape=(1,), dtype=int)\n",
    "    }),\n",
    "    'do_nothing': Discrete(1)  # Using 1 here to indicate one possible 'do nothing' action\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('add_liq',\n",
       "              OrderedDict([('amount_usd', array([4925])),\n",
       "                           ('tick_lower', array([-88705])),\n",
       "                           ('tick_upper', array([-88705]))])),\n",
       "             ('do_nothing', 0),\n",
       "             ('remove_liq',\n",
       "              OrderedDict([('amount_liq', array([3687])),\n",
       "                           ('tick_lower', array([-88705])),\n",
       "                           ('tick_upper', array([-88705]))]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, Box, Discrete, Tuple\n",
    "\n",
    "min_tick = -88705\n",
    "max_tick = -min_tick\n",
    "tick_min = min_tick  # Make sure these are consistent with your specific requirements\n",
    "tick_max = max_tick\n",
    "lp_budget = 10000\n",
    "lp_liq = 5000  # You should define this based on your requirements\n",
    "\n",
    "# Define the parameters for each action\n",
    "add_liq_params = Dict({\n",
    "    'amount_usd': Box(low=0, high=lp_budget, shape=(1,), dtype=int),\n",
    "    'tick_lower': Box(low=min_tick, high=max_tick, shape=(1,), dtype=int),\n",
    "    'tick_upper': Box(low=tick_min, high=tick_max, shape=(1,), dtype=int),\n",
    "})\n",
    "\n",
    "remove_liq_params = Dict({\n",
    "    'amount_liq': Box(low=0, high=lp_liq, shape=(1,), dtype=int),\n",
    "    'tick_lower': Box(low=min_tick, high=max_tick, shape=(1,), dtype=int),\n",
    "    'tick_upper': Box(low=tick_min, high=tick_max, shape=(1,), dtype=int),\n",
    "})\n",
    "\n",
    "do_nothing_params = Discrete(1)  # Using 1 here to indicate one possible 'do nothing' action\n",
    "\n",
    "# Combine these into a single action space\n",
    "action_space = Dict({\n",
    "    'action_type': Discrete(3),  # 0 for 'do_nothing', 1 for 'add_liq', 2 for 'remove_liq'\n",
    "    'params': Tuple((do_nothing_params, add_liq_params, remove_liq_params))\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('action_type', 2),\n",
       "             ('params',\n",
       "              (0,\n",
       "               OrderedDict([('amount_usd', array([9970])),\n",
       "                            ('tick_lower', array([-38149])),\n",
       "                            ('tick_upper', array([-62975]))]),\n",
       "               OrderedDict([('amount_liq', array([4645])),\n",
       "                            ('tick_lower', array([-68053])),\n",
       "                            ('tick_upper', array([614]))])))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abcde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
