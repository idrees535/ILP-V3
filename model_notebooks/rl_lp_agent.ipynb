{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sys\\nimport os\\nbase_path=\"/mnt/d/Code/tempest/Intelligent-Liquidity-Provisioning-Framework-V2/model_notebooks/rl_lp_agent.ipynb\"\\nreset_env_var=False\\nsys.path.append(base_path)\\nos.chdir(base_path)\\nos.environ[\"PATH\"] += \":.\"\\n\\ndef env_setup(base_path, reset_env_var):\\n    base_path=base_path\\n    sys.path.append(base_path)\\n    os.chdir(base_path)\\n    os.environ[\"PATH\"] += \":.\"\\n    reset_env_var=reset_env_var\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!export PATH=$PATH:.\n",
    "#base_path=\"/home/azureuser/Intelligent-Liquidity-Provisioning-Framework\"\n",
    "base_path=\"/mnt/d/Code/tempest/Intelligent-Liquidity-Provisioning-Framework-V2\"\n",
    "import os\n",
    "os.chdir(base_path)\n",
    "os.environ[\"PATH\"] += \":.\"\n",
    "reset_env_var=False\n",
    "\n",
    "'''\n",
    "import sys\n",
    "import os\n",
    "base_path=\"/mnt/d/Code/tempest/Intelligent-Liquidity-Provisioning-Framework-V2/model_notebooks/rl_lp_agent.ipynb\"\n",
    "reset_env_var=False\n",
    "sys.path.append(base_path)\n",
    "os.chdir(base_path)\n",
    "os.environ[\"PATH\"] += \":.\"\n",
    "\n",
    "def env_setup(base_path, reset_env_var):\n",
    "    base_path=base_path\n",
    "    sys.path.append(base_path)\n",
    "    os.chdir(base_path)\n",
    "    os.environ[\"PATH\"] += \":.\"\n",
    "    reset_env_var=reset_env_var\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env():\n",
    "    import shutil\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    # Define the paths\n",
    "    folder_path = os.path.join(base_path, \"v3_core/build/deployments\")\n",
    "    json_file1_path = os.path.join(base_path, \"model_storage/token_pool_addresses.json\")\n",
    "    json_file2_path = os.path.join(base_path, \"model_storage/liq_positions.json\")\n",
    "\n",
    "    # 1. Delete the folder and its contents\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "\n",
    "    # 2. Delete contents of the first JSON file\n",
    "    with open(json_file1_path, 'w') as file:\n",
    "        file.write(\"{}\")\n",
    "\n",
    "    # 3. Delete contents of the second JSON file and add {}\n",
    "    with open(json_file2_path, 'w') as file:\n",
    "        file.write(\"{}\")\n",
    "        \n",
    "if reset_env_var==True:\n",
    "    reset_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling contracts...\n",
      "  Solc version: 0.7.6\n",
      "  Optimizer: Enabled  Runs: 200\n",
      "  EVM Version: Istanbul\n",
      "Generating build data...\n",
      " - NoDelegateCall\n",
      " - Simpletoken\n",
      " - UniswapV3Factory\n",
      " - UniswapV3Pool\n",
      " - UniswapV3PoolDeployer\n",
      " - IERC20Minimal\n",
      " - IUniswapV3Factory\n",
      " - IUniswapV3Pool\n",
      " - IUniswapV3PoolDeployer\n",
      " - IUniswapV3FlashCallback\n",
      " - IUniswapV3MintCallback\n",
      " - IUniswapV3SwapCallback\n",
      " - IUniswapV3PoolActions\n",
      " - IUniswapV3PoolDerivedState\n",
      " - IUniswapV3PoolEvents\n",
      " - IUniswapV3PoolImmutables\n",
      " - IUniswapV3PoolOwnerActions\n",
      " - IUniswapV3PoolState\n",
      " - BitMath\n",
      " - FixedPoint128\n",
      " - FixedPoint96\n",
      " - FullMath\n",
      " - LiquidityMath\n",
      " - LowGasSafeMath\n",
      " - Oracle\n",
      " - Position\n",
      " - SafeCast\n",
      " - SqrtPriceMath\n",
      " - SwapMath\n",
      " - Tick\n",
      " - TickBitmap\n",
      " - TickMath\n",
      " - TransferHelper\n",
      " - UnsafeMath\n",
      " - BitMathEchidnaTest\n",
      " - BitMathTest\n",
      " - FullMathEchidnaTest\n",
      " - FullMathTest\n",
      " - LiquidityMathTest\n",
      " - LowGasSafeMathEchidnaTest\n",
      " - MockTimeUniswapV3Pool\n",
      " - MockTimeUniswapV3PoolDeployer\n",
      " - NoDelegateCallTest\n",
      " - OracleEchidnaTest\n",
      " - OracleTest\n",
      " - SqrtPriceMathEchidnaTest\n",
      " - SqrtPriceMathTest\n",
      " - SwapMathEchidnaTest\n",
      " - SwapMathTest\n",
      " - TestERC20\n",
      " - TestUniswapV3Callee\n",
      " - TestUniswapV3ReentrantCallee\n",
      " - TestUniswapV3Router\n",
      " - TestUniswapV3SwapPay\n",
      " - TickBitmapEchidnaTest\n",
      " - TickBitmapTest\n",
      " - TickEchidnaTest\n",
      " - TickMathEchidnaTest\n",
      " - TickMathTest\n",
      " - TickOverflowSafetyEchidnaTest\n",
      " - TickTest\n",
      " - UniswapV3PoolSwapTest\n",
      " - UnsafeMathEchidnaTest\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Code/tempest/Intelligent-Liquidity-Provisioning-Framework-V2/django_app/ilp_venv_7/lib/python3.9/site-packages/brownie/network/main.py:44: BrownieEnvironmentWarning: Development network has a block height of 1156\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached to local RPC client listening at '127.0.0.1:8545'...\n",
      "Existing pool:WETH_USDC_3000 having pool address: 0xD9bEa687CB326939b1C8cBB605189F9F22af96Fa loaded\n",
      "Existing pool:ETH_DAI_3000 having pool address: 0x57c58548bF5fC4D80D57cd4185A118727FA92E98 loaded\n",
      "Existing pool:BTC_USDT_3000 having pool address: 0xad459ECAe96A33dB3eBc5F564864049D7610C767 loaded\n",
      "Existing pool:WETH_BTC_3000 having pool address: 0xAbe428621F0a44Ab5C039EDF14d9F85d39821722 loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 13:26:31.080330: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-13 13:26:31.139499: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-13 13:26:31.292930: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-13 13:26:31.292970: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-13 13:26:31.293023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-13 13:26:31.316949: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-13 13:26:31.320097: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 13:26:44.869549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from netlists.uniswapV3.netlist import SimStrategy,SimState,netlist_createLogData\n",
    "from engine.SimEngine import SimEngine\n",
    "from util.globaltokens import weth_usdc_pool,eth_dai_pool,btc_usdt_pool,btc_weth_pool\n",
    "import brownie\n",
    "from util.constants import GOD_ACCOUNT,RL_AGENT_ACCOUNT\n",
    "from util.base18 import toBase18, fromBase18,fromBase128,price_to_valid_tick\n",
    "from model_scripts.plot import train_rewards_plot,eval_rewards_plot,train_raw_actions_plot,train_scaled_actions_plot,train_combined_metrics_plot,train_separate_episode_action_plot\n",
    "from model_scripts.sync_pool_subgraph_data import fetch_inference_pool_data\n",
    "\n",
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_probability as tfp\n",
    "#from tensorflow.keras.layers import Dense\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSimpleEnv(gym.Env):\n",
    "    def __init__(self, agent_budget_usd=10000,alpha = 0.5, exploration_std_dev = 0.01, beta=0.1,penalty_param_magnitude=-1,use_running_statistics=False,action_transform='linear'):\n",
    "        super(DiscreteSimpleEnv, self).__init__()\n",
    "\n",
    "        self.pool=None\n",
    "        self.global_state=None\n",
    "        self.curr_price=None\n",
    "        self.action_lower_bound=None\n",
    "        self.action_upper_bound=None\n",
    "        self.state=None\n",
    "        self.engine=None\n",
    "        self.action_transform=action_transform\n",
    "        self.train_data_log=[]\n",
    "        \n",
    "        self.action_space = gym.spaces.Dict({\n",
    "            'price_relative_lower': gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32),\n",
    "            'price_relative_upper': gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32),\n",
    "        })\n",
    "        \n",
    "        self.reward=0\n",
    "        self.cumulative_reward = 0\n",
    "        self.done=False\n",
    "        self.episode=0\n",
    "        self.step_count=0\n",
    "\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'scaled_curr_price': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'scaled_liquidity': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'scaled_feeGrowthGlobal0x128': gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'scaled_feeGrowthGlobal1x128': gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),   \n",
    "        })\n",
    "        self.agent_budget_usd = agent_budget_usd\n",
    "        self.initial_budget_usd = agent_budget_usd\n",
    "\n",
    "        # Initialize rewrad normalization running statistics\n",
    "        self.reward_mean = 0\n",
    "        self.reward_std = 1\n",
    "        self.reward_count = 0\n",
    "\n",
    "        self.exploration_std_dev = exploration_std_dev\n",
    "        self.penalty=0\n",
    "        self.penalty_param=0\n",
    "        self.penalty_param_magnitude=penalty_param_magnitude\n",
    "\n",
    "        # Initialize running statistics for state normalization\n",
    "        self.use_running_statistics=use_running_statistics\n",
    "        self.curr_price_mean = 0\n",
    "        self.curr_price_std = 1\n",
    "        self.liquidity_mean = 0\n",
    "        self.liquidity_std = 1\n",
    "        self.fee_growth_diff_0 = 0\n",
    "        self.fee_growth_diff_1 = 0\n",
    "        self.fee_growth_0_mean = 0\n",
    "        self.fee_growth_1_mean = 0\n",
    "        self.fee_growth_0_std = 1\n",
    "        self.fee_growth_1_std = 1\n",
    "        self.previous_fee_growth_0 = 0\n",
    "        self.previous_fee_growth_1 = 0\n",
    "\n",
    "        #Obs space scaling param\n",
    "        self.alpha = alpha\n",
    "        #Rewrad scaling param\n",
    "        self.beta=beta\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pool=random.choice([weth_usdc_pool,eth_dai_pool,btc_usdt_pool,btc_weth_pool])\n",
    "        \n",
    "        print(f'Pool selcted for this episode: {self.pool.pool_id}')\n",
    "        sim_strategy = SimStrategy()\n",
    "        sim_state = SimState(ss=sim_strategy,pool=self.pool)\n",
    "\n",
    "        output_dir = \"model_outdir_csv\"\n",
    "        netlist_log_func = netlist_createLogData\n",
    "\n",
    "        #from engine.SimEngine import SimEngine\n",
    "        self.engine = SimEngine(sim_state, output_dir, netlist_log_func)\n",
    "\n",
    "        self.global_state=self.pool.get_global_state()\n",
    "        self.curr_price=self.global_state['curr_price']\n",
    "        self.action_lower_bound=self.curr_price*0.1\n",
    "        self.action_upper_bound=self.curr_price*2\n",
    "        self.state = self.get_obs_space()\n",
    "        \n",
    "        self.done=False\n",
    "        self.reward=0\n",
    "        self.cumulative_reward = 0\n",
    "        self.episode+=1\n",
    "        self.step_count=0\n",
    "    \n",
    "        # Used for evaluation only\n",
    "        self.cumulative_reward_rl_agent = 0\n",
    "        self.cumulative_reward_baseline_agent = 0\n",
    "\n",
    "        self.agent_budget_usd = self.initial_budget_usd\n",
    "         \n",
    "        # reset running statistics for reward normalization\n",
    "        '''\n",
    "        self.reward_mean = 0\n",
    "        self.reward_std = 1\n",
    "        self.reward_count = 0\n",
    "        '''\n",
    "        # reset running statistics for state normalization\n",
    "        '''\n",
    "        self.curr_price_mean = 0\n",
    "        self.curr_price_std = 1\n",
    "        self.liquidity_mean = 0\n",
    "        self.liquidity_std = 1\n",
    "        self.fee_growth_diff_0 = 0\n",
    "        self.fee_growth_diff_1 = 0\n",
    "        self.fee_growth_0_mean = 0\n",
    "        self.fee_growth_1_mean = 0\n",
    "        self.fee_growth_0_std = 1\n",
    "        self.fee_growth_1_std = 1\n",
    "        self.previous_fee_growth_0 = 0\n",
    "        self.previous_fee_growth_1 = 0\n",
    "        '''  \n",
    "        return self.state\n",
    "\n",
    "    def step(self, raw_action):\n",
    "              \n",
    "        # Execute agent's action using pool's interface of add/remove liquidity\n",
    "        mint_tx_receipt,action=self._take_action(raw_action)\n",
    "        \n",
    "        # run uniswap abm env of n_steps\n",
    "        print()\n",
    "        print('Environment Step')\n",
    "        self.engine.reset()\n",
    "        self.engine.run()\n",
    "        print()\n",
    "        \n",
    "        self.state=self.get_obs_space()\n",
    "\n",
    "        scaled_reward,raw_reward,fee_income,impermanent_loss = self._calculate_reward(action,mint_tx_receipt)\n",
    "        self.reward=scaled_reward\n",
    "        self.cumulative_reward += self.reward\n",
    "\n",
    "        self.step_count+=1\n",
    "        \n",
    "        print(f\"episode: {self.episode}, step_count: {self.step_count}, scaled_reward: {self.reward}, raw_reward: {raw_reward} cumulative_reward: {self.cumulative_reward}\")\n",
    "        print(f\"raw_pool_state: {self.pool.get_global_state()}\")\n",
    "        print(f\"sclaed_pool_state: {self.state}\")\n",
    "        print()\n",
    "\n",
    "        self.train_data_log.append((self.episode, self.step_count, action, self.pool.get_global_state(), raw_action, self.state, raw_reward, self.reward, self.cumulative_reward, fee_income, impermanent_loss))\n",
    "\n",
    "        self.done = self._is_done()\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "    def get_obs_space(self):\n",
    "        self.global_state = self.pool.get_global_state()\n",
    "\n",
    "        # Scaling for curr_price and liquidity\n",
    "        curr_price = float(self.global_state['curr_price'])\n",
    "        liquidity = float(self.global_state['liquidity_raw'])\n",
    "        fee_growth_0 = float(self.global_state['feeGrowthGlobal0X128'])\n",
    "        fee_growth_1 = float(self.global_state['feeGrowthGlobal1X128'])\n",
    "\n",
    "        self.curr_price_mean = self.alpha * curr_price + (1 - self.alpha) * self.curr_price_mean\n",
    "        self.curr_price_std = np.sqrt(self.alpha * (curr_price - self.curr_price_mean)**2 + (1 - self.alpha) * self.curr_price_std**2)\n",
    "\n",
    "        self.liquidity_mean = self.alpha * liquidity + (1 - self.alpha) * self.liquidity_mean\n",
    "        self.liquidity_std = np.sqrt(self.alpha * (liquidity - self.liquidity_mean)**2 + (1 - self.alpha) * self.liquidity_std**2)\n",
    "\n",
    "        # Scaling for fee growth differences \n",
    "        self.fee_growth_diff_0 = fee_growth_0 - self.previous_fee_growth_0\n",
    "        self.fee_growth_diff_1 = fee_growth_1 - self.previous_fee_growth_1\n",
    "\n",
    "        self.fee_growth_0_mean = self.alpha * self.fee_growth_diff_0 + (1 - self.alpha) * self.fee_growth_0_mean\n",
    "        self.fee_growth_0_std = np.sqrt(self.alpha * (self.fee_growth_diff_0 - self.fee_growth_0_mean)**2 + (1 - self.alpha) * self.fee_growth_0_std**2)\n",
    "\n",
    "        self.fee_growth_1_mean = self.alpha * self.fee_growth_diff_1 + (1 - self.alpha) * self.fee_growth_1_mean\n",
    "        self.fee_growth_1_std = np.sqrt(self.alpha * (self.fee_growth_diff_1 - self.fee_growth_1_mean)**2 + (1 - self.alpha) * self.fee_growth_1_std**2)\n",
    "\n",
    "        if self.use_running_statistics==True:\n",
    "            #Use running stats\n",
    "            obs = {'scaled_curr_price': (curr_price - self.curr_price_mean) / (self.curr_price_std + 1e-10),'scaled_liquidity': (liquidity - self.liquidity_mean) / (self.liquidity_std + 1e-10),}\n",
    "            obs['scaled_feeGrowthGlobal0x128'] = (self.fee_growth_diff_0 - self.fee_growth_0_mean) / (self.fee_growth_0_std + 1e-10)\n",
    "            obs['scaled_feeGrowthGlobal1x128'] = (self.fee_growth_diff_1 - self.fee_growth_1_mean) / (self.fee_growth_1_std + 1e-10)\n",
    "\n",
    "        else:\n",
    "            # Scale obs space using global stats\n",
    "            obs = {'scaled_curr_price': curr_price/5000,'scaled_liquidity': liquidity/1e24,'scaled_feeGrowthGlobal0x128': fee_growth_0/1e34,'scaled_feeGrowthGlobal1x128': fee_growth_1/1e34}\n",
    "\n",
    "        self.previous_fee_growth_0 = fee_growth_0\n",
    "        self.previous_fee_growth_1 = fee_growth_1\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        self.penalty=0\n",
    "\n",
    "        raw_a, raw_b = action[0, 0].numpy(), action[0, 1].numpy()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a_0 = raw_a + np.random.normal(0, self.exploration_std_dev)\n",
    "        a_1 = raw_b + np.random.normal(0, self.exploration_std_dev)\n",
    "        \n",
    "        if self.action_transform=='linear':\n",
    "            a_0 = np.clip(a_0, 0, 1)\n",
    "            a_1 = np.clip(a_1, 0, 1)\n",
    "            price_lower = self.action_lower_bound + a_0 * (self.action_upper_bound - self.action_lower_bound)/2\n",
    "            price_upper = (self.action_upper_bound - self.action_lower_bound)/2 + a_1 * (self.action_upper_bound - self.action_lower_bound)/2\n",
    "        \n",
    "            # Enabling agent to place range orders too (Only feasible when using multiple positions)\n",
    "            #price_lower = self.action_lower_bound + a_0 * (self.action_upper_bound - self.action_lower_bound)\n",
    "            #price_upper = self.action_lower_bound + a_1 * (self.action_upper_bound - self.action_lower_bound)\n",
    "            \n",
    "        elif self.action_transform=='exp':\n",
    "            #Exponential transforom\n",
    "            exp_a_0 = np.exp(a_0)\n",
    "            exp_a_1 = np.exp(a_1)\n",
    "\n",
    "            # Normalize the exponential values to the range [0, 1]\n",
    "            norm_exp_a_0 = exp_a_0 / (exp_a_0 + exp_a_1)\n",
    "            norm_exp_a_1 = exp_a_1 / (exp_a_0 + exp_a_1)\n",
    "\n",
    "            # Calculate the range between action_lower_bound and action_upper_bound\n",
    "            range_bound = self.action_upper_bound - self.action_lower_bound\n",
    "\n",
    "            # Calculate price_lower and price_upper using the normalized exponential values\n",
    "            price_lower = self.action_lower_bound + norm_exp_a_0 * range_bound\n",
    "            price_upper = self.action_lower_bound + norm_exp_a_1 * range_bound\n",
    "\n",
    "        \n",
    "        # Ensure price_lower is less than price_upper - Add penalty\n",
    "        \n",
    "        if price_lower>price_upper:\n",
    "            price_lower = min(price_lower, price_upper)\n",
    "            price_upper = max(price_lower, price_upper)\n",
    "            self.penalty=self.penalty_param_magnitude\n",
    "\n",
    "        # ensure actions are not too close - Add penalty\n",
    "        min_diff_percentage = 0.05  # 5% difference\n",
    "        price_diff = price_upper - price_lower\n",
    "        \n",
    "        if price_diff < min_diff_percentage * price_lower:\n",
    "            self.penalty+=self.penalty_param_magnitude\n",
    "            price_upper = price_lower + min_diff_percentage * price_lower\n",
    "        \n",
    "        action_dict = {\n",
    "            'price_lower': price_lower,\n",
    "            'price_upper': price_upper\n",
    "        }\n",
    "        \n",
    "        print('RL Agent Action')\n",
    "        print(f\"raw_action: {action}, scaled_action: {action_dict}\")\n",
    "\n",
    "        tick_lower=price_to_valid_tick(action_dict['price_lower'])\n",
    "        tick_upper=price_to_valid_tick(action_dict['price_upper'])\n",
    "        amount=self.agent_budget_usd\n",
    "        mint_tx_receipt=self.pool.add_liquidity(GOD_ACCOUNT, tick_lower, tick_upper, amount, b'')\n",
    "\n",
    "        return mint_tx_receipt,action_dict\n",
    "        \n",
    "    def _calculate_reward(self,action,mint_tx_receipt):\n",
    "       \n",
    "        tick_lower=price_to_valid_tick(action['price_lower'],60)\n",
    "        tick_upper=price_to_valid_tick(action['price_upper'],60)\n",
    "        liquidity=mint_tx_receipt.events['Mint']['amount']\n",
    "\n",
    "        # Collecting fee earned by position\n",
    "        print('Collect fee')\n",
    "        collect_tx_receipt,fee_income = self.pool.collect_fee(GOD_ACCOUNT, tick_lower, tick_upper,poke=True)\n",
    "    \n",
    "        print(\"Burn Position and Collect Tokens\")\n",
    "        # Remove position and collect tokens\n",
    "        burn_tx_receipt=self.pool.remove_liquidity_with_liquidty(GOD_ACCOUNT, tick_lower, tick_upper, liquidity)\n",
    "        collect_tx_receipt,curr_budget_usd = self.pool.collect_fee(GOD_ACCOUNT, tick_lower, tick_upper,poke=False)\n",
    "\n",
    "        # Can use online scaling approach as used for reward for this\n",
    "        rel_portofolio_value = 1 - curr_budget_usd/self.agent_budget_usd\n",
    "        \n",
    "        # Instead of using full budget for next step use previous step's reomved liquidity amount as budget in next step\n",
    "        #self.agent_budget_usd = curr_budget_usd\n",
    "        \n",
    "        # Calculate IL\n",
    "        amount0_initial = mint_tx_receipt.events['Mint']['amount0']\n",
    "        amount1_initial = mint_tx_receipt.events['Mint']['amount1']\n",
    "        \n",
    "        amount0_final = burn_tx_receipt.events['Burn']['amount0']\n",
    "        amount1_final = burn_tx_receipt.events['Burn']['amount1']\n",
    "        self.global_state = self.pool.get_global_state()\n",
    "        pool_price = float(self.global_state['curr_price'])\n",
    "\n",
    "        value_initial = (amount0_initial * pool_price + amount1_initial) / 1e18\n",
    "        value_final = (amount0_final * pool_price + amount1_final) / 1e18\n",
    "\n",
    "        impermanent_loss = value_initial - value_final\n",
    "\n",
    "        if fee_income==0:\n",
    "            self.penalty_param+= 0.05\n",
    "            self.penalty += self.penalty_param_magnitude*(1+self.penalty_param)\n",
    "\n",
    "        print(f'fee_earned:{fee_income}, impermannet_loss: {impermanent_loss}, penalty: {self.penalty}, initial_agent_portofolio_value: {value_initial}, final_agent_portofolio_value: {value_final}, reward_mean: {self.reward_mean}, rewrad_std_dev: {self.reward_std}, reward_count: {self.reward_count}')\n",
    "        print()\n",
    "        \n",
    "        raw_reward = fee_income - impermanent_loss + self.penalty\n",
    "        \n",
    "        if self.penalty==0:\n",
    "            self.reward_count += 1\n",
    "            #new_mean = self.reward_mean + (raw_reward - self.reward_mean) / self.reward_count\n",
    "            #new_std = ((self.reward_std ** 2 + (raw_reward - self.reward_mean) * (raw_reward - new_mean)) / self.reward_count) ** 0.5\n",
    "            new_mean = self.beta * raw_reward + (1 - self.beta) * self.reward_mean\n",
    "            new_std = np.sqrt(self.beta * ((raw_reward - new_mean) ** 2) + (1 - self.beta) * (self.reward_std ** 2))\n",
    "            self.reward_mean = new_mean\n",
    "            self.reward_std = new_std\n",
    "\n",
    "        #scaled_reward = (raw_reward - self.reward_mean) / (self.reward_std + 1e-10)\n",
    "        scaled_reward = raw_reward*10\n",
    "\n",
    "        #Reset penlaty for next step\n",
    "        self.penalty=0\n",
    "\n",
    "        return scaled_reward,raw_reward,fee_income, impermanent_loss\n",
    "\n",
    "    def _is_done(self):\n",
    "        \n",
    "        max_reward_threshold = 100000\n",
    "        min_reward_threshold= -100000\n",
    "        max_budget_threshold = 1.5*self.initial_budget_usd\n",
    "        min_budget_threshold = 0.5*self.initial_budget_usd\n",
    "\n",
    "        if self.cumulative_reward >= max_reward_threshold or self.cumulative_reward<=min_reward_threshold or self.agent_budget_usd>max_budget_threshold or self.agent_budget_usd<min_budget_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "#env=DiscreteSimpleEnv(agent_budget_usd=10000,use_running_statistics=False)\n",
    "#n_actions = sum(action_space.shape[0] for action_space in env.action_space.values())\n",
    "#input_dims = sum(np.prod(env.observation_space.spaces[key].shape) for key in env.observation_space.spaces.keys())\n",
    "\n",
    "class DiscreteSimpleEnvEval(DiscreteSimpleEnv):\n",
    "    def __init__(self, agent_budget_usd, percentage_range=0.3, seed=32,penalty_param_magnitude=0,use_running_statistics=False,action_transform='linear'):\n",
    "        #super().__init__(agent_budget_usd)\n",
    "        # Call to the parent class's __init__ method\n",
    "        super(DiscreteSimpleEnvEval, self).__init__(agent_budget_usd=agent_budget_usd, alpha=0.5, exploration_std_dev=0.01, beta=0.1, penalty_param_magnitude=penalty_param_magnitude, use_running_statistics=use_running_statistics,action_transform=action_transform)\n",
    "        self.percentage_range = percentage_range\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.cumulative_reward_rl_agent = 0\n",
    "        self.cumulative_reward_baseline_agent = 0\n",
    "        self.penalty_param_magnitude=penalty_param_magnitude\n",
    "        self.eval_data_log=[]\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        # Disable exploration noise\n",
    "        self.exploration_std_dev = 0.0\n",
    "        return super()._take_action(action)\n",
    "    \n",
    "    def step(self, raw_action_rl_agent):\n",
    "        # The RL agent takes an action\n",
    "        mint_tx_receipt_rl_agent, action_rl_agent = self._take_action(raw_action_rl_agent)\n",
    "        raw_action_baseline_agent=self.baseline_agent_policy()\n",
    "        # The baseline agent takes an action\n",
    "        mint_tx_receipt_baseline_agent, action_baseline_agent = self._take_action_baseline(raw_action_baseline_agent)\n",
    "\n",
    "        # Perform environment step\n",
    "        print('Environment Step')\n",
    "        self.engine.reset()\n",
    "        self.engine.run()\n",
    "        print()\n",
    "\n",
    "        # Calculate rewards for both agents\n",
    "        scaled_reward_rl_agent, raw_reward_rl_agent, fee_income_rl_agent, impermanent_loss_rl_agent = self._calculate_reward(action_rl_agent, mint_tx_receipt_rl_agent)\n",
    "        scaled_reward_baseline_agent, raw_reward_baseline_agent, fee_income_baseline_agent, impermanent_loss_baseline_agent = self._calculate_reward(action_baseline_agent, mint_tx_receipt_baseline_agent)\n",
    "\n",
    "        # Update cumulative rewards\n",
    "        self.cumulative_reward_rl_agent += scaled_reward_rl_agent\n",
    "        self.cumulative_reward_baseline_agent += scaled_reward_baseline_agent\n",
    "\n",
    "        self.step_count+=1\n",
    "        # Print rewards and cumulative rewards for both agents\n",
    "        print(f\"episode: {self.episode}, step_count: {self.step_count}\")\n",
    "        print(f\"rl_agent_scaled_reward: {scaled_reward_rl_agent}, rl_agent_raw_reward: {raw_reward_rl_agent}, rl_agent_cumulative_reward: {self.cumulative_reward_rl_agent}\")\n",
    "        print(f\"baseline_agent_scaled_reward: {scaled_reward_baseline_agent}, baseline_agent_raw_reward: {raw_reward_baseline_agent}, baseline_agent_cumulative_reward: {self.cumulative_reward_baseline_agent}\")\n",
    "        print(f\"raw_pool_state: {self.pool.get_global_state()}\")\n",
    "        print(f\"sclaed_pool_state: {self.state}\")\n",
    "        print()\n",
    "      \n",
    "\n",
    "        # Update the state and check if the episode is done\n",
    "        self.state = self.get_obs_space()\n",
    "        self.done = self._is_done()\n",
    "        self.eval_data_log.append((self.episode, self.step_count, self.pool.get_global_state(), raw_action_rl_agent,action_rl_agent,raw_action_baseline_agent,action_baseline_agent, self.state, raw_reward_rl_agent, raw_reward_baseline_agent,scaled_reward_rl_agent,scaled_reward_baseline_agent, self.cumulative_reward_rl_agent, self.cumulative_reward_baseline_agent, fee_income_rl_agent, impermanent_loss_rl_agent,fee_income_baseline_agent,impermanent_loss_baseline_agent))\n",
    "        # Return the necessary information\n",
    "        return self.state, raw_reward_rl_agent, self.done, {}\n",
    "    \n",
    "    def _take_action_baseline(self, action_dict):\n",
    "        \n",
    "        print('Baseline Agent Action')\n",
    "        print(f\"action: {action_dict}\")\n",
    "\n",
    "        tick_lower=price_to_valid_tick(action_dict['price_lower'])\n",
    "        tick_upper=price_to_valid_tick(action_dict['price_upper'])\n",
    "        amount=self.agent_budget_usd\n",
    "\n",
    "        mint_tx_receipt=self.pool.add_liquidity(GOD_ACCOUNT, tick_lower, tick_upper, amount, b'')\n",
    "\n",
    "        return mint_tx_receipt,action_dict\n",
    "    \n",
    "    \n",
    "    def baseline_agent_policy(self):\n",
    "        global_state = self.pool.get_global_state()\n",
    "        raw_curr_price = global_state['curr_price']\n",
    "        \n",
    "        # Calculate the price range based on the raw current price\n",
    "        lower_price = raw_curr_price * (1 - self.percentage_range)\n",
    "        upper_price = raw_curr_price * (1 + self.percentage_range)\n",
    "\n",
    "        action_baseline={\n",
    "            'price_lower':lower_price,\n",
    "            'price_upper':upper_price\n",
    "        }\n",
    "        \n",
    "        return action_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_dims, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, input_dims))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_dims))  \n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "    \n",
    "    def clear(self):\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *self.state_memory.shape[1:]))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *self.new_state_memory.shape[1:]))\n",
    "        self.action_memory = np.zeros((self.mem_size, *self.action_memory.shape[1:]))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "        \n",
    "class DDPG_Actor(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DDPG_Actor, self).__init__()\n",
    "\n",
    "        self.bn_input = tf.keras.layers.BatchNormalization()\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.output_layer = tf.keras.layers.Dense(n_actions, activation='sigmoid')  # Two output units for 'price_lower' and 'price_upper'\n",
    "\n",
    "    def call(self, state):\n",
    "        state = self.bn_input(state)\n",
    "        x = self.fc1(state)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn2(x)\n",
    "        actions = self.output_layer(x)\n",
    "        \n",
    "        return actions\n",
    "        \n",
    "class DDPG_Critic(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DDPG_Critic, self).__init__()\n",
    "        \n",
    "        self.bn_state = tf.keras.layers.BatchNormalization()\n",
    "        self.bn_action = tf.keras.layers.BatchNormalization()\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.q = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        state = self.bn_state(state)\n",
    "        action = self.bn_action(action)\n",
    "        x = tf.concat([state, action], axis=1) \n",
    "        x = self.fc1(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        q = self.q(x)\n",
    "        return q\n",
    "    \n",
    "class DDPG:\n",
    "    def __init__(self, alpha=0.001, beta=0.002, input_dims=[8], tau=0.005, env=None,gamma=0.99, n_actions=2, max_size=1000000, batch_size=64,training=True,max_grad_norm=10):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = DDPG_Actor(n_actions=n_actions)\n",
    "        self.critic = DDPG_Critic(n_actions=n_actions)\n",
    "        self.target_actor = DDPG_Actor(n_actions=n_actions)\n",
    "        self.target_critic = DDPG_Critic(n_actions=n_actions)\n",
    "\n",
    "        self.actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=beta))\n",
    "        self.target_actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.target_critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=beta))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "        self.env = env\n",
    "        self.training=training\n",
    "        self.max_grad_norm=max_grad_norm\n",
    "\n",
    "        # For tensorboard logging\n",
    "        self.log_dir = os.path.join(base_path,'model_storage/tensorboard_ddpg_logs')\n",
    "        self.train_summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_actor.weights\n",
    "        for i, weight in enumerate(self.actor.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_actor.set_weights(weights)\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_critic.weights\n",
    "        for i, weight in enumerate(self.critic.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        flat_state = self.flatten_state(state)\n",
    "        flat_action = self.flatten_action(action)\n",
    "        flat_new_state = self.flatten_state(new_state)\n",
    "        self.memory.store_transition(flat_state, flat_action, reward, flat_new_state, done)\n",
    "        \n",
    "    def flatten_state(self,state_dict):\n",
    "        scaled_curr_price = float(state_dict['scaled_curr_price'])\n",
    "        scaled_liquidity = float(state_dict['scaled_liquidity'])\n",
    "        scaled_fee_growth_0 = float(state_dict['scaled_feeGrowthGlobal0x128'])\n",
    "        scaled_fee_growth_1 = float(state_dict['scaled_feeGrowthGlobal1x128'])\n",
    "        \n",
    "        return np.array([scaled_curr_price, scaled_liquidity, scaled_fee_growth_0, scaled_fee_growth_1])\n",
    "\n",
    "    def unflatten_state(self,state_array):\n",
    "        return {\n",
    "            'scaled_curr_price': state_array[0],\n",
    "            'scaled_liquidity': state_array[1],\n",
    "            'scaled_feeGrowthGlobal0x128': state_array[2],\n",
    "            'scaled_feeGrowthGlobal1x128': state_array[3]\n",
    "        }\n",
    "\n",
    "    def flatten_action(self,action):\n",
    "        return tf.reshape(action, [-1])\n",
    "\n",
    "    def unflatten_action(self,action):\n",
    "        return tf.reshape(action, [1, -1])\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        state_dict = state\n",
    "        state_dict_float = {\n",
    "            key: float(value) for key, value in state_dict.items()\n",
    "        }\n",
    "\n",
    "        state_array = np.array(list(state_dict_float.values()), dtype=np.float32)\n",
    "        state_array = state_array.reshape(1, -1)\n",
    "        state_tensor = tf.convert_to_tensor(state_array, dtype=tf.float32)\n",
    "        raw_actions_tensor = self.actor(state_tensor,training=False)\n",
    "        \n",
    "        return raw_actions_tensor\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        #print(f\"{state},{action},{reward},{new_state}\")\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(states_,training=False)\n",
    "            critic_value_ = tf.squeeze(self.target_critic(states_, target_actions,training=False), 1)\n",
    "            critic_value = tf.squeeze(self.critic(states, actions,training=True), 1)\n",
    "            target = rewards + self.gamma*critic_value_*(1-done)\n",
    "            critic_loss = tf.keras.losses.MSE(target, critic_value)\n",
    "\n",
    "        critic_network_gradient = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        critic_network_gradient, _ = tf.clip_by_global_norm(critic_network_gradient, self.max_grad_norm)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_network_gradient, self.critic.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_policy_actions = self.actor(states,training=True)\n",
    "            actor_loss = -self.critic(states, new_policy_actions,training=True)\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        actor_network_gradient, _ = tf.clip_by_global_norm(actor_network_gradient, self.max_grad_norm)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_network_gradient, self.actor.trainable_variables))\n",
    "        \n",
    "        print(f\"Actor_Loss: {actor_loss.numpy()}, Critic_Loss: {critic_loss.numpy()}\")\n",
    "       \n",
    "        with self.train_summary_writer.as_default():\n",
    "            tf.summary.scalar('critic_loss', critic_loss.numpy(), step=self.memory.mem_cntr)\n",
    "            tf.summary.scalar('actor_loss', actor_loss.numpy(), step=self.memory.mem_cntr)\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "class DDGPEval(DDPG):\n",
    "    def choose_action(self, state):\n",
    "        # Disable exploration noise\n",
    "        action = super().choose_action(state)\n",
    "        return action\n",
    "    \n",
    "# Training Loop\n",
    "def train_ddpg_agent(max_steps=100, n_episodes=10, model_name='model_storage/ddpg/ddpg_2',alpha=0.001, beta=0.001, tau=0.8,batch_size=50, training=True,agent_budget_usd=10000,use_running_statistics=False,action_transform='linear'):\n",
    "    env=DiscreteSimpleEnv(agent_budget_usd=agent_budget_usd,use_running_statistics=use_running_statistics,action_transform=action_transform)\n",
    "    n_actions = sum(action_space.shape[0] for action_space in env.action_space.values())\n",
    "    input_dims = sum(np.prod(env.observation_space.spaces[key].shape) for key in env.observation_space.spaces.keys())\n",
    "    ddpg_agent = DDPG(alpha=alpha, beta=beta, input_dims=input_dims, tau=tau, env=env, n_actions=n_actions, batch_size=batch_size, training=training)\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = ddpg_agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            ddpg_agent.remember(state, action, reward, next_state, done)\n",
    "            ddpg_agent.learn()\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode {i+1}: Reward = {episode_reward}\")\n",
    "        #ddpg_agent.memory.clear()\n",
    "    \n",
    "    # Create dummy data for model input shape\n",
    "    dummy_state = np.random.random((1, input_dims))\n",
    "    dummy_action = np.random.random((1, n_actions))\n",
    "\n",
    "    # Run dummy data through models to build them\n",
    "    ddpg_agent.actor(dummy_state)\n",
    "    ddpg_agent.critic(dummy_state, dummy_action)\n",
    "\n",
    "    \n",
    "    model_base_path = os.path.join(base_path,model_name)\n",
    "    ddpg_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ddpg_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "\n",
    "    # Saved Trained weights\n",
    "    ddpg_agent.actor.save_weights(ddpg_actor_model_path)\n",
    "    ddpg_agent.critic.save_weights(ddpg_critic_model_path)\n",
    "\n",
    "    # Save trained model\n",
    "    ddpg_agent.actor.save(ddpg_actor_model_path)\n",
    "    ddpg_agent.critic.save(ddpg_critic_model_path)\n",
    "\n",
    "    ddpg_train_data_log=env.train_data_log\n",
    "\n",
    "    return ddpg_train_data_log,ddpg_actor_model_path,ddpg_critic_model_path\n",
    "\n",
    "def ddpg_training_vis(ddpg_train_data_log):\n",
    "    df_data = []\n",
    "\n",
    "    for entry in ddpg_train_data_log:\n",
    "        episode, step_count, scaled_action, raw_state, tensor_data, scaled_state, raw_reward, scaled_reward, cumulative_reward, fee_earned, impermanent_loss = entry\n",
    "        \n",
    "        # Extract raw_action values from tensor_data\n",
    "        raw_action_0 = float(tensor_data[0][0].numpy())\n",
    "        raw_action_1 = float(tensor_data[0][1].numpy())\n",
    "\n",
    "        scaled_action_0 = scaled_action['price_lower']\n",
    "        scaled_action_1 = scaled_action['price_upper']\n",
    "        \n",
    "        # Combine all data into a single dictionary\n",
    "        data = {\n",
    "            'episode': episode,\n",
    "            'step_count': step_count,\n",
    "            'scaled_action_0':scaled_action_0,\n",
    "            'scaled_action_1':scaled_action_1,\n",
    "            'raw_reward': raw_reward,\n",
    "            'scaled_reward': scaled_reward,\n",
    "            'fee_earned': fee_earned,\n",
    "            'impermanent_loss': impermanent_loss,\n",
    "            'cumulative_reward': cumulative_reward,\n",
    "            'raw_action_0': raw_action_0,\n",
    "            'raw_action_1': raw_action_1,\n",
    "        }\n",
    "        \n",
    "        # Add raw_action, global_state, and state data\n",
    "        #data.update(scaled_action)\n",
    "        data.update(raw_state)\n",
    "        data.update(scaled_state)\n",
    "        \n",
    "        df_data.append(data)\n",
    "\n",
    "    ddpg_train_data_df = pd.DataFrame(df_data)\n",
    "    ddpg_train_data_df.to_csv('model_outdir_csv/ddpg_agent_train_data.csv', index=False)\n",
    "\n",
    "    train_rewards_plot(ddpg_train_data_df)\n",
    "    train_raw_actions_plot(ddpg_train_data_df)\n",
    "    train_scaled_actions_plot(ddpg_train_data_df)\n",
    "    train_combined_metrics_plot(ddpg_train_data_df)\n",
    "    #train_separate_episode_action_plot(ddpg_train_data_df)\n",
    "\n",
    "def eval_ddpg_agent(eval_steps=100,eval_episodes=2,model_name='model_storage/ddpg/200_100_step_running_stats_lstm_bn_global_obs_norm',percentage_range=0.5,agent_budget_usd=10000,use_running_statistics=False):\n",
    "\n",
    "    eval_env = DiscreteSimpleEnvEval(agent_budget_usd=agent_budget_usd,percentage_range=percentage_range, seed=42,use_running_statistics=use_running_statistics)\n",
    "    n_actions = sum(action_space.shape[0] for action_space in eval_env.action_space.values())\n",
    "    input_dims = sum(np.prod(eval_env.observation_space.spaces[key].shape) for key in eval_env.observation_space.spaces.keys())\n",
    "    ddpg_eval_agent = DDGPEval(env=eval_env, n_actions=n_actions, input_dims=input_dims, training=False)\n",
    "    model_base_path = os.path.join(base_path, model_name)\n",
    "\n",
    "    ddpg_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ddpg_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "\n",
    "    ddpg_eval_agent.actor.load_weights(ddpg_actor_model_path)\n",
    "    ddpg_eval_agent.critic.load_weights(ddpg_critic_model_path)\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        state = eval_env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(eval_steps):\n",
    "            action = ddpg_eval_agent.choose_action(state)\n",
    "            next_state, reward, done, _ = eval_env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode {episode+1}/{eval_episodes}, Reward: {episode_reward}\")\n",
    "\n",
    "    ppo_eval_data_log=eval_env.eval_data_log\n",
    "\n",
    "    return ppo_eval_data_log\n",
    "\n",
    "def ddpg_eval_vis(ppo_eval_data_log):\n",
    "    df_eval_data = []\n",
    "\n",
    "    for entry in ppo_eval_data_log:\n",
    "        (episode, step_count, global_state, raw_action_rl_agent, action_rl_agent, \n",
    "        raw_action_baseline_agent, action_baseline_agent, state, \n",
    "        raw_reward_rl_agent, raw_reward_baseline_agent,scaled_reward_rl_agent,scaled_reward_baseline_agent, cumulative_reward_rl_agent, \n",
    "        cumulative_reward_baseline_agent, fee_income_rl_agent, impermanent_loss_rl_agent, \n",
    "        fee_income_baseline_agent, impermanent_loss_baseline_agent) = entry\n",
    "        \n",
    "        # Extract raw_action values for RL agent\n",
    "        raw_action_rl_agent_0 = float(raw_action_rl_agent[0][0].numpy())\n",
    "        raw_action_rl_agent_1 = float(raw_action_rl_agent[0][1].numpy())\n",
    "\n",
    "        scaled_action_rl_agent_0 = action_rl_agent['price_lower']\n",
    "        scaled_action_rl_agent_1 = action_rl_agent['price_upper']\n",
    "        \n",
    "        # Extract raw_action values for baseline agent\n",
    "        raw_action_baseline_agent_0 = raw_action_baseline_agent['price_lower']\n",
    "        raw_action_baseline_agent_1 = raw_action_baseline_agent['price_upper']\n",
    "        \n",
    "        # Combine all data into a single dictionary\n",
    "        data = {\n",
    "            'episode': episode,\n",
    "            'step_count': step_count,\n",
    "            'raw_reward_rl_agent': raw_reward_rl_agent,\n",
    "            'scaled_reward_rl_agent':scaled_reward_rl_agent,\n",
    "            'cumulative_reward_rl_agent': cumulative_reward_rl_agent,\n",
    "            'scaled_action_rl_agent_0':scaled_action_rl_agent_0,\n",
    "            'scaled_action_rl_agent_1':scaled_action_rl_agent_1,\n",
    "            'fee_income_rl_agent': fee_income_rl_agent,\n",
    "            'impermanent_loss_rl_agent': impermanent_loss_rl_agent,\n",
    "            'raw_reward_baseline_agent': raw_reward_baseline_agent,\n",
    "            'scaled_reward_baseline_agent':scaled_reward_baseline_agent,\n",
    "            'cumulative_reward_baseline_agent': cumulative_reward_baseline_agent,\n",
    "            'raw_action_baseline_agent_0': raw_action_baseline_agent_0,\n",
    "            'raw_action_baseline_agent_1': raw_action_baseline_agent_1,\n",
    "            'fee_income_baseline_agent': fee_income_baseline_agent,\n",
    "            'impermanent_loss_baseline_agent': impermanent_loss_baseline_agent,\n",
    "            'raw_action_rl_agent_0': raw_action_rl_agent_0,\n",
    "            'raw_action_rl_agent_1': raw_action_rl_agent_1,\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Add action, global_state, and state data\n",
    "        \n",
    "        data.update(global_state)\n",
    "        data.update(state)\n",
    "        \n",
    "        df_eval_data.append(data)\n",
    "\n",
    "    ddpg_eval_data_df = pd.DataFrame(df_eval_data)\n",
    "    ddpg_eval_data_df.to_csv('model_outdir_csv/ddpg_agent_eval_data.csv', index=False)\n",
    "    eval_rewards_plot(ddpg_eval_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent (Stochastic Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, buffer_size, observation_dims, n_actions):\n",
    "        self.states = np.zeros((buffer_size, observation_dims), dtype=np.float32)\n",
    "        self.actions = np.zeros((buffer_size, n_actions), dtype=np.float32)\n",
    "        self.rewards = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.dones = np.zeros(buffer_size, dtype=bool)\n",
    "        self.log_probs = np.zeros((buffer_size, n_actions), dtype=np.float32)\n",
    "        self.values = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.next_values = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.ptr, self.max_size = 0, buffer_size\n",
    "        self.buffer_size=buffer_size\n",
    "        self.observation_dims=observation_dims\n",
    "        self.n_actions=n_actions\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def store_transition(self, state, action, reward, done, log_prob, value, next_value):\n",
    "        index = self.ptr % self.max_size\n",
    "        self.states[index] = state\n",
    "        self.actions[index] = action\n",
    "        self.rewards[index] = reward\n",
    "        self.dones[index] = done\n",
    "        self.log_probs[index] = log_prob\n",
    "        self.values[index] = value\n",
    "        self.next_values[index] = next_value\n",
    "        self.ptr += 1\n",
    "\n",
    "    def sample(self):\n",
    "        self.ptr = 0\n",
    "        return (self.states, self.actions, self.rewards, self.dones, \n",
    "                self.log_probs, self.values, self.next_values)\n",
    "    def reset(self):\n",
    "        self.states = np.zeros((self.buffer_size, self.observation_dims), dtype=np.float32)\n",
    "        self.actions = np.zeros((self.buffer_size, self.n_actions), dtype=np.float32)\n",
    "        self.rewards = np.zeros(self.buffer_size, dtype=np.float32)\n",
    "        self.dones = np.zeros(self.buffer_size, dtype=bool)\n",
    "        self.log_probs = np.zeros((self.buffer_size, self.n_actions), dtype=np.float32)\n",
    "        self.values = np.zeros(self.buffer_size, dtype=np.float32)\n",
    "        self.next_values = np.zeros(self.buffer_size, dtype=np.float32)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def is_full(self):\n",
    "        return self.ptr >= self.buffer_size\n",
    "    \n",
    "class PPO_Actor(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(PPO_Actor, self).__init__()\n",
    "\n",
    "        self.bn_input = tf.keras.layers.BatchNormalization()\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        # Output the mean of the actions\n",
    "        self.mu = tf.keras.layers.Dense(n_actions, activation='sigmoid')\n",
    "        # Output the standard deviation of the actions (log std for numerical stability)\n",
    "        self.sigma = tf.keras.layers.Dense(n_actions, activation='sigmoid')\n",
    "\n",
    "    def call(self, state):\n",
    "        state = self.bn_input(state)\n",
    "        x = self.fc1(state)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "class PPO_Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PPO_Critic, self).__init__()\n",
    "\n",
    "        self.bn_state = tf.keras.layers.BatchNormalization()\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.q = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.bn_state(state) \n",
    "        x = self.fc1(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, n_actions,observation_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95, policy_clip=0.2, buffer_size=64, max_grad_norm=0.5, n_epochs=1,training=True):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.policy_clip = policy_clip\n",
    "        self.buffer_size=buffer_size\n",
    "        self.optimizer=tf.keras.optimizers.Adam(learning_rate=alpha)\n",
    "        self.observation_dims=observation_dims\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.n_epochs=n_epochs\n",
    "        \n",
    "        self.actor = PPO_Actor(n_actions)\n",
    "        self.critic = PPO_Critic()\n",
    "        self.actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "\n",
    "        self.rollout_buffer = RolloutBuffer(self.buffer_size, observation_dims, n_actions)\n",
    "\n",
    "        self.env=env\n",
    "        self.training=training\n",
    "        \n",
    "\n",
    "         # For tensorboard logging\n",
    "        self.log_dir = os.path.join(base_path,'model_storage/tensorboard_ppo_logs')\n",
    "        self.train_summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self.tensorboard_counter=0\n",
    " \n",
    "    def choose_action(self, state):\n",
    "\n",
    "        state_dict = state\n",
    "        state_dict_float = {\n",
    "            key: float(value) for key, value in state_dict.items()\n",
    "        }\n",
    "\n",
    "        state_array = np.array(list(state_dict_float.values()), dtype=np.float32)\n",
    "        state_array = state_array.reshape(1, -1)\n",
    "        state_tensor = tf.convert_to_tensor(state_array, dtype=tf.float32)\n",
    "\n",
    "        mu, sigma = self.actor(state_tensor,training=self.training)\n",
    "        \n",
    "        action_prob = tfp.distributions.Normal(mu, sigma)\n",
    "        action = action_prob.sample()\n",
    "        \n",
    "        #Action clipping\n",
    "        #action = tf.clip_by_value(action, 0, 1)\n",
    "        log_prob = action_prob.log_prob(action)\n",
    "        print(f\"mu: {mu}, sigma: {sigma}, action_prob: {action_prob}, action: {action}, log_prob: {log_prob}\")\n",
    "        return action,log_prob\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done, log_prob):\n",
    "        \n",
    "        flat_state = self.flatten_state(state)\n",
    "        flat_action = self.flatten_action(action)\n",
    "        flat_next_state = self.flatten_state(next_state)\n",
    "        value = self.critic(tf.convert_to_tensor([flat_state], dtype=tf.float32))\n",
    "        next_value = self.critic(tf.convert_to_tensor([flat_next_state], dtype=tf.float32))\n",
    "        self.rollout_buffer.store_transition(flat_state, flat_action, reward, done, log_prob, value, next_value)\n",
    "    \n",
    "    def learn(self):\n",
    "        states, actions, rewards, dones, old_log_probs, values, next_values = self.rollout_buffer.sample()\n",
    "        returns, advantages = self.compute_gae(rewards, values, next_values, dones, self.gamma, self.gae_lambda)\n",
    "        \n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        old_log_probs = tf.convert_to_tensor(old_log_probs, dtype=tf.float32)\n",
    "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "        returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "        # Update policy and value networks for a number of epochs\n",
    "        for _ in range(self.n_epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                total_loss = self.ppo_loss(states, actions, old_log_probs, advantages, returns, self.policy_clip)\n",
    "            gradients = tape.gradient(total_loss, self.actor.trainable_variables + self.critic.trainable_variables)\n",
    "            # Gradient clipping\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, self.max_grad_norm)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.actor.trainable_variables + self.critic.trainable_variables))\n",
    "                \n",
    "        self.rollout_buffer.reset()\n",
    "       \n",
    "    def compute_gae(self, rewards, values, next_values, dones, gamma=0.99, lam=0.95):\n",
    "        gae = 0\n",
    "        returns = np.zeros_like(rewards)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + gamma * next_values[t] * (1 - dones[t]) - values[t]\n",
    "            gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "            returns[t] = gae + values[t]\n",
    "        advantages = returns - values\n",
    "        return returns, (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "    def ppo_loss(self, states, actions, old_log_probs, advantages, returns, clip_param=0.8):\n",
    "        mu, sigma = self.actor(states)\n",
    "        values = tf.squeeze(self.critic(states))\n",
    "\n",
    "        # Calculate new log probabilities using the updated policy\n",
    "        new_policy = tfp.distributions.Normal(mu, sigma)\n",
    "        new_log_probs = new_policy.log_prob(actions)\n",
    "\n",
    "        # Policy loss\n",
    "        ratios = tf.exp(new_log_probs - old_log_probs)\n",
    "        #ratios = tf.reduce_mean(ratios, axis=1)\n",
    "        advantages = tf.expand_dims(advantages, -1)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = tf.clip_by_value(ratios, 1 - clip_param, 1 + clip_param) * advantages\n",
    "        policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))\n",
    "        # Value loss\n",
    "        value_loss = tf.reduce_mean(tf.square(returns - values))\n",
    "        # Total loss\n",
    "        total_loss = policy_loss + 0.5 * value_loss\n",
    "        with self.train_summary_writer.as_default():\n",
    "            tf.summary.scalar('total_loss', total_loss.numpy(), step=self.tensorboard_counter)\n",
    "            tf.summary.scalar('policy_loss', total_loss.numpy(), step=self.tensorboard_counter)\n",
    "            tf.summary.scalar('value_loss', total_loss.numpy(), step=self.tensorboard_counter)\n",
    "\n",
    "        print(f\"total_loss:{total_loss}, policy_loss:{policy_loss}, value_loss:{value_loss}, advantages:{advantages}, returns:{returns}\")\n",
    "        self.tensorboard_counter+=1\n",
    "        return total_loss\n",
    "          \n",
    "    def flatten_state(self,state_dict):\n",
    "        scaled_curr_price = float(state_dict['scaled_curr_price'])\n",
    "        scaled_liquidity = float(state_dict['scaled_liquidity'])\n",
    "        scaled_fee_growth_0 = float(state_dict['scaled_feeGrowthGlobal0x128'])\n",
    "        scaled_fee_growth_1 = float(state_dict['scaled_feeGrowthGlobal1x128'])\n",
    "        \n",
    "        return np.array([scaled_curr_price, scaled_liquidity, scaled_fee_growth_0, scaled_fee_growth_1])\n",
    "\n",
    "    def unflatten_state(self,state_array):\n",
    "        return {\n",
    "            'scaled_curr_price': state_array[0],\n",
    "            'scaled_liquidity': state_array[1],\n",
    "            'scaled_feeGrowthGlobal0x128': state_array[2],\n",
    "            'scaled_feeGrowthGlobal1x128': state_array[3]\n",
    "        }\n",
    "\n",
    "    def flatten_action(self,action):\n",
    "        return tf.reshape(action, [-1])\n",
    "\n",
    "    def unflatten_action(self,action):\n",
    "        return tf.reshape(action, [1, -1])\n",
    "\n",
    "    def map_indices_to_action_values(self, action_indices):\n",
    "        action_dict = {\n",
    "            'price_relative_lower': action_indices[0],\n",
    "            'price_relative_upper': action_indices[1]\n",
    "        }\n",
    "        return action_dict\n",
    "\n",
    "class PPOEval(PPO):\n",
    "    def choose_action(self, state):\n",
    "        # Disable exploration noise\n",
    "        action = super().choose_action(state)\n",
    "        return action\n",
    "\n",
    "def train_ppo_agent(max_steps=100, n_episodes=10, model_name='model_storage/ppo/ppo2', buffer_size=50,n_epochs=10, gamma=0.5, alpha=0.01, gae_lambda=0.75, policy_clip=0.8, max_grad_norm=10,agent_budget_usd=10000,use_running_statistics=False,action_transform='linear'):\n",
    "    \n",
    "    env=DiscreteSimpleEnv(agent_budget_usd=agent_budget_usd,use_running_statistics=use_running_statistics, action_transform=action_transform)\n",
    "    n_actions = sum(action_space.shape[0] for action_space in env.action_space.values())\n",
    "    input_dims = sum(np.prod(env.observation_space.spaces[key].shape) for key in env.observation_space.spaces.keys())\n",
    "    ppo_agent = PPO(env, n_actions, observation_dims=input_dims,buffer_size=buffer_size,n_epochs=n_epochs, gamma=gamma, alpha=alpha, gae_lambda=gae_lambda, policy_clip=policy_clip, max_grad_norm=max_grad_norm)\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action,log_prob = ppo_agent.choose_action(state)\n",
    "        \n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            ppo_agent.remember(state, action, reward, next_state, done,log_prob)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if ppo_agent.rollout_buffer.is_full():\n",
    "                ppo_agent.learn()\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode {i+1}: Reward = {episode_reward}\")\n",
    "\n",
    "    ppo_model_base_path = os.path.join(base_path,model_name)\n",
    "    ppo_actor_model_path = os.path.join(ppo_model_base_path, 'actor')\n",
    "    ppo_critic_model_path = os.path.join(ppo_model_base_path, 'critic')\n",
    "    \n",
    "    # Save trained weights\n",
    "    ppo_agent.actor.save_weights(ppo_actor_model_path)\n",
    "    ppo_agent.critic.save_weights(ppo_critic_model_path)\n",
    "\n",
    "    # Save trained model\n",
    "    ppo_agent.actor.save(ppo_actor_model_path)\n",
    "    ppo_agent.critic.save(ppo_critic_model_path)\n",
    "\n",
    "    ppo_train_data_log=env.train_data_log\n",
    "\n",
    "    return ppo_train_data_log,ppo_actor_model_path,ppo_critic_model_path\n",
    "\n",
    "def ppo_training_vis(ppo_train_data_log):\n",
    "    df_data = []\n",
    "\n",
    "    for entry in ppo_train_data_log:\n",
    "        episode, step_count, scaled_action, raw_state, tensor_data, scaled_state, raw_reward, scaled_reward, cumulative_reward, fee_earned, impermanent_loss = entry\n",
    "        \n",
    "        # Extract raw_action values from tensor_data\n",
    "        raw_action_0 = float(tensor_data[0][0].numpy())\n",
    "        raw_action_1 = float(tensor_data[0][1].numpy())\n",
    "\n",
    "        scaled_action_0 = scaled_action['price_lower']\n",
    "        scaled_action_1 = scaled_action['price_upper']\n",
    "        \n",
    "        # Combine all data into a single dictionary\n",
    "        data = {\n",
    "            'episode': episode,\n",
    "            'step_count': step_count,\n",
    "            'scaled_action_0':scaled_action_0,\n",
    "            'scaled_action_1':scaled_action_1,\n",
    "            'raw_reward': raw_reward,\n",
    "            'scaled_reward': scaled_reward,\n",
    "            'fee_earned': fee_earned,\n",
    "            'impermanent_loss': impermanent_loss,\n",
    "            'cumulative_reward': cumulative_reward,\n",
    "            'raw_action_0': raw_action_0,\n",
    "            'raw_action_1': raw_action_1,\n",
    "        }\n",
    "        \n",
    "        # Add raw_action, global_state, and state data\n",
    "        #data.update(scaled_action)\n",
    "        data.update(raw_state)\n",
    "        data.update(scaled_state)\n",
    "        \n",
    "        df_data.append(data)\n",
    "\n",
    "    ppo_train_data_df = pd.DataFrame(df_data)\n",
    "    ppo_train_data_df.to_csv('model_outdir_csv/ppo_agent_train_data.csv', index=False)\n",
    "\n",
    "    train_rewards_plot(ppo_train_data_df)\n",
    "    train_raw_actions_plot(ppo_train_data_df)\n",
    "    train_scaled_actions_plot(ppo_train_data_df)\n",
    "    train_combined_metrics_plot(ppo_train_data_df)\n",
    "    train_separate_episode_action_plot(ppo_train_data_df)\n",
    "\n",
    "def eval_ppo_agent(eval_steps=100, eval_episodes=2, model_name='model_storage/ppo/lstm_actor_critic_batch_norm',percentage_range=0.6,agent_budget_usd=10000, use_running_statistics=False, action_transform=\"linear\"):\n",
    "\n",
    "    eval_env = DiscreteSimpleEnvEval(agent_budget_usd=agent_budget_usd,percentage_range=percentage_range, seed=42, penalty_param_magnitude=0, use_running_statistics=use_running_statistics, action_transform=action_transform)\n",
    "    n_actions = sum(action_space.shape[0] for action_space in eval_env.action_space.values())\n",
    "    input_dims = sum(np.prod(eval_env.observation_space.spaces[key].shape) for key in eval_env.observation_space.spaces.keys())\n",
    "    ppo_eval_agent = PPOEval(eval_env, n_actions, observation_dims=input_dims, buffer_size=5,training=False)\n",
    "\n",
    "    model_base_path = os.path.join(base_path,model_name)\n",
    "\n",
    "    ppo_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ppo_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "\n",
    "    ppo_eval_agent.actor.load_weights(ppo_actor_model_path)\n",
    "    ppo_eval_agent.critic.load_weights(ppo_critic_model_path)\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        state = eval_env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(eval_steps):\n",
    "            action,_ = ppo_eval_agent.choose_action(state)\n",
    "            next_state, reward, done, _ = eval_env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode {episode+1}/{eval_episodes}, Reward: {episode_reward}\")\n",
    "    ppo_eval_data_log=eval_env.eval_data_log\n",
    "    return ppo_eval_data_log\n",
    "\n",
    "\n",
    "def ppo_eval_vis(ppo_eval_data_log):\n",
    "    df_eval_data = []    \n",
    "    for entry in ppo_eval_data_log:\n",
    "        (episode, step_count, global_state, raw_action_rl_agent, action_rl_agent, \n",
    "        raw_action_baseline_agent, action_baseline_agent, state, \n",
    "        raw_reward_rl_agent, raw_reward_baseline_agent,scaled_reward_rl_agent,scaled_reward_baseline_agent, cumulative_reward_rl_agent, \n",
    "        cumulative_reward_baseline_agent, fee_income_rl_agent, impermanent_loss_rl_agent, \n",
    "        fee_income_baseline_agent, impermanent_loss_baseline_agent) = entry\n",
    "        \n",
    "        # Extract raw_action values for RL agent\n",
    "        raw_action_rl_agent_0 = float(raw_action_rl_agent[0][0].numpy())\n",
    "        raw_action_rl_agent_1 = float(raw_action_rl_agent[0][1].numpy())\n",
    "\n",
    "        scaled_action_rl_agent_0 = action_rl_agent['price_lower']\n",
    "        scaled_action_rl_agent_1 = action_rl_agent['price_upper']\n",
    "        \n",
    "        # Extract raw_action values for baseline agent\n",
    "        raw_action_baseline_agent_0 = raw_action_baseline_agent['price_lower']\n",
    "        raw_action_baseline_agent_1 = raw_action_baseline_agent['price_upper']\n",
    "        \n",
    "        # Combine all data into a single dictionary\n",
    "        data = {\n",
    "            'episode': episode,\n",
    "            'step_count': step_count,\n",
    "            'raw_reward_rl_agent': raw_reward_rl_agent,\n",
    "            'scaled_reward_rl_agent':scaled_reward_rl_agent,\n",
    "            'cumulative_reward_rl_agent': cumulative_reward_rl_agent,\n",
    "            'scaled_action_rl_agent_0':scaled_action_rl_agent_0,\n",
    "            'scaled_action_rl_agent_1':scaled_action_rl_agent_1,\n",
    "            'fee_income_rl_agent': fee_income_rl_agent,\n",
    "            'impermanent_loss_rl_agent': impermanent_loss_rl_agent,\n",
    "            'raw_reward_baseline_agent': raw_reward_baseline_agent,\n",
    "            'scaled_reward_baseline_agent':scaled_reward_baseline_agent,\n",
    "            'cumulative_reward_baseline_agent': cumulative_reward_baseline_agent,\n",
    "            'raw_action_baseline_agent_0': raw_action_baseline_agent_0,\n",
    "            'raw_action_baseline_agent_1': raw_action_baseline_agent_1,\n",
    "            'fee_income_baseline_agent': fee_income_baseline_agent,\n",
    "            'impermanent_loss_baseline_agent': impermanent_loss_baseline_agent,\n",
    "            'raw_action_rl_agent_0': raw_action_rl_agent_0,\n",
    "            'raw_action_rl_agent_1': raw_action_rl_agent_1,\n",
    "\n",
    "        }\n",
    "        # Add action, global_state, and state data\n",
    "        data.update(global_state)\n",
    "        data.update(state)\n",
    "        \n",
    "        df_eval_data.append(data)\n",
    "\n",
    "    ppo_eval_data_df = pd.DataFrame(df_eval_data)\n",
    "    ppo_eval_data_df.to_csv('model_outdir_csv/ppo_agent_eval_data.csv', index=False)\n",
    "    eval_rewards_plot(ppo_eval_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir ./model_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(pool_id=\"0x3416cf6c708da44db2624d63ea0aaef7113527c6\",ddpg_agent_path='model_storage/ddpg/ddpg_1',ppo_agent_path='model_storage/ppo/lstm_actor_critic_batch_norm'):\n",
    "    pool_data = fetch_inference_pool_data(pool_id)\n",
    "    print(f\"State Space: {pool_data}\")\n",
    "\n",
    "    global_state = pool_data\n",
    "    curr_price = global_state['token1Price']\n",
    "    liquidity = global_state['liquidity']\n",
    "    fee_growth_0 = global_state['feeGrowthGlobal0X128']\n",
    "    fee_growth_1 = global_state['feeGrowthGlobal1X128']\n",
    "\n",
    "    obs = {'scaled_curr_price': curr_price/5000, 'scaled_liquidity': liquidity/1e20, \n",
    "           'scaled_feeGrowthGlobal0x128': fee_growth_0/1e34, 'scaled_feeGrowthGlobal1x128': fee_growth_1/1e34}\n",
    "    print(f\"Obs Space: {obs}\")\n",
    "\n",
    "    ddpg_eval_agent,ppo_eval_agent,eval_env=load_inference_agent(ddpg_agent_path,ppo_agent_path)\n",
    "\n",
    "    eval_env.reset()\n",
    "\n",
    "    # DDPG Agent Action\n",
    "    ddpg_action = ddpg_eval_agent.choose_action(obs)\n",
    "    ddpg_action_dict,ddpg_action_ticks = postprocess_action(ddpg_action, curr_price,action_transform='linear')\n",
    "\n",
    "    # PPO Agent Action\n",
    "    ppo_action, _ = ppo_eval_agent.choose_action(obs)\n",
    "    ppo_action_dict,ppo_action_ticks = postprocess_action(ppo_action, curr_price,action_transform='exp')\n",
    "\n",
    "    return ddpg_action,ddpg_action_dict,ddpg_action_ticks,ppo_action, ppo_action_dict,ppo_action_ticks\n",
    "\n",
    "def postprocess_action(action, curr_price,action_transform='linear'):\n",
    "    a_0, a_1 = action[0, 0].numpy(), action[0, 1].numpy()\n",
    "\n",
    "    action_lower_bound = curr_price * 0.1\n",
    "    action_upper_bound = curr_price * 2\n",
    "\n",
    "    if action_transform=='linear':\n",
    "        a_0 = np.clip(a_0, 0, 1)\n",
    "        a_1 = np.clip(a_1, 0, 1)\n",
    "        price_lower = action_lower_bound + a_0 * (action_upper_bound - action_lower_bound)/2\n",
    "        price_upper = (action_upper_bound - action_lower_bound)/2 + a_1 * (action_upper_bound - action_lower_bound)/2\n",
    "\n",
    "    elif action_transform=='exp':\n",
    "        exp_a_0 = np.exp(a_0)\n",
    "        exp_a_1 = np.exp(a_1)\n",
    "        norm_exp_a_0 = exp_a_0 / (exp_a_0 + exp_a_1)\n",
    "        norm_exp_a_1 = exp_a_1 / (exp_a_0 + exp_a_1)\n",
    "        range_bound = action_upper_bound - action_lower_bound\n",
    "        price_lower = action_lower_bound + norm_exp_a_0 * range_bound\n",
    "        price_upper = action_lower_bound + norm_exp_a_1 * range_bound\n",
    "\n",
    "\n",
    "    if price_lower > price_upper:\n",
    "        price_lower, price_upper = price_upper, price_lower\n",
    "\n",
    "    action_dict = {\n",
    "        'price_lower': price_lower,\n",
    "        'price_upper': price_upper\n",
    "    }\n",
    "    ticks_dict={\n",
    "    'tick_lower':price_to_valid_tick(action_dict['price_lower']),\n",
    "    'tick_upper':price_to_valid_tick(action_dict['price_upper'])\n",
    "    }\n",
    "    return action_dict,ticks_dict\n",
    "\n",
    "def load_inference_agent(ddpg_agent_path='model_storage/ddpg/ddpg_1',ppo_agent_path='model_storage/ppo/lstm_actor_critic_batch_norm'):\n",
    "    eval_data_log=[]\n",
    "    eval_env = DiscreteSimpleEnvEval(agent_budget_usd=10000,percentage_range=0.30, seed=42)\n",
    "    n_actions = sum(action_space.shape[0] for action_space in eval_env.action_space.values())\n",
    "    input_dims = sum(np.prod(eval_env.observation_space.spaces[key].shape) for key in eval_env.observation_space.spaces.keys())\n",
    "\n",
    "    # Load ddpg eval agent for predictions\n",
    "    ddpg_eval_agent = DDGPEval(env=eval_env, n_actions=n_actions, input_dims=input_dims, training=False)\n",
    "    model_base_path = os.path.join(base_path,ddpg_agent_path)\n",
    "    ddpg_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ddpg_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "    ddpg_eval_agent.actor.load_weights(ddpg_actor_model_path)\n",
    "    ddpg_eval_agent.critic.load_weights(ddpg_critic_model_path)\n",
    "\n",
    "    # Load ppo eval agent for infernece\n",
    "    eval_data_log=[]\n",
    "    eval_env = DiscreteSimpleEnvEval(agent_budget_usd=10000,percentage_range=0.50, seed=42,penalty_param_magnitude=0,use_running_statistics=False)\n",
    "    ppo_eval_agent = PPOEval(eval_env, n_actions, observation_dims=input_dims, buffer_size=5,training=False)\n",
    "    model_base_path = os.path.join(base_path,ppo_agent_path)\n",
    "    ppo_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ppo_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "    ppo_eval_agent.actor.load_weights(ppo_actor_model_path)\n",
    "    ppo_eval_agent.critic.load_weights(ppo_critic_model_path)\n",
    "\n",
    "    return ddpg_eval_agent,ppo_eval_agent, eval_env\n",
    "\n",
    "def perform_inference(user_preferences,pool_state,pool_id=\"0x99ac8ca7087fa4a2a1fb6357269965a2014abc35\",ddpg_agent_path='model_storage/ddpg/ddpg_1',ppo_agent_path='model_storage/ppo/lstm_actor_critic_batch_norm'):\n",
    "    # Extracting necessary information from the pool state\n",
    "    current_profit = pool_state['current_profit']\n",
    "    price_out_of_range = pool_state['price_out_of_range']\n",
    "    time_since_last_adjustment = pool_state['time_since_last_adjustment']\n",
    "    pool_volatility = pool_state['pool_volatility']\n",
    "\n",
    "    # User Preferences\n",
    "    risk_tolerance = user_preferences['risk_tolerance']\n",
    "    investment_horizon = user_preferences['investment_horizon']\n",
    "    liquidity_preference = user_preferences['liquidity_preference']\n",
    "    volatility_threshold=user_preferences['risk_aversion_threshold']\n",
    "\n",
    "\n",
    "    # Adjust thresholds based on user preferences\n",
    "    profit_taking_threshold = risk_tolerance['profit_taking']\n",
    "    stop_loss_threshold = risk_tolerance['stop_loss']\n",
    "    rebalance_interval = investment_horizon * 24 * 60 * 60  # Convert days to seconds\n",
    "\n",
    "    # Predicted actions from RL agents\n",
    "    ddpg_action,ddpg_action_dict,ddpg_action_ticks, ppo_action,ppo_action_dict,ppo_action_ticks=predict_action(pool_id=pool_id,ddpg_agent_path=ddpg_agent_path,ppo_agent_path=ppo_agent_path)\n",
    "\n",
    "    # Decision Logic\n",
    "    if user_preferences['user_status']=='new_user':\n",
    "        return 'Add new liquidity position',ddpg_action_dict,ppo_action_dict\n",
    "    if current_profit >= profit_taking_threshold:\n",
    "        return 'adjust_position', ddpg_action_dict,ppo_action_dict\n",
    "    elif price_out_of_range and liquidity_preference['adjust_on_price_out_of_range']:\n",
    "        return 'adjust_position', ddpg_action_dict,ppo_action_dict  \n",
    "    elif pool_volatility > volatility_threshold:\n",
    "        return 'exit_position', None,None  # Exit position in case of high volatility\n",
    "    elif time_since_last_adjustment >= rebalance_interval:\n",
    "        return 'rebalance_position', None,None  \n",
    "    elif current_profit <= stop_loss_threshold:\n",
    "        return 'exit_position', None,None  # Exit the position to stop further losses\n",
    "    else:\n",
    "        return 'maintain_position', None,None  # Maintain the current position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"DDPG_Training\") as run:\n",
    "    ddpg_train_data_log,ddpg_actor_model_path,ddpg_critic_model_path=train_ddpg_agent(max_steps=2, n_episodes=2, model_name='model_storage/ddpg/ddpg_fazool',alpha=0.001, beta=0.001, tau=0.8,batch_size=50, training=True,agent_budget_usd=10000,use_running_statistics=False)\n",
    "    ddpg_training_vis(ddpg_train_data_log)\n",
    "\n",
    "    # Log models after loading them\n",
    "    actor_model = tf.keras.models.load_model(ddpg_actor_model_path)\n",
    "    critic_model = tf.keras.models.load_model(ddpg_critic_model_path)\n",
    "\n",
    "    #mlflow.tensorflow.log_model(actor_model, \"ddpg_actor_model\")\n",
    "    #mlflow.tensorflow.log_model(critic_model, \"ddpg_critic_model\")\n",
    "    \n",
    "with mlflow.start_run(run_name=\"DDPG_Evaluation\"):\n",
    "    ddpg_eval_data_log=eval_ddpg_agent(eval_steps=2, eval_episodes=2, model_name='model_storage/ddpg/ddpg_fazool', percentage_range=0.6, agent_budget_usd=10000, use_running_statistics=False)\n",
    "    ddpg_eval_vis(ddpg_eval_data_log)\n",
    "\n",
    "#run_id = run.info.run_id\n",
    "#print(f\"run_id: {run_id}\")\n",
    "#!mlflow models serve -m \"runs:/97ab3853db6642d6b3abd7f9284aa6cc/ddpg_actor_model\" -p 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"PPO_Training\") as run:\n",
    "    ppo_train_data_log,ppo_actor_model_path,ppo_critic_model_path=train_ppo_agent(max_steps=2, n_episodes=2, model_name='model_storage/ppo/ppo2_fazool', buffer_size=5,n_epochs=20, gamma=0.5, alpha=0.001, gae_lambda=0.75, policy_clip=0.6, max_grad_norm=0.6,agent_budget_usd=10000,use_running_statistics=False,action_transform='linear')\n",
    "    ppo_training_vis(ppo_train_data_log)\n",
    "\n",
    "    ppo_actor_model = tf.keras.models.load_model(ppo_actor_model_path)\n",
    "    ppo_critic_model = tf.keras.models.load_model(ppo_critic_model_path)\n",
    "\n",
    "    #mlflow.tensorflow.log_model(ppo_actor_model, \"ppo_actor_model\")\n",
    "    #mlflow.tensorflow.log_model(ppo_critic_model, \"ppo_critic_model\")\n",
    "    \n",
    "\n",
    "with mlflow.start_run(run_name=\"PPO_Evaluation\"):\n",
    "    ppo_eval_data_log=eval_ppo_agent(eval_steps=2, eval_episodes=2, model_name='model_storage/ppo/ppo2_fazool', percentage_range=0.5, agent_budget_usd=10000, use_running_statistics=False, action_transform='linear')\n",
    "    ppo_eval_vis(ppo_eval_data_log)\n",
    "\n",
    "#run_id = run.info.run_id\n",
    "#print(\"run_id: {run_id}\")\n",
    "#!mlflow models serve -m \"runs:/run_id/ppo_actor_model\" -p 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space: {'feeGrowthGlobal0X128': 4.121811279136787e+31, 'feeGrowthGlobal1X128': 6.395044142047031e+42, 'liquidity': 5.5701893525929363e+17, 'token1Price': 20.8881718355201}\n",
      "Obs Space: {'scaled_curr_price': 0.00417763436710402, 'scaled_liquidity': 0.005570189352592936, 'scaled_feeGrowthGlobal0x128': 0.004121811279136787, 'scaled_feeGrowthGlobal1x128': 639504414.2047031}\n",
      "Pool selcted for this episode: ETH_DAI_3000\n",
      "mnemonic: '\u001b[0;1;36magent vast jazz salt remain replace menu flash document runway approve oxygen\u001b[0;m'\n",
      "funded account with token0: {'Transfer': [OrderedDict([('from', '0x3A91B4B861bCE45f28AA8D733d7f34DFb09e0533'), ('to', '0x9c3725273552e1F42026a9eA5b9b402206e441Bd'), ('value', 1000000000000000019884624838656)])]}\n",
      "funded account with token1: {'Transfer': [OrderedDict([('from', '0x3A91B4B861bCE45f28AA8D733d7f34DFb09e0533'), ('to', '0x9c3725273552e1F42026a9eA5b9b402206e441Bd'), ('value', 110000000000000000498458871988224)])]}\n",
      "mnemonic: '\u001b[0;1;36mwaste moment case blue attract glad squirrel option mobile account stove voyage\u001b[0;m'\n",
      "funded account with token0: {'Transfer': [OrderedDict([('from', '0x3A91B4B861bCE45f28AA8D733d7f34DFb09e0533'), ('to', '0xF24429cee3F3212293F30B6eb143818a44F75d34'), ('value', 4999999999999999727876154935214080)])]}\n",
      "funded account with token1: {'Transfer': [OrderedDict([('from', '0x3A91B4B861bCE45f28AA8D733d7f34DFb09e0533'), ('to', '0xF24429cee3F3212293F30B6eb143818a44F75d34'), ('value', 5500000000000000565354898883870720)])]}\n",
      "mu: [[1.6891198e-16 1.0000000e+00]], sigma: [[1. 1.]], action_prob: tfp.distributions.Normal(\"Normal\", batch_shape=[1, 2], event_shape=[], dtype=float32), action: [[-0.01117085  1.1570716 ]], log_prob: [[-0.9190009  -0.93127424]]\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startegy Action: Add new liquidity position, DDPG Agent Action: {'price_lower': 17.926010975482967, 'price_upper': 39.23951881907353}, PPO Agent Action: {'price_lower': 11.501620633562087, 'price_upper': 32.363542586591045}\n"
     ]
    }
   ],
   "source": [
    "# Example pool state\n",
    "pool_state = {\n",
    "    'current_profit': 500,\n",
    "    'price_out_of_range': False,\n",
    "    'time_since_last_adjustment': 40000,\n",
    "    'pool_volatility': 0.2\n",
    "}\n",
    "\n",
    "# Example user preferences\n",
    "user_preferences = {\n",
    "    'risk_tolerance': {'profit_taking': 50, 'stop_loss': -500},\n",
    "    'investment_horizon': 7,  # days\n",
    "    'liquidity_preference': {'adjust_on_price_out_of_range': True},\n",
    "    'risk_aversion_threshold':0.1,\n",
    "    'user_status':'new_user'\n",
    "}\n",
    "#pool=\"0x3416cf6c708da44db2624d63ea0aaef7113527c6\" #USDC/USDT\n",
    "#pool=\"0x6c6bc977e13df9b0de53b251522280bb72383700\" #DAI/USDC\n",
    "#pool=\"0x4e68ccd3e89f51c3074ca5072bbac773960dfa36\" #ETH/USDT\n",
    "#pool=\"0x99ac8ca7087fa4a2a1fb6357269965a2014abc35\" #WBTC/USDC\n",
    "pool=\"0xcbcdf9626bc03e24f779434178a73a0b4bad62ed\" # WBTC/ETH\n",
    "strategy_action, ddpg_action,ppo_action = perform_inference(user_preferences,pool_state,pool_id=pool,ddpg_agent_path='model_storage/ddpg/ddpg_1',ppo_agent_path='model_storage/ppo/lstm_actor_critic_batch_norm')\n",
    "print(f\"Startegy Action: {strategy_action}, DDPG Agent Action: {ddpg_action}, PPO Agent Action: {ppo_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liquidity Strategy Framework\n",
    "1. User Profile Assessment\n",
    "Risk Tolerance Assessment: Gauge the user's appetite for risk (low, medium, high) and set profit-taking and stop-loss thresholds accordingly.\n",
    "Investment Horizon: Determine the duration the user plans to engage in liquidity provisioning (short-term, mid-term, long-term).\n",
    "Liquidity Preference: Assess the user's preference for types of pools (e.g., stablecoin pairs vs. high-volatility pairs).\n",
    "2. Market and Pool Analysis\n",
    "Volatility Analysis: Use historical data to analyze the volatility of the pool. Higher volatility may require a more dynamic strategy.\n",
    "Fee vs. Impermanent Loss Analysis: Assess the historical balance between fee income and impermanent loss in the pool.\n",
    "Token Pair Correlation: Study the correlation between the assets in the pool and how it affects price movements and liquidity depth.\n",
    "3. Integration of RL Agent Predictions\n",
    "Predictive Modeling: Regularly run the DDPG and PPO agents to predict optimal liquidity ranges based on the current pool state.\n",
    "Adjustment Frequency: Decide the frequency of querying the RL agents for adjustments based on market conditions and user preferences.\n",
    "4. Strategy Implementation\n",
    "Initiating Position: Enter the liquidity pool based on initial RL agent predictions and user preferences.\n",
    "Continuous Monitoring: Set up a system to continuously monitor the pool's state and the performance of the liquidity position.\n",
    "Adjustment Protocols:\n",
    "Profit Taking: If the profit reaches the user's threshold, adjust or rebalance as per the RL agent's suggestions.\n",
    "Price Range Exit: If the pool price moves out of the predicted range significantly, adjust the position.\n",
    "Stop Loss: Exit the position if losses reach the user-defined stop-loss threshold.\n",
    "Handling Volatility: In high volatility, reduce exposure or exit based on impermanent loss risk and market trends.\n",
    "Regular Review and Rebalancing: Periodically review the position independent of the RL predictions and rebalance if necessary based on market shifts.\n",
    "\n",
    "\n",
    "1. Initial Setup and User Input\n",
    "User Interface: Develop a user interface where the user can select a pool, allocate a budget (e.g., 10000 USD), choose a time horizon, and specify risk preferences (risk aversion and tolerance).\n",
    "Pool Selection: Allow the user to select a Uniswap V3 pool for liquidity provisioning.\n",
    "User Preferences: Capture the user's risk tolerance, investment horizon, and other relevant preferences.\n",
    "2. Initial Liquidity Provisioning\n",
    "Predict Action: When a new user decides to add liquidity, the strategy uses both DDPG and PPO agents to predict the optimal liquidity range (price_lower and price_upper) based on the current state of the selected pool.\n",
    "Action Selection: The strategy selects one of the predicted actions (from DDPG or PPO) to initiate the liquidity provisioning in the pool.\n",
    "Execute Liquidity Provision: Add liquidity to the selected pool within the predicted range using the user's allocated budget.\n",
    "3. Continuous Strategy Management\n",
    "Data Fetching: Regularly (e.g., hourly) fetch data from the pool to monitor the state of the liquidity position and the pool's market conditions.\n",
    "State Assessment: Assess the current state of the liquidity position in terms of profit/loss, whether the price is within the range, and other relevant metrics.\n",
    "Decision Criteria: Define criteria for adjusting, maintaining, or exiting the position based on user preferences and real-time market data.\n",
    "4. Ongoing Position Management\n",
    "Rebalance Position: If the strategy determines that rebalancing is needed (e.g., price out of the current range or based on time interval), it again uses DDPG and PPO agents to predict a new optimal range and adjusts the position accordingly.\n",
    "Maintain Position: If the current position is deemed optimal, the strategy will continue to hold the position without changes.\n",
    "Exit Position: If the strategy identifies adverse market conditions or meets the user's risk parameters (like stop loss), it exits the position to protect the user's capital.\n",
    "5. Strategy Execution Loop\n",
    "Loop Implementation: Implement a loop that continuously monitors the pool state, re-evaluates the position, and executes the strategy's decisions (adjust, maintain, or exit) based on real-time data and user preferences.\n",
    "6. User Feedback and Adjustment\n",
    "Feedback Mechanism: Allow users to review their position performance and adjust their preferences if needed.\n",
    "Adaptive Strategy: Ensure the strategy adapts to changes in user preferences and ongoing market developments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Served Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn_state.gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn_state.gamma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn_state.beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn_state.beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn_state.moving_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn_state.moving_mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn_state.moving_variance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn_state.moving_variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc1.kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc1.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc1.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc1.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn1.gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn1.gamma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn1.beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn1.beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn1.moving_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn1.moving_mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn1.moving_variance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn1.moving_variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc2.kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc2.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc2.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc2.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn2.gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn2.gamma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn2.beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn2.beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn2.moving_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn2.moving_mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn2.moving_variance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn2.moving_variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc3.kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc3.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc3.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).fc3.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn3.gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn3.gamma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn3.beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn3.beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn3.moving_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn3.moving_mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn3.moving_variance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).bn3.moving_variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).q.kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).q.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).q.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).q.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm1.cell.kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm1.cell.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm1.cell.recurrent_kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm1.cell.recurrent_kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm1.cell.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm1.cell.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm2.cell.kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm2.cell.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm2.cell.recurrent_kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm2.cell.recurrent_kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm2.cell.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).lstm2.cell.bias\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=123): Max retries exceeded with url: /invocations (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f11c3f6c670>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m \n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f11c3f6c670>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=123): Max retries exceeded with url: /invocations (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f11c3f6c670>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:123/invocations\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Send POST request\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Print the response\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse Code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mstatus_code)\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/tokenspice/tsp_venv/lib/python3.9/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=123): Max retries exceeded with url: /invocations (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f11c3f6c670>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "pool_id=\"0x4e68ccd3e89f51c3074ca5072bbac773960dfa36\" #ETH/USDT\n",
    "\n",
    "pool_data = fetch_inference_pool_data(pool_id)\n",
    "print(f\"State Space: {pool_data}\")\n",
    "\n",
    "global_state = pool_data\n",
    "curr_price = global_state['token1Price']\n",
    "liquidity = global_state['liquidity']\n",
    "fee_growth_0 = global_state['feeGrowthGlobal0X128']\n",
    "fee_growth_1 = global_state['feeGrowthGlobal1X128']\n",
    "\n",
    "data = {'scaled_curr_price': curr_price/5000, 'scaled_liquidity': liquidity/1e20, \n",
    "        'scaled_feeGrowthGlobal0x128': fee_growth_0/1e34, 'scaled_feeGrowthGlobal1x128': fee_growth_1/1e34}\n",
    "\n",
    "# URL for the predict endpoint\n",
    "url = 'http://127.0.0.1:123/invocations'\n",
    "\n",
    "# Send POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response\n",
    "print(\"Response Code:\", response.status_code)\n",
    "print(\"Predicted Response:\", response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
