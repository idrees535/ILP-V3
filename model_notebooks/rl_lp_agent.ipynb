{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=$PATH:.\n",
    "base_path=\"/home/azureuser/Intelligent-Liquidity-Provisioning-Framework\"\n",
    "#base_path=\"/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/Intelligent-Liquidity-Provisioning-Framework-V1\"\n",
    "import os\n",
    "os.chdir(base_path)\n",
    "os.environ[\"PATH\"] += \":.\"\n",
    "reset_env=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env():\n",
    "    import shutil\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    # Define the paths\n",
    "    folder_path = os.path.join(base_path, \"v3_core/build/deployments\")\n",
    "    json_file1_path = os.path.join(base_path, \"model_storage/token_pool_addresses.json\")\n",
    "    json_file2_path = os.path.join(base_path, \"model_storage/liq_positions.json\")\n",
    "\n",
    "    # 1. Delete the folder and its contents\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "\n",
    "    # 2. Delete contents of the first JSON file\n",
    "    with open(json_file1_path, 'w') as file:\n",
    "        file.write(\"{}\")\n",
    "\n",
    "    # 3. Delete contents of the second JSON file and add {}\n",
    "    with open(json_file2_path, 'w') as file:\n",
    "        file.write(\"{}\")\n",
    "        \n",
    "if reset_env==True:\n",
    "    reset_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/Intelligent-Liquidity-Provisioning-Framework/ilp_venv/lib/python3.9/site-packages/brownie/network/main.py:44: BrownieEnvironmentWarning: Development network has a block height of 1785\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached to local RPC client listening at '127.0.0.1:8545'...\n",
      "Existing pool:WETH_USDC_3000 having pool address: 0xF7b41bB2138725cdc873083ed94AD8026CB812A1 loaded\n",
      "Existing pool:ETH_DAI_3000 having pool address: 0x3567c215Bc2BedA21f7F14707676039578Fc5d57 loaded\n",
      "Existing pool:BTC_USDT_3000 having pool address: 0x3b2d41e6F1EEa08Bb03bafbc185162529dccF08c loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 11:08:12.815079: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-14 11:08:12.818414: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-14 11:08:12.858949: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-14 11:08:12.858986: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-14 11:08:12.859011: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-14 11:08:12.866793: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-14 11:08:12.867511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 11:08:13.743823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from netlists.uniswapV3.netlist import SimStrategy,SimState,netlist_createLogData\n",
    "from engine.SimEngine import SimEngine\n",
    "from util.globaltokens import weth_usdc_pool,eth_dai_pool,btc_usdt_pool\n",
    "import brownie\n",
    "from util.constants import GOD_ACCOUNT,RL_AGENT_ACCOUNT\n",
    "from util.base18 import toBase18, fromBase18,fromBase128,price_to_valid_tick\n",
    "from model_scripts.plot import train_rewards_plot,eval_rewards_plot,train_raw_actions_plot,train_scaled_actions_plot,train_combined_metrics_plot,train_separate_episode_action_plot\n",
    "from model_scripts.sync_pool_subgraph_data import fetch_inference_pool_data\n",
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSimpleEnv(gym.Env):\n",
    "    def __init__(self,agent_budget_usd,alpha = 0.5, exploration_std_dev = 0.01, beta=0.1,penalty_param_magnitude=-1,use_running_statistics=False):\n",
    "        super(DiscreteSimpleEnv, self).__init__()\n",
    "\n",
    "        self.pool=None\n",
    "        self.global_state=None\n",
    "        self.curr_price=None\n",
    "        self.action_lower_bound=None\n",
    "        self.action_upper_bound=None\n",
    "        self.state=None\n",
    "        self.engine=None\n",
    "        \n",
    "        self.action_space = gym.spaces.Dict({\n",
    "            'price_relative_lower': gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32),\n",
    "            'price_relative_upper': gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32),\n",
    "        })\n",
    "        \n",
    "        self.reward=0\n",
    "        self.cumulative_reward = 0\n",
    "        self.done=False\n",
    "        self.episode=0\n",
    "        self.step_count=0\n",
    "\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'scaled_curr_price': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'scaled_liquidity': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'scaled_feeGrowthGlobal0x128': gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
    "            'scaled_feeGrowthGlobal1x128': gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),   \n",
    "        })\n",
    "        self.agent_budget_usd = agent_budget_usd\n",
    "        self.initial_budget_usd = agent_budget_usd\n",
    "\n",
    "        # Initialize rewrad normalization running statistics\n",
    "        self.reward_mean = 0\n",
    "        self.reward_std = 1\n",
    "        self.reward_count = 0\n",
    "\n",
    "        self.exploration_std_dev = exploration_std_dev\n",
    "        self.penalty=0\n",
    "        self.penalty_param=0\n",
    "        self.penalty_param_magnitude=penalty_param_magnitude\n",
    "\n",
    "        # Initialize running statistics for state normalization\n",
    "        self.use_running_statistics=use_running_statistics\n",
    "        self.curr_price_mean = 0\n",
    "        self.curr_price_std = 1\n",
    "        self.liquidity_mean = 0\n",
    "        self.liquidity_std = 1\n",
    "        self.fee_growth_diff_0 = 0\n",
    "        self.fee_growth_diff_1 = 0\n",
    "        self.fee_growth_0_mean = 0\n",
    "        self.fee_growth_1_mean = 0\n",
    "        self.fee_growth_0_std = 1\n",
    "        self.fee_growth_1_std = 1\n",
    "        self.previous_fee_growth_0 = 0\n",
    "        self.previous_fee_growth_1 = 0\n",
    "\n",
    "        #Obs space scaling param\n",
    "        self.alpha = alpha\n",
    "        #Rewrad scaling param\n",
    "        self.beta=beta\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pool=random.choice([weth_usdc_pool,eth_dai_pool,btc_usdt_pool])\n",
    "        \n",
    "        print(f'Pool selcted for this episode: {self.pool.pool_id}')\n",
    "        sim_strategy = SimStrategy()\n",
    "        sim_state = SimState(ss=sim_strategy,pool=self.pool)\n",
    "\n",
    "        output_dir = \"model_outdir_csv\"\n",
    "        netlist_log_func = netlist_createLogData\n",
    "\n",
    "        #from engine.SimEngine import SimEngine\n",
    "        self.engine = SimEngine(sim_state, output_dir, netlist_log_func)\n",
    "\n",
    "        self.global_state=self.pool.get_global_state()\n",
    "        self.curr_price=self.global_state['curr_price']\n",
    "        self.action_lower_bound=self.curr_price*0.1\n",
    "        self.action_upper_bound=self.curr_price*2\n",
    "        self.state = self.get_obs_space()\n",
    "        \n",
    "        self.done=False\n",
    "        self.reward=0\n",
    "        self.cumulative_reward = 0\n",
    "        self.episode+=1\n",
    "        self.step_count=0\n",
    "    \n",
    "        # Used for evaluation only\n",
    "        self.cumulative_reward_rl_agent = 0\n",
    "        self.cumulative_reward_baseline_agent = 0\n",
    "\n",
    "        self.agent_budget_usd = self.initial_budget_usd\n",
    "         \n",
    "        # reset running statistics for reward normalization\n",
    "        '''\n",
    "        self.reward_mean = 0\n",
    "        self.reward_std = 1\n",
    "        self.reward_count = 0\n",
    "        '''\n",
    "        # reset running statistics for state normalization\n",
    "        '''\n",
    "        self.curr_price_mean = 0\n",
    "        self.curr_price_std = 1\n",
    "        self.liquidity_mean = 0\n",
    "        self.liquidity_std = 1\n",
    "        self.fee_growth_diff_0 = 0\n",
    "        self.fee_growth_diff_1 = 0\n",
    "        self.fee_growth_0_mean = 0\n",
    "        self.fee_growth_1_mean = 0\n",
    "        self.fee_growth_0_std = 1\n",
    "        self.fee_growth_1_std = 1\n",
    "        self.previous_fee_growth_0 = 0\n",
    "        self.previous_fee_growth_1 = 0\n",
    "        '''  \n",
    "        return self.state\n",
    "\n",
    "    def step(self, raw_action):\n",
    "              \n",
    "        # Execute agent's action using pool's interface of add/remove liquidity\n",
    "        mint_tx_receipt,action=self._take_action(raw_action)\n",
    "        \n",
    "        # run uniswap abm env of n_steps\n",
    "        print()\n",
    "        print('Environment Step')\n",
    "        self.engine.reset()\n",
    "        self.engine.run()\n",
    "        print()\n",
    "        \n",
    "        self.state=self.get_obs_space()\n",
    "\n",
    "        scaled_reward,raw_reward,fee_income,impermanent_loss = self._calculate_reward(action,mint_tx_receipt)\n",
    "        self.reward=scaled_reward\n",
    "        self.cumulative_reward += self.reward\n",
    "\n",
    "        self.step_count+=1\n",
    "        \n",
    "        print(f\"episode: {self.episode}, step_count: {self.step_count}, scaled_reward: {self.reward}, raw_reward: {raw_reward} cumulative_reward: {self.cumulative_reward}\")\n",
    "        print(f\"raw_pool_state: {self.pool.get_global_state()}\")\n",
    "        print(f\"sclaed_pool_state: {self.state}\")\n",
    "        print()\n",
    "\n",
    "        train_data_log.append((self.episode, self.step_count, action, self.pool.get_global_state(), raw_action, self.state, raw_reward, self.reward, self.cumulative_reward, fee_income, impermanent_loss))\n",
    "\n",
    "        self.done = self._is_done()\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "    def get_obs_space(self):\n",
    "        self.global_state = self.pool.get_global_state()\n",
    "\n",
    "        # Scaling for curr_price and liquidity\n",
    "        curr_price = float(self.global_state['curr_price'])\n",
    "        liquidity = float(self.global_state['liquidity_raw'])\n",
    "        fee_growth_0 = float(self.global_state['feeGrowthGlobal0X128'])\n",
    "        fee_growth_1 = float(self.global_state['feeGrowthGlobal1X128'])\n",
    "\n",
    "        self.curr_price_mean = self.alpha * curr_price + (1 - self.alpha) * self.curr_price_mean\n",
    "        self.curr_price_std = np.sqrt(self.alpha * (curr_price - self.curr_price_mean)**2 + (1 - self.alpha) * self.curr_price_std**2)\n",
    "\n",
    "        self.liquidity_mean = self.alpha * liquidity + (1 - self.alpha) * self.liquidity_mean\n",
    "        self.liquidity_std = np.sqrt(self.alpha * (liquidity - self.liquidity_mean)**2 + (1 - self.alpha) * self.liquidity_std**2)\n",
    "\n",
    "        # Scaling for fee growth differences \n",
    "        self.fee_growth_diff_0 = fee_growth_0 - self.previous_fee_growth_0\n",
    "        self.fee_growth_diff_1 = fee_growth_1 - self.previous_fee_growth_1\n",
    "\n",
    "        self.fee_growth_0_mean = self.alpha * self.fee_growth_diff_0 + (1 - self.alpha) * self.fee_growth_0_mean\n",
    "        self.fee_growth_0_std = np.sqrt(self.alpha * (self.fee_growth_diff_0 - self.fee_growth_0_mean)**2 + (1 - self.alpha) * self.fee_growth_0_std**2)\n",
    "\n",
    "        self.fee_growth_1_mean = self.alpha * self.fee_growth_diff_1 + (1 - self.alpha) * self.fee_growth_1_mean\n",
    "        self.fee_growth_1_std = np.sqrt(self.alpha * (self.fee_growth_diff_1 - self.fee_growth_1_mean)**2 + (1 - self.alpha) * self.fee_growth_1_std**2)\n",
    "\n",
    "        if self.use_running_statistics==True:\n",
    "            #Use running stats\n",
    "            obs = {'scaled_curr_price': (curr_price - self.curr_price_mean) / (self.curr_price_std + 1e-10),'scaled_liquidity': (liquidity - self.liquidity_mean) / (self.liquidity_std + 1e-10),}\n",
    "            obs['scaled_feeGrowthGlobal0x128'] = (self.fee_growth_diff_0 - self.fee_growth_0_mean) / (self.fee_growth_0_std + 1e-10)\n",
    "            obs['scaled_feeGrowthGlobal1x128'] = (self.fee_growth_diff_1 - self.fee_growth_1_mean) / (self.fee_growth_1_std + 1e-10)\n",
    "\n",
    "        else:\n",
    "            # Scale obs space without using global stats\n",
    "            obs = {'scaled_curr_price': curr_price/5000,'scaled_liquidity': liquidity/1e24,'scaled_feeGrowthGlobal0x128': fee_growth_0/1e34,'scaled_feeGrowthGlobal1x128': fee_growth_1/1e34}\n",
    "\n",
    "        self.previous_fee_growth_0 = fee_growth_0\n",
    "        self.previous_fee_growth_1 = fee_growth_1\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        self.penalty=0\n",
    "\n",
    "        raw_a, raw_b = action[0, 0].numpy(), action[0, 1].numpy()\n",
    "\n",
    "        # Add exploration noise\n",
    "        \n",
    "        a_0 = raw_a + np.random.normal(0, self.exploration_std_dev)\n",
    "        a_1 = raw_b + np.random.normal(0, self.exploration_std_dev)\n",
    "\n",
    "        a_0 = np.clip(a_0, 0, 1)\n",
    "        a_1 = np.clip(a_1, 0, 1)\n",
    " \n",
    "        # Only enabling agent to provide liquiidty around bounds midpoint\n",
    "        price_lower = self.action_lower_bound + a_0 * (self.action_upper_bound - self.action_lower_bound)/2\n",
    "        price_upper = (self.action_upper_bound - self.action_lower_bound)/2 + a_1 * (self.action_upper_bound - self.action_lower_bound)/2\n",
    "        \n",
    "        # Enabling agent to place range orders too (Only feasible when using multiple positions)\n",
    "        #price_lower = self.action_lower_bound + a_0 * (self.action_upper_bound - self.action_lower_bound)\n",
    "        #price_upper = self.action_lower_bound + a_1 * (self.action_upper_bound - self.action_lower_bound)\n",
    "        \n",
    "        # Ensure price_lower is less than price_upper - Add penalty\n",
    "        \n",
    "        if price_lower>price_upper:\n",
    "            price_lower = min(price_lower, price_upper)\n",
    "            price_upper = max(price_lower, price_upper)\n",
    "            self.penalty=self.penalty_param_magnitude\n",
    "\n",
    "        # ensure actions are not too close - Add penalty\n",
    "        min_diff_percentage = 0.05  # 5% difference\n",
    "        price_diff = price_upper - price_lower\n",
    "        \n",
    "        if price_diff < min_diff_percentage * price_lower:\n",
    "            self.penalty+=self.penalty_param_magnitude\n",
    "            price_upper = price_lower + min_diff_percentage * price_lower\n",
    "        \n",
    "        action_dict = {\n",
    "            'price_lower': price_lower,\n",
    "            'price_upper': price_upper\n",
    "        }\n",
    "        \n",
    "        print('RL Agent Action')\n",
    "        print(f\"raw_action: {action}, scaled_action: {action_dict}\")\n",
    "\n",
    "        tick_lower=price_to_valid_tick(action_dict['price_lower'])\n",
    "        tick_upper=price_to_valid_tick(action_dict['price_upper'])\n",
    "        amount=self.agent_budget_usd\n",
    "        mint_tx_receipt=self.pool.add_liquidity(GOD_ACCOUNT, tick_lower, tick_upper, amount, b'')\n",
    "\n",
    "        return mint_tx_receipt,action_dict\n",
    "        \n",
    "    def _calculate_reward(self,action,mint_tx_receipt):\n",
    "       \n",
    "        tick_lower=price_to_valid_tick(action['price_lower'],60)\n",
    "        tick_upper=price_to_valid_tick(action['price_upper'],60)\n",
    "        liquidity=mint_tx_receipt.events['Mint']['amount']\n",
    "\n",
    "        # Collecting fee earned by position\n",
    "        print('Collect fee')\n",
    "        collect_tx_receipt,fee_income = self.pool.collect_fee(GOD_ACCOUNT, tick_lower, tick_upper,poke=True)\n",
    "    \n",
    "        print(\"Burn Position and Collect Tokens\")\n",
    "        # Remove position and collect tokens\n",
    "        burn_tx_receipt=self.pool.remove_liquidity_with_liquidty(GOD_ACCOUNT, tick_lower, tick_upper, liquidity)\n",
    "        collect_tx_receipt,curr_budget_usd = self.pool.collect_fee(GOD_ACCOUNT, tick_lower, tick_upper,poke=False)\n",
    "\n",
    "        # Can use online scaling approach as used for reward for this\n",
    "        rel_portofolio_value = 1 - curr_budget_usd/self.agent_budget_usd\n",
    "        \n",
    "        # Instead of using full budget for next step use previous step's reomved liquidity amount as budget in next step\n",
    "        #self.agent_budget_usd = curr_budget_usd\n",
    "        \n",
    "        # Calculate IL\n",
    "        amount0_initial = mint_tx_receipt.events['Mint']['amount0']\n",
    "        amount1_initial = mint_tx_receipt.events['Mint']['amount1']\n",
    "        \n",
    "        amount0_final = burn_tx_receipt.events['Burn']['amount0']\n",
    "        amount1_final = burn_tx_receipt.events['Burn']['amount1']\n",
    "        self.global_state = self.pool.get_global_state()\n",
    "        pool_price = float(self.global_state['curr_price'])\n",
    "\n",
    "        value_initial = (amount0_initial * pool_price + amount1_initial) / 1e18\n",
    "        value_final = (amount0_final * pool_price + amount1_final) / 1e18\n",
    "\n",
    "        impermanent_loss = value_initial - value_final\n",
    "\n",
    "        if fee_income==0:\n",
    "            self.penalty_param+= 0.05\n",
    "            self.penalty += self.penalty_param_magnitude*(1+self.penalty_param)\n",
    "\n",
    "        print(f'fee_earned:{fee_income}, impermannet_loss: {impermanent_loss}, penalty: {self.penalty}, initial_agent_portofolio_value: {value_initial}, final_agent_portofolio_value: {value_final}, reward_mean: {self.reward_mean}, rewrad_std_dev: {self.reward_std}, reward_count: {self.reward_count}')\n",
    "        print()\n",
    "        \n",
    "        raw_reward = fee_income - impermanent_loss + self.penalty\n",
    "        \n",
    "        if self.penalty==0:\n",
    "            self.reward_count += 1\n",
    "            #new_mean = self.reward_mean + (raw_reward - self.reward_mean) / self.reward_count\n",
    "            #new_std = ((self.reward_std ** 2 + (raw_reward - self.reward_mean) * (raw_reward - new_mean)) / self.reward_count) ** 0.5\n",
    "            new_mean = self.beta * raw_reward + (1 - self.beta) * self.reward_mean\n",
    "            new_std = np.sqrt(self.beta * ((raw_reward - new_mean) ** 2) + (1 - self.beta) * (self.reward_std ** 2))\n",
    "            self.reward_mean = new_mean\n",
    "            self.reward_std = new_std\n",
    "\n",
    "        #scaled_reward = (raw_reward - self.reward_mean) / (self.reward_std + 1e-10)\n",
    "        scaled_reward = raw_reward*10\n",
    "\n",
    "        #Reset penlaty for next step\n",
    "        self.penalty=0\n",
    "\n",
    "        return scaled_reward,raw_reward,fee_income, impermanent_loss\n",
    "\n",
    "    def _is_done(self):\n",
    "        \n",
    "        max_reward_threshold = 100000\n",
    "        min_reward_threshold= -100000\n",
    "        max_budget_threshold = 1.5*self.initial_budget_usd\n",
    "        min_budget_threshold = 0.5*self.initial_budget_usd\n",
    "\n",
    "        if self.cumulative_reward >= max_reward_threshold or self.cumulative_reward<=min_reward_threshold or self.agent_budget_usd>max_budget_threshold or self.agent_budget_usd<min_budget_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "env=DiscreteSimpleEnv(agent_budget_usd=10000,use_running_statistics=False)\n",
    "n_actions = sum(action_space.shape[0] for action_space in env.action_space.values())\n",
    "input_dims = sum(np.prod(env.observation_space.spaces[key].shape) for key in env.observation_space.spaces.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_dims, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, input_dims))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_dims))  \n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "        #print(f\"action_memory:{self.action_memory}\")\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "    \n",
    "    def clear(self):\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *self.state_memory.shape[1:]))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *self.new_state_memory.shape[1:]))\n",
    "        self.action_memory = np.zeros((self.mem_size, *self.action_memory.shape[1:]))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "        \n",
    "class DDPG_Actor(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DDPG_Actor, self).__init__()\n",
    "\n",
    "        self.bn_input = tf.keras.layers.BatchNormalization()\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.output_layer = tf.keras.layers.Dense(n_actions, activation='sigmoid')  # Two output units for 'price_lower' and 'price_upper'\n",
    "\n",
    "    def call(self, state):\n",
    "        state = self.bn_input(state)\n",
    "        x = self.fc1(state)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn2(x)\n",
    "        actions = self.output_layer(x)\n",
    "        \n",
    "        return actions\n",
    "        \n",
    "class DDPG_Critic(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DDPG_Critic, self).__init__()\n",
    "        \n",
    "        self.bn_state = tf.keras.layers.BatchNormalization()\n",
    "        self.bn_action = tf.keras.layers.BatchNormalization()\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.q = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        state = self.bn_state(state)\n",
    "        action = self.bn_action(action)\n",
    "        x = tf.concat([state, action], axis=1) \n",
    "        x = self.fc1(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        q = self.q(x)\n",
    "        return q\n",
    "    \n",
    "class DDPG:\n",
    "    def __init__(self, alpha=0.001, beta=0.002, input_dims=[8], tau=0.005, env=None,gamma=0.99, n_actions=2, max_size=1000000, batch_size=64,training=True,max_grad_norm=10):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = DDPG_Actor(n_actions=n_actions)\n",
    "        self.critic = DDPG_Critic(n_actions=n_actions)\n",
    "        self.target_actor = DDPG_Actor(n_actions=n_actions)\n",
    "        self.target_critic = DDPG_Critic(n_actions=n_actions)\n",
    "\n",
    "        self.actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=beta))\n",
    "        self.target_actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.target_critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=beta))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "        self.env = env\n",
    "        self.training=training\n",
    "        self.max_grad_norm=max_grad_norm\n",
    "\n",
    "        # For tensorboard logging\n",
    "        self.log_dir = os.path.join(base_path,'model_storage/tensorboard_ddpg_logs')\n",
    "        self.train_summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_actor.weights\n",
    "        for i, weight in enumerate(self.actor.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_actor.set_weights(weights)\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_critic.weights\n",
    "        for i, weight in enumerate(self.critic.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        flat_state = self.flatten_state(state)\n",
    "        flat_action = self.flatten_action(action)\n",
    "        flat_new_state = self.flatten_state(new_state)\n",
    "        self.memory.store_transition(flat_state, flat_action, reward, flat_new_state, done)\n",
    "        \n",
    "    def flatten_state(self,state_dict):\n",
    "        scaled_curr_price = float(state_dict['scaled_curr_price'])\n",
    "        scaled_liquidity = float(state_dict['scaled_liquidity'])\n",
    "        scaled_fee_growth_0 = float(state_dict['scaled_feeGrowthGlobal0x128'])\n",
    "        scaled_fee_growth_1 = float(state_dict['scaled_feeGrowthGlobal1x128'])\n",
    "        \n",
    "        return np.array([scaled_curr_price, scaled_liquidity, scaled_fee_growth_0, scaled_fee_growth_1])\n",
    "\n",
    "    def unflatten_state(self,state_array):\n",
    "        return {\n",
    "            'scaled_curr_price': state_array[0],\n",
    "            'scaled_liquidity': state_array[1],\n",
    "            'scaled_feeGrowthGlobal0x128': state_array[2],\n",
    "            'scaled_feeGrowthGlobal1x128': state_array[3]\n",
    "        }\n",
    "\n",
    "    def flatten_action(self,action):\n",
    "        return tf.reshape(action, [-1])\n",
    "\n",
    "    def unflatten_action(self,action):\n",
    "        return tf.reshape(action, [1, -1])\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        state_dict = state\n",
    "        state_dict_float = {\n",
    "            key: float(value) for key, value in state_dict.items()\n",
    "        }\n",
    "\n",
    "        state_array = np.array(list(state_dict_float.values()), dtype=np.float32)\n",
    "        state_array = state_array.reshape(1, -1)\n",
    "        state_tensor = tf.convert_to_tensor(state_array, dtype=tf.float32)\n",
    "        raw_actions_tensor = self.actor(state_tensor,training=False)\n",
    "        \n",
    "        return raw_actions_tensor\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        #print(f\"{state},{action},{reward},{new_state}\")\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(states_,training=False)\n",
    "            critic_value_ = tf.squeeze(self.target_critic(states_, target_actions,training=False), 1)\n",
    "            critic_value = tf.squeeze(self.critic(states, actions,training=True), 1)\n",
    "            target = rewards + self.gamma*critic_value_*(1-done)\n",
    "            critic_loss = tf.keras.losses.MSE(target, critic_value)\n",
    "\n",
    "        critic_network_gradient = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        critic_network_gradient, _ = tf.clip_by_global_norm(critic_network_gradient, self.max_grad_norm)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_network_gradient, self.critic.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_policy_actions = self.actor(states,training=True)\n",
    "            actor_loss = -self.critic(states, new_policy_actions,training=True)\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        actor_network_gradient, _ = tf.clip_by_global_norm(actor_network_gradient, self.max_grad_norm)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_network_gradient, self.actor.trainable_variables))\n",
    "        \n",
    "        print(f\"Actor_Loss: {actor_loss.numpy()}, Critic_Loss: {critic_loss.numpy()}\")\n",
    "       \n",
    "        with self.train_summary_writer.as_default():\n",
    "            tf.summary.scalar('critic_loss', critic_loss.numpy(), step=self.memory.mem_cntr)\n",
    "            tf.summary.scalar('actor_loss', actor_loss.numpy(), step=self.memory.mem_cntr)\n",
    "\n",
    "\n",
    "        self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_ddpg_agent(max_steps=100, n_episodes=10, model_name='model_storage/ddpg/lstm_actor_critic_batch_norm',alpha=0.001, beta=0.001, input_dims=input_dims, tau=0.8, env=env,n_actions=n_actions, batch_size=50, training=True,agent_budget_usd=10000,use_running_statistics=False):\n",
    "    env=DiscreteSimpleEnv(agent_budget_usd=agent_budget_usd,use_running_statistics=use_running_statistics)\n",
    "    ddpg_agent = DDPG(alpha=alpha, beta=beta, input_dims=input_dims, tau=tau, env=env, n_actions=n_actions, batch_size=batch_size, training=training)\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = ddpg_agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            ddpg_agent.remember(state, action, reward, next_state, done)\n",
    "            ddpg_agent.learn()\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode {i+1}: Reward = {episode_reward}\")\n",
    "        #ddpg_agent.memory.clear()\n",
    "\n",
    "    # Saved Trained weights\n",
    "    model_base_path = os.path.join(base_path,model_name)\n",
    "    ddpg_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ddpg_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "    ddpg_agent.actor.save_weights(ddpg_actor_model_path)\n",
    "    ddpg_agent.critic.save_weights(ddpg_critic_model_path)\n",
    "\n",
    "#train_ddpg_agent(max_steps=100,n_episodes=10,model_name='model_storage/ddpg/lstm_actor_critic_batch_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Visulalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_training_vis():\n",
    "    df_data = []\n",
    "\n",
    "    for entry in train_data_log:\n",
    "        episode, step_count, scaled_action, raw_state, tensor_data, scaled_state, raw_reward, scaled_reward, cumulative_reward, fee_earned, impermanent_loss = entry\n",
    "        \n",
    "        # Extract raw_action values from tensor_data\n",
    "        raw_action_0 = float(tensor_data[0][0].numpy())\n",
    "        raw_action_1 = float(tensor_data[0][1].numpy())\n",
    "\n",
    "        scaled_action_0 = scaled_action['price_lower']\n",
    "        scaled_action_1 = scaled_action['price_upper']\n",
    "        \n",
    "        # Combine all data into a single dictionary\n",
    "        data = {\n",
    "            'episode': episode,\n",
    "            'step_count': step_count,\n",
    "            'scaled_action_0':scaled_action_0,\n",
    "            'scaled_action_1':scaled_action_1,\n",
    "            'raw_reward': raw_reward,\n",
    "            'scaled_reward': scaled_reward,\n",
    "            'fee_earned': fee_earned,\n",
    "            'impermanent_loss': impermanent_loss,\n",
    "            'cumulative_reward': cumulative_reward,\n",
    "            'raw_action_0': raw_action_0,\n",
    "            'raw_action_1': raw_action_1,\n",
    "        }\n",
    "        \n",
    "        # Add raw_action, global_state, and state data\n",
    "        #data.update(scaled_action)\n",
    "        data.update(raw_state)\n",
    "        data.update(scaled_state)\n",
    "        \n",
    "        df_data.append(data)\n",
    "\n",
    "    ddpg_train_data_df = pd.DataFrame(df_data)\n",
    "    ddpg_train_data_df.to_csv('model_outdir_csv/ddpg_agent_train_data.csv', index=False)\n",
    "\n",
    "    train_rewards_plot(ddpg_train_data_df)\n",
    "    train_raw_actions_plot(ddpg_train_data_df)\n",
    "    train_scaled_actions_plot(ddpg_train_data_df)\n",
    "    train_combined_metrics_plot(ddpg_train_data_df)\n",
    "    #train_separate_episode_action_plot(ddpg_train_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent (Stochastic Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, buffer_size, observation_dims, n_actions):\n",
    "        self.states = np.zeros((buffer_size, observation_dims), dtype=np.float32)\n",
    "        self.actions = np.zeros((buffer_size, n_actions), dtype=np.float32)\n",
    "        self.rewards = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.dones = np.zeros(buffer_size, dtype=bool)\n",
    "        self.log_probs = np.zeros((buffer_size, n_actions), dtype=np.float32)\n",
    "        self.values = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.next_values = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.ptr, self.max_size = 0, buffer_size\n",
    "        self.buffer_size=buffer_size\n",
    "        self.observation_dims=observation_dims\n",
    "        self.n_actions=n_actions\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def store_transition(self, state, action, reward, done, log_prob, value, next_value):\n",
    "        index = self.ptr % self.max_size\n",
    "        self.states[index] = state\n",
    "        self.actions[index] = action\n",
    "        self.rewards[index] = reward\n",
    "        self.dones[index] = done\n",
    "        self.log_probs[index] = log_prob\n",
    "        self.values[index] = value\n",
    "        self.next_values[index] = next_value\n",
    "        self.ptr += 1\n",
    "\n",
    "    def sample(self):\n",
    "        self.ptr = 0\n",
    "        return (self.states, self.actions, self.rewards, self.dones, \n",
    "                self.log_probs, self.values, self.next_values)\n",
    "    def reset(self):\n",
    "        self.states = np.zeros((self.buffer_size, self.observation_dims), dtype=np.float32)\n",
    "        self.actions = np.zeros((self.buffer_size, self.n_actions), dtype=np.float32)\n",
    "        self.rewards = np.zeros(self.buffer_size, dtype=np.float32)\n",
    "        self.dones = np.zeros(self.buffer_size, dtype=bool)\n",
    "        self.log_probs = np.zeros((self.buffer_size, self.n_actions), dtype=np.float32)\n",
    "        self.values = np.zeros(self.buffer_size, dtype=np.float32)\n",
    "        self.next_values = np.zeros(self.buffer_size, dtype=np.float32)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def is_full(self):\n",
    "        return self.ptr >= self.buffer_size\n",
    "    \n",
    "class PPO_Actor(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(PPO_Actor, self).__init__()\n",
    "\n",
    "        self.bn_input = tf.keras.layers.BatchNormalization()\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        # Output the mean of the actions\n",
    "        self.mu = tf.keras.layers.Dense(n_actions, activation='sigmoid')\n",
    "        # Output the standard deviation of the actions (log std for numerical stability)\n",
    "        self.sigma = tf.keras.layers.Dense(n_actions, activation='sigmoid')\n",
    "\n",
    "    def call(self, state):\n",
    "        state = self.bn_input(state)\n",
    "        x = self.fc1(state)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "class PPO_Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PPO_Critic, self).__init__()\n",
    "\n",
    "        self.bn_state = tf.keras.layers.BatchNormalization()\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.q = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.bn_state(state) \n",
    "        x = self.fc1(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, n_actions,observation_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95, policy_clip=0.2, buffer_size=64, max_grad_norm=0.5, n_epochs=1,training=True):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.policy_clip = policy_clip\n",
    "        self.buffer_size=buffer_size\n",
    "        self.optimizer=tf.keras.optimizers.Adam(learning_rate=alpha)\n",
    "        self.observation_dims=observation_dims\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.n_epochs=n_epochs\n",
    "        \n",
    "        self.actor = PPO_Actor(n_actions)\n",
    "        self.critic = PPO_Critic()\n",
    "        self.actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "\n",
    "        self.rollout_buffer = RolloutBuffer(self.buffer_size, observation_dims, n_actions)\n",
    "\n",
    "        self.env=env\n",
    "        self.training=training\n",
    "        \n",
    "\n",
    "         # For tensorboard logging\n",
    "        self.log_dir = os.path.join(base_path,'model_storage/tensorboard_ppo_logs')\n",
    "        self.train_summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self.tensorboard_counter=0\n",
    " \n",
    "    def choose_action(self, state):\n",
    "\n",
    "        state_dict = state\n",
    "        state_dict_float = {\n",
    "            key: float(value) for key, value in state_dict.items()\n",
    "        }\n",
    "\n",
    "        state_array = np.array(list(state_dict_float.values()), dtype=np.float32)\n",
    "        state_array = state_array.reshape(1, -1)\n",
    "        state_tensor = tf.convert_to_tensor(state_array, dtype=tf.float32)\n",
    "\n",
    "        mu, sigma = self.actor(state_tensor,training=self.training)\n",
    "        \n",
    "        action_prob = tfp.distributions.Normal(mu, sigma)\n",
    "        action = action_prob.sample()\n",
    "        \n",
    "        #Action clipping\n",
    "        #action = tf.clip_by_value(action, 0, 1)\n",
    "        log_prob = action_prob.log_prob(action)\n",
    "        print(f\"mu: {mu}, sigma: {sigma}, action_prob: {action_prob}, action: {action}, log_prob: {log_prob}\")\n",
    "        return action,log_prob\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done, log_prob):\n",
    "        \n",
    "        flat_state = self.flatten_state(state)\n",
    "        flat_action = self.flatten_action(action)\n",
    "        flat_next_state = self.flatten_state(next_state)\n",
    "        value = self.critic(tf.convert_to_tensor([flat_state], dtype=tf.float32))\n",
    "        next_value = self.critic(tf.convert_to_tensor([flat_next_state], dtype=tf.float32))\n",
    "        self.rollout_buffer.store_transition(flat_state, flat_action, reward, done, log_prob, value, next_value)\n",
    "    \n",
    "    def learn(self):\n",
    "        states, actions, rewards, dones, old_log_probs, values, next_values = self.rollout_buffer.sample()\n",
    "        returns, advantages = self.compute_gae(rewards, values, next_values, dones, self.gamma, self.gae_lambda)\n",
    "        \n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        old_log_probs = tf.convert_to_tensor(old_log_probs, dtype=tf.float32)\n",
    "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "        returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "        # Update policy and value networks for a number of epochs\n",
    "        for _ in range(self.n_epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                total_loss = self.ppo_loss(states, actions, old_log_probs, advantages, returns, self.policy_clip)\n",
    "            gradients = tape.gradient(total_loss, self.actor.trainable_variables + self.critic.trainable_variables)\n",
    "            # Gradient clipping\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, self.max_grad_norm)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.actor.trainable_variables + self.critic.trainable_variables))\n",
    "                \n",
    "        self.rollout_buffer.reset()\n",
    "       \n",
    "    def compute_gae(self, rewards, values, next_values, dones, gamma=0.99, lam=0.95):\n",
    "        gae = 0\n",
    "        returns = np.zeros_like(rewards)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + gamma * next_values[t] * (1 - dones[t]) - values[t]\n",
    "            gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "            returns[t] = gae + values[t]\n",
    "        advantages = returns - values\n",
    "        return returns, (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "    def ppo_loss(self, states, actions, old_log_probs, advantages, returns, clip_param=0.2):\n",
    "        mu, sigma = self.actor(states)\n",
    "        values = tf.squeeze(self.critic(states))\n",
    "\n",
    "        # Calculate new log probabilities using the updated policy\n",
    "        new_policy = tfp.distributions.Normal(mu, sigma)\n",
    "        new_log_probs = new_policy.log_prob(actions)\n",
    "\n",
    "        # Policy loss\n",
    "        ratios = tf.exp(new_log_probs - old_log_probs)\n",
    "        #ratios = tf.reduce_mean(ratios, axis=1)\n",
    "        advantages = tf.expand_dims(advantages, -1)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = tf.clip_by_value(ratios, 1 - clip_param, 1 + clip_param) * advantages\n",
    "        policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))\n",
    "        # Value loss\n",
    "        value_loss = tf.reduce_mean(tf.square(returns - values))\n",
    "        # Total loss\n",
    "        total_loss = policy_loss + 0.5 * value_loss\n",
    "        with self.train_summary_writer.as_default():\n",
    "            tf.summary.scalar('total_loss', total_loss.numpy(), step=self.tensorboard_counter)\n",
    "            tf.summary.scalar('policy_loss', total_loss.numpy(), step=self.tensorboard_counter)\n",
    "            tf.summary.scalar('value_loss', total_loss.numpy(), step=self.tensorboard_counter)\n",
    "\n",
    "        print(f\"total_loss:{total_loss}, policy_loss:{policy_loss}, value_loss:{value_loss}, advantages:{advantages}, returns:{returns}\")\n",
    "        self.tensorboard_counter+=1\n",
    "        return total_loss\n",
    "          \n",
    "    def flatten_state(self,state_dict):\n",
    "        scaled_curr_price = float(state_dict['scaled_curr_price'])\n",
    "        scaled_liquidity = float(state_dict['scaled_liquidity'])\n",
    "        scaled_fee_growth_0 = float(state_dict['scaled_feeGrowthGlobal0x128'])\n",
    "        scaled_fee_growth_1 = float(state_dict['scaled_feeGrowthGlobal1x128'])\n",
    "        \n",
    "        return np.array([scaled_curr_price, scaled_liquidity, scaled_fee_growth_0, scaled_fee_growth_1])\n",
    "\n",
    "    def unflatten_state(self,state_array):\n",
    "        return {\n",
    "            'scaled_curr_price': state_array[0],\n",
    "            'scaled_liquidity': state_array[1],\n",
    "            'scaled_feeGrowthGlobal0x128': state_array[2],\n",
    "            'scaled_feeGrowthGlobal1x128': state_array[3]\n",
    "        }\n",
    "\n",
    "    def flatten_action(self,action):\n",
    "        return tf.reshape(action, [-1])\n",
    "\n",
    "    def unflatten_action(self,action):\n",
    "        return tf.reshape(action, [1, -1])\n",
    "\n",
    "    def map_indices_to_action_values(self, action_indices):\n",
    "        action_dict = {\n",
    "            'price_relative_lower': action_indices[0],\n",
    "            'price_relative_upper': action_indices[1]\n",
    "        }\n",
    "        return action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo_agent(max_steps=100, n_episodes=10, model_name='model_storage/ppo/lstm_actor_critic_batch_norm', n_actions=n_actions, observation_dims=input_dims,buffer_size=50,n_epochs=10, gamma=0.5, alpha=0.01, gae_lambda=0.75, policy_clip=0.8, max_grad_norm=10,agent_budget_usd=10000,use_running_statistics=False):\n",
    "    \n",
    "    env=DiscreteSimpleEnv(agent_budget_usd=agent_budget_usd,use_running_statistics=use_running_statistics)\n",
    "    ppo_agent = PPO(env, n_actions, observation_dims=input_dims,buffer_size=buffer_size,n_epochs=n_epochs, gamma=gamma, alpha=alpha, gae_lambda=gae_lambda, policy_clip=policy_clip, max_grad_norm=max_grad_norm)\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action,log_prob = ppo_agent.choose_action(state)\n",
    "        \n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            ppo_agent.remember(state, action, reward, next_state, done,log_prob)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if ppo_agent.rollout_buffer.is_full():\n",
    "                ppo_agent.learn()\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode {i+1}: Reward = {episode_reward}\")\n",
    "\n",
    "    ppo_model_base_path = os.path.join(base_path,model_name)\n",
    "    ppo_actor_model_path = os.path.join(ppo_model_base_path, 'actor')\n",
    "    ppo_critic_model_path = os.path.join(ppo_model_base_path, 'critic')\n",
    "    ppo_agent.actor.save_weights(ppo_actor_model_path)\n",
    "    ppo_agent.critic.save_weights(ppo_critic_model_path)\n",
    "\n",
    "#train_ppo_agent(max_steps=100,n_episodes=10,model_name='model_storage/ppo/lstm_actor_critic_batch_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Visulizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_training_vis():\n",
    "    df_data = []\n",
    "\n",
    "    for entry in train_data_log:\n",
    "        episode, step_count, scaled_action, raw_state, tensor_data, scaled_state, raw_reward, scaled_reward, cumulative_reward, fee_earned, impermanent_loss = entry\n",
    "        \n",
    "        # Extract raw_action values from tensor_data\n",
    "        raw_action_0 = float(tensor_data[0][0].numpy())\n",
    "        raw_action_1 = float(tensor_data[0][1].numpy())\n",
    "\n",
    "        scaled_action_0 = scaled_action['price_lower']\n",
    "        scaled_action_1 = scaled_action['price_upper']\n",
    "        \n",
    "        # Combine all data into a single dictionary\n",
    "        data = {\n",
    "            'episode': episode,\n",
    "            'step_count': step_count,\n",
    "            'scaled_action_0':scaled_action_0,\n",
    "            'scaled_action_1':scaled_action_1,\n",
    "            'raw_reward': raw_reward,\n",
    "            'scaled_reward': scaled_reward,\n",
    "            'fee_earned': fee_earned,\n",
    "            'impermanent_loss': impermanent_loss,\n",
    "            'cumulative_reward': cumulative_reward,\n",
    "            'raw_action_0': raw_action_0,\n",
    "            'raw_action_1': raw_action_1,\n",
    "        }\n",
    "        \n",
    "        # Add raw_action, global_state, and state data\n",
    "        #data.update(scaled_action)\n",
    "        data.update(raw_state)\n",
    "        data.update(scaled_state)\n",
    "        \n",
    "        df_data.append(data)\n",
    "\n",
    "    ppo_train_data_df = pd.DataFrame(df_data)\n",
    "    ppo_train_data_df.to_csv('model_outdir_csv/ppo_agent_train_data.csv', index=False)\n",
    "\n",
    "    train_rewards_plot(ppo_train_data_df)\n",
    "    train_raw_actions_plot(ppo_train_data_df)\n",
    "    train_scaled_actions_plot(ppo_train_data_df)\n",
    "    train_combined_metrics_plot(ppo_train_data_df)\n",
    "    train_separate_episode_action_plot(ppo_train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir ./model_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSimpleEnvEval(DiscreteSimpleEnv):\n",
    "    def __init__(self, agent_budget_usd, percentage_range=0.3, seed=32,penalty_param_magnitude=0,use_running_statistics=False):\n",
    "        super().__init__(agent_budget_usd)\n",
    "        self.percentage_range = percentage_range\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.cumulative_reward_rl_agent = 0\n",
    "        self.cumulative_reward_baseline_agent = 0\n",
    "        self.penalty_param_magnitude=penalty_param_magnitude\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        # Disable exploration noise\n",
    "        self.exploration_std_dev = 0.0\n",
    "        return super()._take_action(action)\n",
    "    \n",
    "    def step(self, raw_action_rl_agent):\n",
    "        # The RL agent takes an action\n",
    "        mint_tx_receipt_rl_agent, action_rl_agent = self._take_action(raw_action_rl_agent)\n",
    "        raw_action_baseline_agent=self.baseline_agent_policy()\n",
    "        # The baseline agent takes an action\n",
    "        mint_tx_receipt_baseline_agent, action_baseline_agent = self._take_action_baseline(raw_action_baseline_agent)\n",
    "\n",
    "        # Perform environment step\n",
    "        print('Environment Step')\n",
    "        self.engine.reset()\n",
    "        self.engine.run()\n",
    "        print()\n",
    "\n",
    "        # Calculate rewards for both agents\n",
    "        scaled_reward_rl_agent, raw_reward_rl_agent, fee_income_rl_agent, impermanent_loss_rl_agent = self._calculate_reward(action_rl_agent, mint_tx_receipt_rl_agent)\n",
    "        scaled_reward_baseline_agent, raw_reward_baseline_agent, fee_income_baseline_agent, impermanent_loss_baseline_agent = self._calculate_reward(action_baseline_agent, mint_tx_receipt_baseline_agent)\n",
    "\n",
    "        # Update cumulative rewards\n",
    "        self.cumulative_reward_rl_agent += scaled_reward_rl_agent\n",
    "        self.cumulative_reward_baseline_agent += scaled_reward_baseline_agent\n",
    "\n",
    "        self.step_count+=1\n",
    "        # Print rewards and cumulative rewards for both agents\n",
    "        print(f\"episode: {self.episode}, step_count: {self.step_count}\")\n",
    "        print(f\"rl_agent_scaled_reward: {scaled_reward_rl_agent}, rl_agent_raw_reward: {raw_reward_rl_agent}, rl_agent_cumulative_reward: {self.cumulative_reward_rl_agent}\")\n",
    "        print(f\"baseline_agent_scaled_reward: {scaled_reward_baseline_agent}, baseline_agent_raw_reward: {raw_reward_baseline_agent}, baseline_agent_cumulative_reward: {self.cumulative_reward_baseline_agent}\")\n",
    "        print(f\"raw_pool_state: {self.pool.get_global_state()}\")\n",
    "        print(f\"sclaed_pool_state: {self.state}\")\n",
    "        print()\n",
    "      \n",
    "\n",
    "        # Update the state and check if the episode is done\n",
    "        self.state = self.get_obs_space()\n",
    "        self.done = self._is_done()\n",
    "        eval_data_log.append((self.episode, self.step_count, self.pool.get_global_state(), raw_action_rl_agent,action_rl_agent,raw_action_baseline_agent,action_baseline_agent, self.state, raw_reward_rl_agent, raw_reward_baseline_agent,scaled_reward_rl_agent,scaled_reward_baseline_agent, self.cumulative_reward_rl_agent, self.cumulative_reward_baseline_agent, fee_income_rl_agent, impermanent_loss_rl_agent,fee_income_baseline_agent,impermanent_loss_baseline_agent))\n",
    "        # Return the necessary information\n",
    "        return self.state, raw_reward_rl_agent, self.done, {}\n",
    "    \n",
    "    def _take_action_baseline(self, action_dict):\n",
    "        \n",
    "        print('Baseline Agent Action')\n",
    "        print(f\"action: {action_dict}\")\n",
    "\n",
    "        tick_lower=price_to_valid_tick(action_dict['price_lower'])\n",
    "        tick_upper=price_to_valid_tick(action_dict['price_upper'])\n",
    "        amount=self.agent_budget_usd\n",
    "\n",
    "        mint_tx_receipt=self.pool.add_liquidity(GOD_ACCOUNT, tick_lower, tick_upper, amount, b'')\n",
    "\n",
    "        return mint_tx_receipt,action_dict\n",
    "    \n",
    "    \n",
    "    def baseline_agent_policy(self):\n",
    "        global_state = self.pool.get_global_state()\n",
    "        raw_curr_price = global_state['curr_price']\n",
    "        \n",
    "        # Calculate the price range based on the raw current price\n",
    "        lower_price = raw_curr_price * (1 - self.percentage_range)\n",
    "        upper_price = raw_curr_price * (1 + self.percentage_range)\n",
    "\n",
    "        action_baseline={\n",
    "            'price_lower':lower_price,\n",
    "            'price_upper':upper_price\n",
    "        }\n",
    "        \n",
    "        return action_baseline\n",
    "\n",
    "class DDGPEval(DDPG):\n",
    "    def choose_action(self, state):\n",
    "        # Disable exploration noise\n",
    "        action = super().choose_action(state)\n",
    "        return action\n",
    "    \n",
    "class PPOEval(PPO):\n",
    "    def choose_action(self, state):\n",
    "        # Disable exploration noise\n",
    "        action = super().choose_action(state)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ddpg_agent(eval_steps=100,eval_episodes=2,model_name='model_storage/ddpg/200_100_step_running_stats_lstm_bn_global_obs_norm'):    \n",
    "    eval_env = DiscreteSimpleEnvEval(agent_budget_usd=10000,percentage_range=0.30, seed=42)\n",
    "    ddpg_eval_agent = DDGPEval(env=eval_env, n_actions=n_actions, input_dims=input_dims, training=False)\n",
    "    model_base_path = os.path.join(base_path,model_name)\n",
    "\n",
    "    ddpg_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ddpg_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "\n",
    "    ddpg_eval_agent.actor.load_weights(ddpg_actor_model_path)\n",
    "    ddpg_eval_agent.critic.load_weights(ddpg_critic_model_path)\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        state = eval_env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(eval_steps):\n",
    "            action = ddpg_eval_agent.choose_action(state)\n",
    "            next_state, reward, done, _ = eval_env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode {episode+1}/{eval_episodes}, Reward: {episode_reward}\")\n",
    "    \n",
    "#eval_ddpg_agent(eval_steps=100,eval_episodes=2,model_name='model_storage/ddpg/200_100_step_running_stats_lstm_bn_global_obs_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_eval_vis():\n",
    "    df_eval_data = []\n",
    "\n",
    "    for entry in eval_data_log:\n",
    "        (episode, step_count, global_state, raw_action_rl_agent, action_rl_agent, \n",
    "        raw_action_baseline_agent, action_baseline_agent, state, \n",
    "        raw_reward_rl_agent, raw_reward_baseline_agent,scaled_reward_rl_agent,scaled_reward_baseline_agent, cumulative_reward_rl_agent, \n",
    "        cumulative_reward_baseline_agent, fee_income_rl_agent, impermanent_loss_rl_agent, \n",
    "        fee_income_baseline_agent, impermanent_loss_baseline_agent) = entry\n",
    "        \n",
    "        # Extract raw_action values for RL agent\n",
    "        raw_action_rl_agent_0 = float(raw_action_rl_agent[0][0].numpy())\n",
    "        raw_action_rl_agent_1 = float(raw_action_rl_agent[0][1].numpy())\n",
    "\n",
    "        scaled_action_rl_agent_0 = action_rl_agent['price_lower']\n",
    "        scaled_action_rl_agent_1 = action_rl_agent['price_upper']\n",
    "        \n",
    "        # Extract raw_action values for baseline agent\n",
    "        raw_action_baseline_agent_0 = raw_action_baseline_agent['price_lower']\n",
    "        raw_action_baseline_agent_1 = raw_action_baseline_agent['price_upper']\n",
    "        \n",
    "        # Combine all data into a single dictionary\n",
    "        data = {\n",
    "            'episode': episode,\n",
    "            'step_count': step_count,\n",
    "            'raw_reward_rl_agent': raw_reward_rl_agent,\n",
    "            'scaled_reward_rl_agent':scaled_reward_rl_agent,\n",
    "            'cumulative_reward_rl_agent': cumulative_reward_rl_agent,\n",
    "            'scaled_action_rl_agent_0':scaled_action_rl_agent_0,\n",
    "            'scaled_action_rl_agent_1':scaled_action_rl_agent_1,\n",
    "            'fee_income_rl_agent': fee_income_rl_agent,\n",
    "            'impermanent_loss_rl_agent': impermanent_loss_rl_agent,\n",
    "            'raw_reward_baseline_agent': raw_reward_baseline_agent,\n",
    "            'scaled_reward_baseline_agent':scaled_reward_baseline_agent,\n",
    "            'cumulative_reward_baseline_agent': cumulative_reward_baseline_agent,\n",
    "            'raw_action_baseline_agent_0': raw_action_baseline_agent_0,\n",
    "            'raw_action_baseline_agent_1': raw_action_baseline_agent_1,\n",
    "            'fee_income_baseline_agent': fee_income_baseline_agent,\n",
    "            'impermanent_loss_baseline_agent': impermanent_loss_baseline_agent,\n",
    "            'raw_action_rl_agent_0': raw_action_rl_agent_0,\n",
    "            'raw_action_rl_agent_1': raw_action_rl_agent_1,\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Add action, global_state, and state data\n",
    "        \n",
    "        data.update(global_state)\n",
    "        data.update(state)\n",
    "        \n",
    "        df_eval_data.append(data)\n",
    "\n",
    "    ddpg_eval_data_df = pd.DataFrame(df_eval_data)\n",
    "    ddpg_eval_data_df.to_csv('model_outdir_csv/ddpg_agent_eval_data.csv', index=False)\n",
    "    eval_rewards_plot(ddpg_eval_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppo_agent(eval_steps=100, eval_episodes=2, model_name='model_storage/ppo/lstm_actor_critic_batch_norm'):\n",
    "    eval_env = DiscreteSimpleEnvEval(agent_budget_usd=10000,percentage_range=0.50, seed=42, penalty_param_magnitude=0, use_running_statistics=False)\n",
    "    ppo_eval_agent = PPOEval(eval_env, n_actions, observation_dims=input_dims, buffer_size=5,training=False)\n",
    "\n",
    "    #model_base_path = os.path.join(base_path,'model_storage/ppo/200_100_step_running_stats_lstm_bn_global_obs_norm')\n",
    "    #ppo_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    #ppo_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "    model_base_path = os.path.join(base_path,model_name)\n",
    "\n",
    "    ppo_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ppo_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "\n",
    "    ppo_eval_agent.actor.load_weights(ppo_actor_model_path)\n",
    "    ppo_eval_agent.critic.load_weights(ppo_critic_model_path)\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        state = eval_env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(eval_steps):\n",
    "            action,_ = ppo_eval_agent.choose_action(state)\n",
    "            next_state, reward, done, _ = eval_env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode {episode+1}/{eval_episodes}, Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_eval_vis():\n",
    "    df_eval_data = []    \n",
    "    for entry in eval_data_log:\n",
    "        (episode, step_count, global_state, raw_action_rl_agent, action_rl_agent, \n",
    "        raw_action_baseline_agent, action_baseline_agent, state, \n",
    "        raw_reward_rl_agent, raw_reward_baseline_agent,scaled_reward_rl_agent,scaled_reward_baseline_agent, cumulative_reward_rl_agent, \n",
    "        cumulative_reward_baseline_agent, fee_income_rl_agent, impermanent_loss_rl_agent, \n",
    "        fee_income_baseline_agent, impermanent_loss_baseline_agent) = entry\n",
    "        \n",
    "        # Extract raw_action values for RL agent\n",
    "        raw_action_rl_agent_0 = float(raw_action_rl_agent[0][0].numpy())\n",
    "        raw_action_rl_agent_1 = float(raw_action_rl_agent[0][1].numpy())\n",
    "\n",
    "        scaled_action_rl_agent_0 = action_rl_agent['price_lower']\n",
    "        scaled_action_rl_agent_1 = action_rl_agent['price_upper']\n",
    "        \n",
    "        # Extract raw_action values for baseline agent\n",
    "        raw_action_baseline_agent_0 = raw_action_baseline_agent['price_lower']\n",
    "        raw_action_baseline_agent_1 = raw_action_baseline_agent['price_upper']\n",
    "        \n",
    "        # Combine all data into a single dictionary\n",
    "        data = {\n",
    "            'episode': episode,\n",
    "            'step_count': step_count,\n",
    "            'raw_reward_rl_agent': raw_reward_rl_agent,\n",
    "            'scaled_reward_rl_agent':scaled_reward_rl_agent,\n",
    "            'cumulative_reward_rl_agent': cumulative_reward_rl_agent,\n",
    "            'scaled_action_rl_agent_0':scaled_action_rl_agent_0,\n",
    "            'scaled_action_rl_agent_1':scaled_action_rl_agent_1,\n",
    "            'fee_income_rl_agent': fee_income_rl_agent,\n",
    "            'impermanent_loss_rl_agent': impermanent_loss_rl_agent,\n",
    "            'raw_reward_baseline_agent': raw_reward_baseline_agent,\n",
    "            'scaled_reward_baseline_agent':scaled_reward_baseline_agent,\n",
    "            'cumulative_reward_baseline_agent': cumulative_reward_baseline_agent,\n",
    "            'raw_action_baseline_agent_0': raw_action_baseline_agent_0,\n",
    "            'raw_action_baseline_agent_1': raw_action_baseline_agent_1,\n",
    "            'fee_income_baseline_agent': fee_income_baseline_agent,\n",
    "            'impermanent_loss_baseline_agent': impermanent_loss_baseline_agent,\n",
    "            'raw_action_rl_agent_0': raw_action_rl_agent_0,\n",
    "            'raw_action_rl_agent_1': raw_action_rl_agent_1,\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Add action, global_state, and state data\n",
    "        \n",
    "        data.update(global_state)\n",
    "        data.update(state)\n",
    "        \n",
    "        df_eval_data.append(data)\n",
    "\n",
    "    ppo_eval_data_df = pd.DataFrame(df_eval_data)\n",
    "    ppo_eval_data_df.to_csv('model_outdir_csv/ppo_agent_eval_data.csv', index=False)\n",
    "    eval_rewards_plot(ppo_eval_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_log=[]\n",
    "ddpg_model_name='model_storage/ddpg/ddpg_1'\n",
    "train_ddpg_agent(max_steps=200, n_episodes=50, model_name=ddpg_model_name,alpha=0.001, beta=0.001, input_dims=input_dims, tau=0.8, env=env,n_actions=n_actions, batch_size=50, training=True,agent_budget_usd=10000,use_running_statistics=False)\n",
    "eval_data_log=[]\n",
    "eval_ddpg_agent(eval_steps=100,eval_episodes=3,model_name=ddpg_model_name)\n",
    "ddpg_training_vis()\n",
    "ddpg_eval_vis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_log=[]\n",
    "ppo_model_name='model_storage/ppo/ppo_1'\n",
    "train_ppo_agent(max_steps=200, n_episodes=50, model_name=ppo_model_name, n_actions=n_actions, observation_dims=input_dims,buffer_size=50,n_epochs=10, gamma=0.5, alpha=0.01, gae_lambda=0.75, policy_clip=0.8, max_grad_norm=10,agent_budget_usd=10000,use_running_statistics=False)\n",
    "eval_data_log=[]\n",
    "eval_ppo_agent(eval_steps=100,eval_episodes=3,model_name=ppo_model_name)\n",
    "ppo_training_vis()\n",
    "ppo_eval_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(pool_id=\"0x3416cf6c708da44db2624d63ea0aaef7113527c6\",agent=\"ddpg\",ddpg_model_path='model_storage/ddpg/200_100_step_running_stats_lstm_bn_global_obs_norm',ppo_model_path='model_storage/ppo/lstm_actor_critic_batch_norm'):\n",
    "    eval_data_log=[]\n",
    "    eval_env = DiscreteSimpleEnvEval(agent_budget_usd=10000,percentage_range=0.30, seed=42)\n",
    "    n_actions = sum(action_space.shape[0] for action_space in eval_env.action_space.values())\n",
    "    input_dims = sum(np.prod(eval_env.observation_space.spaces[key].shape) for key in eval_env.observation_space.spaces.keys())\n",
    "\n",
    "    # Load ddpg eval agent for predictions\n",
    "    ddpg_eval_agent = DDGPEval(env=eval_env, n_actions=n_actions, input_dims=input_dims, training=False)\n",
    "    model_base_path = os.path.join(base_path,ddpg_model_path)\n",
    "    ddpg_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ddpg_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "    ddpg_eval_agent.actor.load_weights(ddpg_actor_model_path)\n",
    "    ddpg_eval_agent.critic.load_weights(ddpg_critic_model_path)\n",
    "\n",
    "    # Load ppo eval agent for infernece\n",
    "    eval_data_log=[]\n",
    "    eval_env = DiscreteSimpleEnvEval(agent_budget_usd=10000,percentage_range=0.50, seed=42,penalty_param_magnitude=0,use_running_statistics=False)\n",
    "    ppo_eval_agent = PPOEval(eval_env, n_actions, observation_dims=input_dims, buffer_size=5,training=False)\n",
    "    \n",
    "    model_base_path = os.path.join(base_path,ppo_model_path)\n",
    "    ppo_actor_model_path = os.path.join(model_base_path, 'actor')\n",
    "    ppo_critic_model_path = os.path.join(model_base_path, 'critic')\n",
    "    ppo_eval_agent.actor.load_weights(ppo_actor_model_path)\n",
    "    ppo_eval_agent.critic.load_weights(ppo_critic_model_path)\n",
    "    \n",
    "    pool_data = fetch_inference_pool_data(pool_id)\n",
    "    print(f\"State Space: {pool_data}\")\n",
    "\n",
    "    global_state=pool_data\n",
    "    curr_price = global_state['token1Price']\n",
    "    liquidity = global_state['liquidity']\n",
    "    fee_growth_0 = global_state['feeGrowthGlobal0X128']\n",
    "    fee_growth_1 = global_state['feeGrowthGlobal1X128']\n",
    "\n",
    "    obs = {'scaled_curr_price': curr_price/5000,'scaled_liquidity': liquidity/1e20,'scaled_feeGrowthGlobal0x128': fee_growth_0/1e34,'scaled_feeGrowthGlobal1x128': fee_growth_1/1e34}\n",
    "    print(f\"Obs Space: {obs}\")\n",
    "    #action,_=ppo_eval_agent.choose_action(obs)\n",
    "    if agent==\"ddpg\":\n",
    "        action=ddpg_eval_agent.choose_action(obs)\n",
    "    elif agent==\"ppo\":\n",
    "        action,_=ppo_eval_agent.choose_action(obs)\n",
    "    eval_env.reset()\n",
    "\n",
    "    raw_a, raw_b = action[0, 0].numpy(), action[0, 1].numpy()\n",
    "\n",
    "    a_0 = raw_a#np.clip(raw_a, 0, 1)\n",
    "    a_1 = raw_b#np.clip(raw_b, 0, 1)\n",
    "\n",
    "    action_lower_bound = curr_price*0.1\n",
    "    action_upper_bound =curr_price*2\n",
    "\n",
    "    price_lower = action_lower_bound + a_0 * (action_upper_bound - action_lower_bound)/2\n",
    "    price_upper = (action_upper_bound - action_lower_bound)/2 + a_1 * (action_upper_bound - action_lower_bound)/2\n",
    "    if price_lower>price_upper:\n",
    "        print(\"flipped\")\n",
    "        price_upper=price_lower\n",
    "    action_dict = {\n",
    "        'price_lower': price_lower,\n",
    "        'price_upper': price_upper\n",
    "    }\n",
    "    print()\n",
    "    print(f'RL Agent Action for pool id: {pool_id}')\n",
    "    print(f\"raw_action: {action} \\n{action_dict}\")\n",
    "\n",
    "    tick_lower=price_to_valid_tick(action_dict['price_lower'])\n",
    "    tick_upper=price_to_valid_tick(action_dict['price_upper'])\n",
    "    print(f\"'tick_lower:' {tick_lower}, 'tick_upper:' {tick_upper}\")\n",
    "\n",
    "    return action,action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space: {'feeGrowthGlobal0X128': 1.4834076456280344e+42, 'feeGrowthGlobal1X128': 3.079134033286047e+33, 'liquidity': 9.249994970856413e+18, 'token1Price': 2278.4855634051128}\n",
      "Obs Space: {'scaled_curr_price': 0.45569711268102253, 'scaled_liquidity': 0.09249994970856414, 'scaled_feeGrowthGlobal0x128': 148340764.56280345, 'scaled_feeGrowthGlobal1x128': 0.30791340332860473}\n",
      "Pool selcted for this episode: BTC_USDT_3000\n",
      "mnemonic: '\u001b[0;1;36mcheck eager rifle magnet congress rapid either buzz sock brick arrow basic\u001b[0;m'\n",
      "funded account with token0: {'Transfer': [OrderedDict([('from', '0x330997E70b83f1a562490FCaA5996314fA5a971a'), ('to', '0x2BD7bfCCa793217D2de8e1d30C7A3159FccA15c9'), ('value', 1000000000000000019884624838656)])]}\n",
      "funded account with token1: {'Transfer': [OrderedDict([('from', '0x330997E70b83f1a562490FCaA5996314fA5a971a'), ('to', '0x2BD7bfCCa793217D2de8e1d30C7A3159FccA15c9'), ('value', 110000000000000000498458871988224)])]}\n",
      "mnemonic: '\u001b[0;1;36mbubble improve nice theory author check nice beef birth exile orient fit\u001b[0;m'\n",
      "funded account with token0: {'Transfer': [OrderedDict([('from', '0x330997E70b83f1a562490FCaA5996314fA5a971a'), ('to', '0x05ce65457B17e3167Bcc1a6512963610347F45f3'), ('value', 4999999999999999727876154935214080)])]}\n",
      "funded account with token1: {'Transfer': [OrderedDict([('from', '0x330997E70b83f1a562490FCaA5996314fA5a971a'), ('to', '0x05ce65457B17e3167Bcc1a6512963610347F45f3'), ('value', 5500000000000000565354898883870720)])]}\n",
      "\n",
      "RL Agent Action for pool id: 0x4e68ccd3e89f51c3074ca5072bbac773960dfa36\n",
      "raw_action: [[0.96538496 0.24952851]] \n",
      "{'price_lower': 2317.4834664823593, 'price_upper': 2704.6810426486722}\n",
      "tick_lower: 77460, tick_upper: 79020\n"
     ]
    }
   ],
   "source": [
    "#pool=\"0x3416cf6c708da44db2624d63ea0aaef7113527c6\" #USDC/USDT\n",
    "#pool=\"0x6c6bc977e13df9b0de53b251522280bb72383700\" #DAI/USDC\n",
    "pool=\"0x4e68ccd3e89f51c3074ca5072bbac773960dfa36\" #ETH/USDT\n",
    "#pool=\"0x99ac8ca7087fa4a2a1fb6357269965a2014abc35\" #WBTC/USDC\n",
    "#pool=\"0xcbcdf9626bc03e24f779434178a73a0b4bad62ed\" #WBTC/ETH\n",
    "\n",
    "action, action_dict = predict_action(pool_id=pool, agent=\"ddpg\",ddpg_model_path='model_storage/ddpg/200_100_step_running_stats_lstm_bn_global_obs_norm',ppo_model_path='model_storage/ppo/lstm_actor_critic_batch_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_action(current_state, rl_agent, user_preferences):\n",
    "    current_profit = current_state['current_profit']\n",
    "    price_out_of_range = current_state['price_out_of_range']\n",
    "    time_since_last_adjustment = current_state['time_since_last_adjustment']\n",
    "\n",
    "    # User Preferences\n",
    "    risk_tolerance = user_preferences['risk_tolerance']\n",
    "    investment_horizon = user_preferences['investment_horizon']\n",
    "    liquidity_preference = user_preferences['liquidity_preference']\n",
    "\n",
    "    # Adjust thresholds based on user preferences\n",
    "    profit_taking_threshold = risk_tolerance['profit_taking']\n",
    "    stop_loss_threshold = risk_tolerance['stop_loss']\n",
    "    rebalance_interval = investment_horizon * 24 * 60 * 60  # Convert days to seconds\n",
    "\n",
    "    # Action decision logic\n",
    "    if current_profit >= profit_taking_threshold:\n",
    "        return 'adjust_position'  # Take profit and possibly re-enter with a new position\n",
    "    elif price_out_of_range and liquidity_preference['adjust_on_price_out_of_range']:\n",
    "        return 'adjust_position'  # Rebalance the position as the price moved out of the current range\n",
    "    elif time_since_last_adjustment >= rebalance_interval:\n",
    "        return 'adjust_position'  # Periodic rebalancing based on time interval\n",
    "    elif current_profit <= stop_loss_threshold:\n",
    "        return 'exit_position'  # Exit the position to stop further losses\n",
    "    else:\n",
    "        return 'maintain_position'  # Maintain the current position\n",
    "\n",
    "# Example user preferences\n",
    "user_preferences = {\n",
    "    'risk_tolerance': {'profit_taking': 1000, 'stop_loss': -500},\n",
    "    'investment_horizon': 7,  # days\n",
    "    'liquidity_preference': {'adjust_on_price_out_of_range': True}\n",
    "}\n",
    "\n",
    "# Assuming 'current_state' is a dictionary containing current pool state information\n",
    "action = determine_action(current_state, rl_agent, user_preferences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
